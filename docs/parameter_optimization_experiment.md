# AutoTabPFN å‚æ•°è°ƒä¼˜å®éªŒæ–‡æ¡£

## æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜äº† `predict_healthcare_auto_external_adjust_parameter.py` è„šæœ¬çš„å®éªŒè®¾è®¡å’Œå®ç°ã€‚è¯¥å®éªŒæ—¨åœ¨é€šè¿‡ç³»ç»Ÿæ€§çš„è¶…å‚æ•°ä¼˜åŒ–ï¼Œæ‰¾åˆ° AutoTabPFN æ¨¡å‹åœ¨åŒ»ç–—æ•°æ®è·¨åŸŸé¢„æµ‹ä»»åŠ¡ä¸­çš„æœ€ä½³é…ç½®ï¼Œä»¥æå‡æ¨¡å‹åœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šçš„æ³›åŒ–æ€§èƒ½ã€‚

## ğŸ¯ å®éªŒç›®æ ‡

### ä¸»è¦ç›®æ ‡
- **è¶…å‚æ•°ä¼˜åŒ–**: é€šè¿‡è´å¶æ–¯ä¼˜åŒ–æˆ–éšæœºæœç´¢æ‰¾åˆ° AutoTabPFN çš„æœ€ä½³å‚æ•°ç»„åˆ
- **è·¨åŸŸæ³›åŒ–**: ä¼˜åŒ–æ¨¡å‹åœ¨ AI4Health â†’ æ²³å—ç™Œç—‡åŒ»é™¢æ•°æ®é›†çš„è·¨åŸŸé¢„æµ‹æ€§èƒ½
- **ç‰¹å¾å·¥ç¨‹**: è¯„ä¼°åˆ†ç±»ç‰¹å¾å¤„ç†å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“
- **æ€§èƒ½åŸºå‡†**: å»ºç«‹åŸºäºæœ€ä½³7ç‰¹å¾çš„æ€§èƒ½åŸºå‡†çº¿

### è¯„ä¼°æŒ‡æ ‡

#### è¶…å‚æ•°ä¼˜åŒ–ç›®æ ‡å‡½æ•°
- **ä¸»è¦æŒ‡æ ‡**: **BåŸŸAUC** (æ²³å—ç™Œç—‡åŒ»é™¢æ•°æ®é›†ä¸Šçš„AUC) - ç”¨äºè¶…å‚æ•°ä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°
- **ä¼˜åŒ–ç›®æ ‡**: æœ€å¤§åŒ–æ¨¡å‹åœ¨ç›®æ ‡åŸŸï¼ˆBåŸŸï¼‰ä¸Šçš„è·¨åŸŸæ³›åŒ–æ€§èƒ½
- **é‡è¦æ€§**: è¿™æ˜¯å®éªŒçš„æ ¸å¿ƒç›®æ ‡ï¼Œç›´æ¥åæ˜ æ¨¡å‹çš„å®é™…åº”ç”¨ä»·å€¼

#### å…¨é¢æ€§èƒ½è¯„ä¼°æŒ‡æ ‡
- **AåŸŸæŒ‡æ ‡**: åœ¨AI4Healthæ•°æ®ä¸Šçš„æ€§èƒ½ï¼ˆè®­ç»ƒé›†åŸºçº¿æ€§èƒ½ï¼‰
  - AUCã€å‡†ç¡®ç‡ã€F1åˆ†æ•°ã€ç±»åˆ«ç‰¹å¼‚æ€§å‡†ç¡®ç‡
- **BåŸŸæŒ‡æ ‡**: åœ¨æ²³å—ç™Œç—‡åŒ»é™¢æ•°æ®ä¸Šçš„æ€§èƒ½ï¼ˆç›®æ ‡åŸŸæ³›åŒ–æ€§èƒ½ï¼‰
  - AUCã€å‡†ç¡®ç‡ã€F1åˆ†æ•°ã€ç±»åˆ«ç‰¹å¼‚æ€§å‡†ç¡®ç‡
- **ç¨³å®šæ€§æŒ‡æ ‡**: é€šè¿‡10æŠ˜äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹åœ¨ä¸¤ä¸ªåŸŸä¸Šçš„æ€§èƒ½ç¨³å®šæ€§

## ğŸ“Š å®éªŒè®¾è®¡

### æ•°æ®é›†å®šä¹‰å’Œç‰¹ç‚¹

#### æ•°æ®é›†æ¦‚è§ˆ
- **æ•°æ®é›†A (AI4Health)**ï¼šæºåŸŸè®­ç»ƒæ•°æ®ï¼Œæ ‡å‡†åŒ–ç¨‹åº¦é«˜ï¼Œæ•°æ®è´¨é‡å¥½
- **æ•°æ®é›†B (æ²³å—ç™Œç—‡åŒ»é™¢)**ï¼šç›®æ ‡åŸŸæµ‹è¯•æ•°æ®ï¼Œä¸“ç§‘åŒ»é™¢ç‰¹è‰²ï¼Œåˆ†å¸ƒæœ‰å·®å¼‚
- **æ•°æ®é›†C (å¹¿å·åŒ»ç§‘å¤§å­¦)**ï¼šå¤‡é€‰ç›®æ ‡åŸŸï¼Œæ•™å­¦åŒ»é™¢ç‰¹ç‚¹ï¼Œæ•°æ®è§„èŒƒ

| æ•°æ®é›† | åŒ»é™¢ç±»å‹ | æ•°æ®ç‰¹ç‚¹ | æ ·æœ¬è§„æ¨¡ | ç‰¹å¾æ•°é‡ | æ–‡ä»¶è·¯å¾„ |
|--------|----------|----------|----------|----------|----------|
| A (AI4Health) | ç»¼åˆåŒ»é™¢ | æ ‡å‡†åŒ–ç¨‹åº¦é«˜ï¼Œæ•°æ®è´¨é‡å¥½ | ~1500 | 63ä¸ªåŸå§‹ç‰¹å¾ | `data/AI4healthcare.xlsx` |
| B (æ²³å—ç™Œç—‡åŒ»é™¢) | ä¸“ç§‘åŒ»é™¢ | ç™Œç—‡ä¸“ç§‘ç‰¹è‰²ï¼Œåˆ†å¸ƒæœ‰å·®å¼‚ | ~800 | 58ä¸ªåŸå§‹ç‰¹å¾ | `data/HenanCancerHospital_features63_58.xlsx` |
| C (å¹¿å·åŒ»ç§‘å¤§å­¦) | æ•™å­¦åŒ»é™¢ | æ•™å­¦åŒ»é™¢ç‰¹ç‚¹ï¼Œæ•°æ®è§„èŒƒ | ~600 | 58ä¸ªåŸå§‹ç‰¹å¾ | `data/GuangzhouMedicalHospital_features63_58.xlsx` |

### æ•°æ®é›†åˆ’åˆ†ç­–ç•¥

#### åˆ’åˆ†åŸç†
æœ¬å®éªŒé‡‡ç”¨**è·¨åŸŸéªŒè¯**ç­–ç•¥ï¼Œå³åœ¨æºåŸŸï¼ˆAåŸŸï¼‰ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œåœ¨ç›®æ ‡åŸŸï¼ˆBåŸŸï¼‰ä¸Šæµ‹è¯•æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™ç§åˆ’åˆ†æ–¹å¼æ¨¡æ‹Ÿäº†çœŸå®åŒ»ç–—åœºæ™¯ä¸­çš„è·¨åŒ»é™¢é¢„æµ‹éœ€æ±‚ã€‚

#### ä¸å…¶ä»–å®éªŒæ–¹æ³•çš„æ•°æ®åˆ’åˆ†å¯¹æ¯”

| å®éªŒç±»å‹ | è®­ç»ƒæ•°æ® | éªŒè¯æ•°æ® | æµ‹è¯•æ•°æ® | åˆ’åˆ†ç›®çš„ |
|----------|----------|----------|----------|----------|
| **å‚æ•°è°ƒä¼˜å®éªŒ** | AåŸŸå…¨éƒ¨ (10æŠ˜CV) | AåŸŸå†…éƒ¨éªŒè¯ | BåŸŸå…¨éƒ¨ (10æŠ˜CV) | è·¨åŸŸæ³›åŒ–æ€§èƒ½ä¼˜åŒ– |
| **å›ºå®šå‚æ•°åŸŸé€‚åº”** | AåŸŸ80% | AåŸŸ20% | BåŸŸ100% | å¿«é€ŸåŸŸé€‚åº”éªŒè¯ |
| **æ ‡å‡†åŸŸé€‚åº”** | AåŸŸ80% | BåŸŸ80% | BåŸŸ20% | åŸŸé€‚åº”å‚æ•°ä¼˜åŒ– |
| **ä¼ ç»Ÿæœºå™¨å­¦ä¹ ** | AåŸŸ80% | AåŸŸ20% | AåŸŸå†…éƒ¨ | å•åŸŸæ€§èƒ½ä¼˜åŒ– |

#### æ•°æ®åˆ’åˆ†çš„ä¼˜åŠ¿
1. **çœŸå®æ€§**: æ¨¡æ‹Ÿå®é™…è·¨åŒ»é™¢éƒ¨ç½²åœºæ™¯
2. **ä¸¥æ ¼æ€§**: å®Œå…¨ç‹¬ç«‹çš„æµ‹è¯•é›†ï¼Œé¿å…æ•°æ®æ³„éœ²
3. **ç¨³å®šæ€§**: ä½¿ç”¨äº¤å‰éªŒè¯å‡å°‘éšæœºæ€§å½±å“
4. **å¯æ¯”æ€§**: ä¸å…¶ä»–è·¨åŸŸå®éªŒä¿æŒä¸€è‡´çš„è¯„ä¼°æ ‡å‡†

#### å…·ä½“åˆ’åˆ†æ–¹æ¡ˆ

```mermaid
graph TD
    A["AI4Health æ•°æ®é›†<br/>(AåŸŸ - æºåŸŸ)"] --> B["è®­ç»ƒé›†è¯„ä¼°<br/>10æŠ˜äº¤å‰éªŒè¯"]
    A --> C["æœ€ç»ˆæ¨¡å‹è®­ç»ƒ<br/>ä½¿ç”¨å…¨éƒ¨AåŸŸæ•°æ®"]
    
    D["æ²³å—ç™Œç—‡åŒ»é™¢æ•°æ®é›†<br/>(BåŸŸ - ç›®æ ‡åŸŸ)"] --> E["å¤–éƒ¨æµ‹è¯•é›†<br/>10æŠ˜äº¤å‰éªŒè¯è¯„ä¼°"]
    
    C --> F["è·¨åŸŸé¢„æµ‹<br/>AåŸŸè®­ç»ƒ â†’ BåŸŸæµ‹è¯•"]
    E --> F
    
    F --> G["æ€§èƒ½è¯„ä¼°<br/>AUC, Accuracy, F1ç­‰"]
    
    style A fill:#e1f5fe
    style D fill:#fff3e0
    style F fill:#ffcdd2
    style G fill:#e8f5e8
```

#### æ•°æ®åˆ’åˆ†å®ç°

```python
def load_and_prepare_data():
    """æ•°æ®åŠ è½½å’Œé¢„å¤„ç†çš„å®Œæ•´æµç¨‹"""
    
    # 1. æ•°æ®åŠ è½½
    train_df = pd.read_excel("data/AI4healthcare.xlsx")
    external_df = pd.read_excel("data/HenanCancerHospital_features63_58.xlsx")
    
    # 2. ç‰¹å¾é€‰æ‹© (åŸºäºå‰æœŸRFEåˆ†æç¡®å®š)
    best_features = [
        'Feature63', 'Feature2', 'Feature46', 
        'Feature56', 'Feature42', 'Feature39', 'Feature43'
    ]
    
    # 3. æ•°æ®æå–
    X_train = train_df[best_features].copy()
    y_train = train_df["Label"].copy()
    X_external = external_df[best_features].copy()
    y_external = external_df["Label"].copy()
    
    # 4. æ•°æ®éªŒè¯
    print(f"è®­ç»ƒé›†å½¢çŠ¶: {X_train.shape}")
    print(f"å¤–éƒ¨æµ‹è¯•é›†å½¢çŠ¶: {X_external.shape}")
    print(f"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ:\n{y_train.value_counts()}")
    print(f"å¤–éƒ¨æ•°æ®é›†æ ‡ç­¾åˆ†å¸ƒ:\n{y_external.value_counts()}")
    
    return X_train, y_train, X_external, y_external

# å¯é€‰åˆ†ç±»ç‰¹å¾é…ç½®
categorical_features = ['Feature63', 'Feature46']
```

### æ•°æ®å¤„ç†æµç¨‹

#### å®Œæ•´æ•°æ®å¤„ç†ç®¡é“

```mermaid
graph TD
    A["åŸå§‹æ•°æ®åŠ è½½"] --> B["æ•°æ®éªŒè¯å’Œæ¸…æ´—"]
    B --> C["ç‰¹å¾é€‰æ‹©"]
    C --> D["ç¼ºå¤±å€¼å¤„ç†"]
    D --> E["æ•°æ®æ ‡å‡†åŒ–"]
    E --> F["åˆ†ç±»ç‰¹å¾å¤„ç†"]
    F --> G["æ•°æ®åˆ’åˆ†"]
    G --> H["æœ€ç»ˆæ•°æ®å‡†å¤‡"]
    
    style A fill:#e1f5fe
    style E fill:#fff3e0
    style F fill:#ffcdd2
    style H fill:#e8f5e8
```

#### 1. æ•°æ®åŠ è½½å’ŒéªŒè¯
```python
def load_and_validate_data():
    """æ•°æ®åŠ è½½å’ŒåŸºæœ¬éªŒè¯"""
    
    # åŠ è½½æ•°æ®æ–‡ä»¶
    try:
        train_df = pd.read_excel("data/AI4healthcare.xlsx")
        external_df = pd.read_excel("data/HenanCancerHospital_features63_58.xlsx")
        logging.info("æ•°æ®æ–‡ä»¶åŠ è½½æˆåŠŸ")
    except FileNotFoundError as e:
        logging.error(f"æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        raise
    
    # åŸºæœ¬æ•°æ®éªŒè¯
    assert 'Label' in train_df.columns, "è®­ç»ƒæ•°æ®ç¼ºå°‘Labelåˆ—"
    assert 'Label' in external_df.columns, "å¤–éƒ¨æ•°æ®ç¼ºå°‘Labelåˆ—"
    
    # æ£€æŸ¥ç‰¹å¾å®Œæ•´æ€§
    required_features = get_required_features()
    missing_features_train = set(required_features) - set(train_df.columns)
    missing_features_external = set(required_features) - set(external_df.columns)
    
    if missing_features_train:
        logging.warning(f"è®­ç»ƒæ•°æ®ç¼ºå°‘ç‰¹å¾: {missing_features_train}")
    if missing_features_external:
        logging.warning(f"å¤–éƒ¨æ•°æ®ç¼ºå°‘ç‰¹å¾: {missing_features_external}")
    
    return train_df, external_df
```

#### 2. ç‰¹å¾é€‰æ‹©å’Œå·¥ç¨‹
```python
def feature_selection_and_engineering(train_df, external_df, feature_type='best7'):
    """ç‰¹å¾é€‰æ‹©å’Œå·¥ç¨‹å¤„ç†"""
    
    if feature_type == 'best7':
        # ä½¿ç”¨ç»è¿‡RFEåˆ†æç¡®å®šçš„æœ€ä½³7ç‰¹å¾
        selected_features = [
            'Feature63', 'Feature2', 'Feature46', 
            'Feature56', 'Feature42', 'Feature39', 'Feature43'
        ]
        logging.info("ä½¿ç”¨æœ€ä½³7ç‰¹å¾é…ç½®")
    else:
        # ä½¿ç”¨æ‰€æœ‰å¯ç”¨ç‰¹å¾
        common_features = list(set(train_df.columns) & set(external_df.columns))
        selected_features = [f for f in common_features if f != 'Label']
        logging.info(f"ä½¿ç”¨å…¨éƒ¨{len(selected_features)}ä¸ªç‰¹å¾")
    
    # æå–ç‰¹å¾å’Œæ ‡ç­¾
    X_train = train_df[selected_features].copy()
    y_train = train_df["Label"].copy()
    X_external = external_df[selected_features].copy()
    y_external = external_df["Label"].copy()
    
    return X_train, y_train, X_external, y_external, selected_features
```

#### 3. ç¼ºå¤±å€¼å¤„ç†
```python
def handle_missing_values(X_train, X_external):
    """ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥"""
    
    # æ£€æŸ¥ç¼ºå¤±å€¼æƒ…å†µ
    train_missing = X_train.isnull().sum()
    external_missing = X_external.isnull().sum()
    
    if train_missing.sum() > 0:
        logging.warning(f"è®­ç»ƒæ•°æ®ç¼ºå¤±å€¼: {train_missing[train_missing > 0]}")
    if external_missing.sum() > 0:
        logging.warning(f"å¤–éƒ¨æ•°æ®ç¼ºå¤±å€¼: {external_missing[external_missing > 0]}")
    
    # ç¼ºå¤±å€¼å¡«å……ç­–ç•¥
    from sklearn.impute import SimpleImputer
    
    # æ•°å€¼ç‰¹å¾ç”¨ä¸­ä½æ•°å¡«å……
    numeric_features = X_train.select_dtypes(include=[np.number]).columns
    if len(numeric_features) > 0:
        numeric_imputer = SimpleImputer(strategy='median')
        X_train[numeric_features] = numeric_imputer.fit_transform(X_train[numeric_features])
        X_external[numeric_features] = numeric_imputer.transform(X_external[numeric_features])
    
    # åˆ†ç±»ç‰¹å¾ç”¨ä¼—æ•°å¡«å……
    categorical_features = X_train.select_dtypes(include=['object']).columns
    if len(categorical_features) > 0:
        categorical_imputer = SimpleImputer(strategy='most_frequent')
        X_train[categorical_features] = categorical_imputer.fit_transform(X_train[categorical_features])
        X_external[categorical_features] = categorical_imputer.transform(X_external[categorical_features])
    
    logging.info("ç¼ºå¤±å€¼å¤„ç†å®Œæˆ")
    return X_train, X_external
```

#### 4. æ•°æ®æ ‡å‡†åŒ–
```python
def apply_standardization(X_train, X_external):
    """æ•°æ®æ ‡å‡†åŒ–å¤„ç†"""
    
    # ä½¿ç”¨StandardScalerè¿›è¡Œæ ‡å‡†åŒ–
    # é‡è¦ï¼šåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆï¼Œç„¶ååº”ç”¨åˆ°æµ‹è¯•é›†
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_external_scaled = scaler.transform(X_external)
    
    logging.info("æ•°æ®æ ‡å‡†åŒ–å®Œæˆ")
    logging.info(f"è®­ç»ƒé›†æ ‡å‡†åŒ–åå½¢çŠ¶: {X_train_scaled.shape}")
    logging.info(f"å¤–éƒ¨æµ‹è¯•é›†æ ‡å‡†åŒ–åå½¢çŠ¶: {X_external_scaled.shape}")
    
    # éªŒè¯æ ‡å‡†åŒ–æ•ˆæœ
    train_mean = np.mean(X_train_scaled, axis=0)
    train_std = np.std(X_train_scaled, axis=0)
    logging.info(f"è®­ç»ƒé›†æ ‡å‡†åŒ–åå‡å€¼èŒƒå›´: [{train_mean.min():.6f}, {train_mean.max():.6f}]")
    logging.info(f"è®­ç»ƒé›†æ ‡å‡†åŒ–åæ ‡å‡†å·®èŒƒå›´: [{train_std.min():.6f}, {train_std.max():.6f}]")
    
    return X_train_scaled, X_external_scaled, scaler
```

#### 5. åˆ†ç±»ç‰¹å¾å¤„ç†
```python
def get_categorical_indices(all_features, categorical_features):
    """è·å–åˆ†ç±»ç‰¹å¾åœ¨ç‰¹å¾åˆ—è¡¨ä¸­çš„ç´¢å¼•ä½ç½®"""
    indices = []
    for cat_feature in categorical_features:
        if cat_feature in all_features:
            indices.append(all_features.index(cat_feature))
            logging.info(f"åˆ†ç±»ç‰¹å¾ {cat_feature} ä½äºç´¢å¼• {len(indices)-1}")
    
    if indices:
        logging.info(f"å…±è¯†åˆ«åˆ° {len(indices)} ä¸ªåˆ†ç±»ç‰¹å¾: {categorical_features}")
    else:
        logging.info("æœªä½¿ç”¨åˆ†ç±»ç‰¹å¾")
    
    return indices

def prepare_categorical_features(features, use_categorical=True):
    """å‡†å¤‡åˆ†ç±»ç‰¹å¾é…ç½®"""
    
    # é¢„å®šä¹‰çš„åˆ†ç±»ç‰¹å¾
    predefined_categorical = ['Feature63', 'Feature46']
    
    if use_categorical:
        # åªä¿ç•™å®é™…å­˜åœ¨çš„åˆ†ç±»ç‰¹å¾
        categorical_features = [f for f in predefined_categorical if f in features]
        categorical_indices = get_categorical_indices(features, categorical_features)
    else:
        categorical_features = []
        categorical_indices = []
    
    return categorical_features, categorical_indices
```

### äº¤å‰éªŒè¯ç­–ç•¥

#### è®­ç»ƒé›†äº¤å‰éªŒè¯
```python
def perform_cross_validation(X_train_scaled, y_train, n_folds=10):
    """åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œäº¤å‰éªŒè¯è¯„ä¼°"""
    
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)
    cv_scores = []
    
    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_scaled), 1):
        X_fold_train, X_fold_val = X_train_scaled[train_idx], X_train_scaled[val_idx]
        y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]
        
        # è®­ç»ƒæ¨¡å‹
        model = AutoTabPFNClassifier(device='cuda', max_time=2, random_state=42)
        model.fit(X_fold_train, y_fold_train)
        
        # è¯„ä¼°
        y_val_pred = model.predict(X_fold_val)
        y_val_proba = model.predict_proba(X_fold_val)
        
        # è®¡ç®—æŒ‡æ ‡
        fold_acc = accuracy_score(y_fold_val, y_val_pred)
        fold_auc = roc_auc_score(y_fold_val, y_val_proba[:, 1])
        fold_f1 = f1_score(y_fold_val, y_val_pred)
        
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        conf_matrix = confusion_matrix(y_fold_val, y_val_pred)
        fold_acc_0 = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])
        fold_acc_1 = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])
        
        cv_scores.append({
            'fold': fold,
            'accuracy': fold_acc,
            'auc': fold_auc,
            'f1': fold_f1,
            'acc_0': fold_acc_0,
            'acc_1': fold_acc_1
        })
        
        logging.info(f"Fold {fold}: AUC={fold_auc:.4f}, Acc={fold_acc:.4f}")
    
    return cv_scores
```

#### å¤–éƒ¨æ•°æ®é›†äº¤å‰éªŒè¯
```python
def evaluate_model_on_external(model, X_external, y_external, n_folds=10):
    """ä½¿ç”¨KæŠ˜äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹åœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šçš„æ€§èƒ½"""
    
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)
    fold_results = []
    all_preds = []
    all_probs = []
    all_true = []
    
    for fold, (_, test_idx) in enumerate(kf.split(X_external), 1):
        X_test_fold = X_external[test_idx]
        y_test_fold = y_external.iloc[test_idx]
        
        # é¢„æµ‹
        y_pred = model.predict(X_test_fold)
        y_proba = model.predict_proba(X_test_fold)
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        all_preds.extend(y_pred)
        all_probs.extend(y_proba[:, 1])
        all_true.extend(y_test_fold)
        
        # è®¡ç®—æŠ˜çº§æŒ‡æ ‡
        fold_acc = accuracy_score(y_test_fold, y_pred)
        fold_auc = roc_auc_score(y_test_fold, y_proba[:, 1])
        fold_f1 = f1_score(y_test_fold, y_pred)
        
        fold_results.append({
            'fold': fold,
            'accuracy': fold_acc,
            'auc': fold_auc,
            'f1': fold_f1
        })
    
    # è®¡ç®—æ•´ä½“æŒ‡æ ‡
    overall_acc = accuracy_score(all_true, all_preds)
    overall_auc = roc_auc_score(all_true, all_probs)
    overall_f1 = f1_score(all_true, all_preds)
    
    return {
        'fold_results': fold_results,
        'overall': {
            'accuracy': overall_acc,
            'auc': overall_auc,
            'f1': overall_f1
        }
    }
```

### æ•°æ®è´¨é‡æ§åˆ¶

#### æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
```python
def validate_data_consistency(X_train, X_external, features):
    """éªŒè¯è®­ç»ƒé›†å’Œå¤–éƒ¨æµ‹è¯•é›†çš„æ•°æ®ä¸€è‡´æ€§"""
    
    # 1. ç‰¹å¾åç§°ä¸€è‡´æ€§
    train_features = set(X_train.columns)
    external_features = set(X_external.columns)
    
    if train_features != external_features:
        missing_in_external = train_features - external_features
        missing_in_train = external_features - train_features
        
        if missing_in_external:
            logging.warning(f"å¤–éƒ¨æ•°æ®ç¼ºå°‘ç‰¹å¾: {missing_in_external}")
        if missing_in_train:
            logging.warning(f"è®­ç»ƒæ•°æ®ç¼ºå°‘ç‰¹å¾: {missing_in_train}")
    
    # 2. æ•°æ®ç±»å‹ä¸€è‡´æ€§
    for feature in features:
        if feature in X_train.columns and feature in X_external.columns:
            train_dtype = X_train[feature].dtype
            external_dtype = X_external[feature].dtype
            
            if train_dtype != external_dtype:
                logging.warning(f"ç‰¹å¾ {feature} æ•°æ®ç±»å‹ä¸ä¸€è‡´: "
                              f"è®­ç»ƒé›†={train_dtype}, å¤–éƒ¨={external_dtype}")
    
    # 3. æ•°å€¼èŒƒå›´æ£€æŸ¥
    for feature in features:
        if feature in X_train.columns and feature in X_external.columns:
            train_range = (X_train[feature].min(), X_train[feature].max())
            external_range = (X_external[feature].min(), X_external[feature].max())
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ˜¾è‘—çš„åˆ†å¸ƒå·®å¼‚
            if (external_range[0] < train_range[0] * 0.5 or 
                external_range[1] > train_range[1] * 2.0):
                logging.warning(f"ç‰¹å¾ {feature} æ•°å€¼èŒƒå›´å·®å¼‚è¾ƒå¤§: "
                              f"è®­ç»ƒé›†={train_range}, å¤–éƒ¨={external_range}")
    
    logging.info("æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å®Œæˆ")
```

#### æ ‡ç­¾åˆ†å¸ƒåˆ†æ
```python
def analyze_label_distribution(y_train, y_external):
    """åˆ†æè®­ç»ƒé›†å’Œå¤–éƒ¨æµ‹è¯•é›†çš„æ ‡ç­¾åˆ†å¸ƒ"""
    
    # è®¡ç®—æ ‡ç­¾åˆ†å¸ƒ
    train_dist = y_train.value_counts(normalize=True).sort_index()
    external_dist = y_external.value_counts(normalize=True).sort_index()
    
    logging.info("æ ‡ç­¾åˆ†å¸ƒåˆ†æ:")
    logging.info(f"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {train_dist.to_dict()}")
    logging.info(f"å¤–éƒ¨æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ: {external_dist.to_dict()}")
    
    # è®¡ç®—åˆ†å¸ƒå·®å¼‚
    if len(train_dist) == len(external_dist):
        distribution_diff = abs(train_dist - external_dist).sum()
        logging.info(f"æ ‡ç­¾åˆ†å¸ƒå·®å¼‚ (æ€»å˜å·®è·ç¦»): {distribution_diff:.4f}")
        
        if distribution_diff > 0.2:
            logging.warning("è®­ç»ƒé›†å’Œå¤–éƒ¨æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒå·®å¼‚è¾ƒå¤§ï¼Œå¯èƒ½å½±å“æ¨¡å‹æ€§èƒ½")
        elif distribution_diff > 0.1:
            logging.info("è®­ç»ƒé›†å’Œå¤–éƒ¨æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒå­˜åœ¨ä¸­ç­‰å·®å¼‚")
        else:
            logging.info("è®­ç»ƒé›†å’Œå¤–éƒ¨æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒç›¸å¯¹ä¸€è‡´")
    
    return train_dist, external_dist
```

### å®éªŒæµç¨‹

#### 1. æ•°æ®é¢„å¤„ç†é˜¶æ®µ
```python
def complete_data_preprocessing():
    """å®Œæ•´çš„æ•°æ®é¢„å¤„ç†æµç¨‹"""
    
    # 1. æ•°æ®åŠ è½½å’ŒéªŒè¯
    train_df, external_df = load_and_validate_data()
    
    # 2. ç‰¹å¾é€‰æ‹©å’Œå·¥ç¨‹
    X_train, y_train, X_external, y_external, features = feature_selection_and_engineering(
        train_df, external_df, feature_type='best7'
    )
    
    # 3. æ•°æ®è´¨é‡æ§åˆ¶
    validate_data_consistency(X_train, X_external, features)
    analyze_label_distribution(y_train, y_external)
    
    # 4. ç¼ºå¤±å€¼å¤„ç†
    X_train, X_external = handle_missing_values(X_train, X_external)
    
    # 5. æ•°æ®æ ‡å‡†åŒ–
    X_train_scaled, X_external_scaled, scaler = apply_standardization(X_train, X_external)
    
    # 6. åˆ†ç±»ç‰¹å¾å¤„ç†
    categorical_features, categorical_indices = prepare_categorical_features(
        features, use_categorical=True
    )
    
    logging.info("æ•°æ®é¢„å¤„ç†å®Œæˆ")
    return {
        'X_train_scaled': X_train_scaled,
        'y_train': y_train,
        'X_external_scaled': X_external_scaled,
        'y_external': y_external,
        'features': features,
        'categorical_features': categorical_features,
        'categorical_indices': categorical_indices,
        'scaler': scaler
    }
```

#### 2. åŸºçº¿æ€§èƒ½è¯„ä¼°
åœ¨è¿›è¡Œå‚æ•°ä¼˜åŒ–å‰ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°è¿›è¡Œ10æŠ˜äº¤å‰éªŒè¯ï¼Œå»ºç«‹æ€§èƒ½åŸºçº¿ï¼š

```python
# 10æŠ˜äº¤å‰éªŒè¯é…ç½®
kf = KFold(n_splits=10, shuffle=True, random_state=42)

# é»˜è®¤æ¨¡å‹é…ç½®
model = AutoTabPFNClassifier(device='cuda', max_time=2, random_state=42)
```

#### 3. è¶…å‚æ•°ä¼˜åŒ–é˜¶æ®µ

##### è´å¶æ–¯ä¼˜åŒ– (æ¨èæ–¹æ³•)
å½“ `scikit-optimize` å¯ç”¨æ—¶ï¼Œä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–è¿›è¡Œæ™ºèƒ½å‚æ•°æœç´¢ï¼š

```python
# å‚æ•°æœç´¢ç©ºé—´ (åŸºäºå‰æœŸå®éªŒç»“æœè°ƒæ•´)
param_space = {
    'use_categorical': Categorical([True, False]),
    'max_time': Integer(15, 60),              # è®­ç»ƒæ—¶é—´é™åˆ¶
    'preset': Categorical(['default', 'avoid_overfitting']),
    'ges_scoring': Categorical(['f1', 'roc', 'accuracy']),
    'max_models': Categorical([5, 10, 15, 20, 25]),
    'validation_method': Categorical(['holdout', 'cv']),
    'n_repeats': Integer(100, 200),
    'n_folds': Categorical([5, 10]),
    'holdout_fraction': Real(0.3, 0.5),
    'ges_n_iterations': Integer(15, 25),
    'ignore_limits': Categorical([True, False])
}
```

##### éšæœºæœç´¢ (å¤‡é€‰æ–¹æ³•)
å½“è´å¶æ–¯ä¼˜åŒ–ä¸å¯ç”¨æ—¶ï¼Œä½¿ç”¨éšæœºæœç´¢ï¼š

```python
# å‚æ•°ç½‘æ ¼
param_grid = {
    'max_time': [15, 30, 45, 60],
    'preset': ['default', 'avoid_overfitting'],
    'ges_scoring': ['f1', 'roc', 'accuracy'],
    'max_models': [5, 10, 15, 20, 25],
    'validation_method': ['holdout', 'cv'],
    'n_repeats': [100, 150, 200],
    'n_folds': [5, 10],
    'holdout_fraction': [0.3, 0.4, 0.5],
    'ges_n_iterations': [15, 20, 25],
    'ignore_limits': [True, False]
}
```

#### 4. æ¨¡å‹è¯„ä¼°ç­–ç•¥

##### è®­ç»ƒé›†è¯„ä¼°
- ä½¿ç”¨å®Œæ•´è®­ç»ƒé›†è®­ç»ƒæœ€ç»ˆæ¨¡å‹
- è®¡ç®—è®­ç»ƒé›†ä¸Šçš„æ€§èƒ½æŒ‡æ ‡
- è®°å½•è®­ç»ƒæ—¶é—´

##### å¤–éƒ¨æ•°æ®é›†è¯„ä¼°
ä½¿ç”¨10æŠ˜äº¤å‰éªŒè¯åœ¨å¤–éƒ¨æ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼š

```python
def evaluate_model_on_external(model, X_external, y_external, n_folds=10):
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)
    
    fold_results = []
    for fold, (_, test_idx) in enumerate(kf.split(X_external), 1):
        X_test_fold = X_external[test_idx]
        y_test_fold = y_external.iloc[test_idx]
        
        # é¢„æµ‹å’Œè¯„ä¼°
        y_pred = model.predict(X_test_fold)
        y_proba = model.predict_proba(X_test_fold)
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        fold_results.append({
            'accuracy': accuracy_score(y_test_fold, y_pred),
            'auc': roc_auc_score(y_test_fold, y_proba[:, 1]),
            'f1': f1_score(y_test_fold, y_pred)
        })
    
    return fold_results
```

## ğŸ”§ æ ¸å¿ƒç®—æ³•å®ç°

### å‚æ•°ä¼˜åŒ–ç›®æ ‡å‡½æ•°
```python
def train_and_evaluate_model(X_train, y_train, X_external, y_external, 
                           params, args, use_categorical=True):
    # 1. é…ç½®åˆ†ç±»ç‰¹å¾ç´¢å¼•
    categorical_indices = get_categorical_indices(
        args.features, args.categorical_features
    ) if use_categorical else []
    
    # 2. åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = AutoTabPFNClassifier(
        max_time=params.get('max_time', 60),
        preset=params.get('preset', 'default'),
        ges_scoring_string=params.get('ges_scoring', 'roc'),
        device=args.device,
        random_state=args.random_state,
        ignore_pretraining_limits=params.get('ignore_limits', False),
        categorical_feature_indices=categorical_indices,
        phe_init_args=phe_init_args
    )
    
    # 3. è®­ç»ƒæ¨¡å‹
    start_time = time.time()
    model.fit(X_train, y_train)
    train_time = time.time() - start_time
    
    # 4. è¯„ä¼°æ€§èƒ½
    evaluation_results = evaluate_model_on_external(
        model, X_external, y_external, n_folds=10
    )
    
    return evaluation_results, evaluation_results['overall']['auc']
```

### è´å¶æ–¯ä¼˜åŒ–å®ç°
```python
def optimize_with_bayesian(X_train, y_train, X_external, y_external, args):
    # å®šä¹‰ç›®æ ‡å‡½æ•°
    @use_named_args(dimensions=dimensions)
    def objective(**params):
        use_categorical = params.pop('use_categorical', False)
        
        try:
            _, auc = train_and_evaluate_model(
                X_train, y_train, X_external, y_external, 
                params, args, use_categorical
            )
            return -auc  # æœ€å°åŒ–è´ŸAUC
        except Exception as e:
            logging.error(f"è¯„ä¼°æ—¶å‡ºé”™: {str(e)}")
            return 0.0
    
    # è¿è¡Œè´å¶æ–¯ä¼˜åŒ–
    result = gp_minimize(
        objective,
        dimensions=dimensions,
        n_calls=args.n_trials,
        random_state=args.random_state,
        verbose=True
    )
    
    return result
```

## ğŸ“‹ ä½¿ç”¨æ–¹æ³•

### å‘½ä»¤è¡Œå‚æ•°

#### åŸºæœ¬å‚æ•°
```bash
# ä½¿ç”¨é»˜è®¤é…ç½®è¿è¡Œ
python predict_healthcare_auto_external_adjust_parameter.py

# æŒ‡å®šè¾“å‡ºç›®å½•
python predict_healthcare_auto_external_adjust_parameter.py \
    --output_dir ./my_results

# ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–
python predict_healthcare_auto_external_adjust_parameter.py \
    --use_bayesian \
    --n_trials 100
```

#### é«˜çº§é…ç½®
```bash
# è‡ªå®šä¹‰ç‰¹å¾å’Œåˆ†ç±»ç‰¹å¾
python predict_healthcare_auto_external_adjust_parameter.py \
    --features Feature63 Feature2 Feature46 Feature56 \
    --categorical_features Feature63 Feature46 \
    --n_trials 200 \
    --n_cv_folds 5

# ä½¿ç”¨CPUè®¾å¤‡
python predict_healthcare_auto_external_adjust_parameter.py \
    --device cpu \
    --random_state 123
```

### å®Œæ•´å‚æ•°åˆ—è¡¨
```python
parser.add_argument('--output_dir', type=str, default='./results_hyperopt_best7',
                    help='è¾“å‡ºç»“æœçš„ç›®å½•')
parser.add_argument('--features', type=str, nargs='+', 
                    default=['Feature63', 'Feature2', 'Feature46', 
                            'Feature56', 'Feature42', 'Feature39', 'Feature43'],
                    help='ç”¨äºé¢„æµ‹çš„ç‰¹å¾åˆ—è¡¨')
parser.add_argument('--categorical_features', type=str, nargs='+',
                    default=['Feature63', 'Feature46'],
                    help='å¯é€‰çš„åˆ†ç±»ç‰¹å¾åˆ—è¡¨')
parser.add_argument('--device', type=str, default='cuda', 
                    choices=['cpu', 'cuda'], help='è®¡ç®—è®¾å¤‡')
parser.add_argument('--random_state', type=int, default=42,
                    help='éšæœºç§å­ï¼Œç”¨äºå¯é‡å¤æ€§')
parser.add_argument('--n_trials', type=int, default=300,
                    help='è¶…å‚æ•°ç»„åˆå°è¯•æ¬¡æ•°')
parser.add_argument('--n_cv_folds', type=int, default=10,
                    help='äº¤å‰éªŒè¯æŠ˜æ•°')
parser.add_argument('--use_bayesian', action='store_true',
                    help='æ˜¯å¦ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–(éœ€è¦scikit-optimize)')
```

## ğŸ“Š è¾“å‡ºç»“æœ

### ç›®å½•ç»“æ„
```
results_hyperopt_best7/
â”œâ”€â”€ all_trial_results.json              # æ‰€æœ‰è¯•éªŒçš„è¯¦ç»†ç»“æœ
â”œâ”€â”€ best_params.json                    # æœ€ä½³å‚æ•°é…ç½®
â”œâ”€â”€ best_result.json                    # æœ€ä½³æ¨¡å‹çš„è¯¦ç»†è¯„ä¼°ç»“æœ
â”œâ”€â”€ best_model_summary.txt              # äººç±»å¯è¯»çš„ç»“æœæ‘˜è¦
â””â”€â”€ best_model_confusion_matrix.png     # æ··æ·†çŸ©é˜µå¯è§†åŒ–
```

### best_params.json ç»“æ„
```json
{
  "model_params": {
    "max_time": 30,
    "preset": "default",
    "ges_scoring": "f1",
    "max_models": 10,
    "validation_method": "holdout",
    "n_repeats": 150,
    "n_folds": 10,
    "holdout_fraction": 0.5,
    "ges_n_iterations": 20,
    "ignore_limits": false
  },
  "use_categorical": true
}
```

### best_result.json ç»“æ„
```json
{
  "train_metrics": {
    "accuracy": 0.8234,
    "auc": 0.8567,
    "f1": 0.8123,
    "acc_0": 0.8456,
    "acc_1": 0.8012,
    "confusion_matrix": [[45, 8], [12, 49]]
  },
  "overall": {
    "accuracy": 0.7856,
    "auc": 0.8234,
    "f1": 0.7723,
    "acc_0": 0.8012,
    "acc_1": 0.7645,
    "confusion_matrix": [[123, 23], [34, 145]]
  },
  "means": {
    "accuracy": 0.7856,
    "auc": 0.8234,
    "f1": 0.7723,
    "acc_0": 0.8012,
    "acc_1": 0.7645
  },
  "stds": {
    "accuracy": 0.0234,
    "auc": 0.0156,
    "f1": 0.0198,
    "acc_0": 0.0267,
    "acc_1": 0.0289
  },
  "fold_results": [
    {
      "fold": 1,
      "accuracy": 0.7823,
      "auc": 0.8156,
      "f1": 0.7645,
      "acc_0": 0.7934,
      "acc_1": 0.7712
    }
  ],
  "train_time": 45.67
}
```

### best_model_summary.txt ç¤ºä¾‹
```
æœ€ä½³æ¨¡å‹å‚æ•°å’Œç»“æœ (åŸºäºå¤–éƒ¨æµ‹è¯•é›†AUC)
==================================================

æ•°æ®é›†ç‰¹å¾:
1. Feature63 (åˆ†ç±»)
2. Feature2
3. Feature46 (åˆ†ç±»)
4. Feature56
5. Feature42
6. Feature39
7. Feature43

æœ€ä½³å‚æ•°:
------------------------------
max_time: 30
preset: default
ges_scoring: f1
max_models: 10
validation_method: holdout
n_repeats: 150
n_folds: 10
holdout_fraction: 0.5
ges_n_iterations: 20
ignore_limits: false

ä½¿ç”¨åˆ†ç±»ç‰¹å¾å¤„ç†:
æ˜¯ - ä½¿ç”¨ä»¥ä¸‹ç‰¹å¾ä½œä¸ºåˆ†ç±»ç‰¹å¾: ['Feature63', 'Feature46']

è®­ç»ƒé›†æ€§èƒ½æŒ‡æ ‡:
------------------------------
å‡†ç¡®ç‡: 0.8234
AUC: 0.8567
F1åˆ†æ•°: 0.8123
ç±»åˆ«0å‡†ç¡®ç‡: 0.8456
ç±»åˆ«1å‡†ç¡®ç‡: 0.8012
è®­ç»ƒæ—¶é—´: 45.67 ç§’

å¤–éƒ¨æµ‹è¯•é›†æ€§èƒ½æŒ‡æ ‡:
------------------------------
å‡†ç¡®ç‡: 0.7856 (Â±0.0234)
AUC: 0.8234 (Â±0.0156)
F1åˆ†æ•°: 0.7723 (Â±0.0198)
ç±»åˆ«0å‡†ç¡®ç‡: 0.8012 (Â±0.0267)
ç±»åˆ«1å‡†ç¡®ç‡: 0.7645 (Â±0.0289)

å¤–éƒ¨æµ‹è¯•é›†æ··æ·†çŸ©é˜µ:
[[123  23]
 [ 34 145]]
```

## ğŸ” å…³é”®æŠ€æœ¯ç»†èŠ‚

### 1. åˆ†ç±»ç‰¹å¾å¤„ç†
```python
def get_categorical_indices(all_features, categorical_features):
    """è·å–åˆ†ç±»ç‰¹å¾åœ¨ç‰¹å¾åˆ—è¡¨ä¸­çš„ç´¢å¼•ä½ç½®"""
    indices = []
    for cat_feature in categorical_features:
        if cat_feature in all_features:
            indices.append(all_features.index(cat_feature))
    return indices
```

### 2. æ•°æ®æ ‡å‡†åŒ–ç­–ç•¥
- åœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆ StandardScaler
- å°†ç›¸åŒçš„ç¼©æ”¾å‚æ•°åº”ç”¨åˆ°å¤–éƒ¨æµ‹è¯•é›†
- ç¡®ä¿æ•°æ®åˆ†å¸ƒçš„ä¸€è‡´æ€§

### 3. äº¤å‰éªŒè¯ç­–ç•¥
- è®­ç»ƒé›†: 10æŠ˜äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹ç¨³å®šæ€§
- å¤–éƒ¨æµ‹è¯•é›†: 10æŠ˜äº¤å‰éªŒè¯è¯„ä¼°æ³›åŒ–èƒ½åŠ›
- ä½¿ç”¨ç›¸åŒçš„éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§

### 4. é”™è¯¯å¤„ç†æœºåˆ¶
```python
try:
    _, auc = train_and_evaluate_model(...)
    return -auc  # è´å¶æ–¯ä¼˜åŒ–æœ€å°åŒ–ç›®æ ‡
except Exception as e:
    logging.error(f"è¯„ä¼°æ—¶å‡ºé”™: {str(e)}")
    return 0.0  # è¿”å›æœ€å·®åˆ†æ•°
```

## ğŸ“ˆ å®éªŒç»“æœè§£è¯»

### æ€§èƒ½æŒ‡æ ‡è§£é‡Š

#### AåŸŸï¼ˆè®­ç»ƒé›†ï¼‰æ€§èƒ½æŒ‡æ ‡
- **AåŸŸAUC**: æ¨¡å‹åœ¨æºåŸŸä¸Šçš„åŒºåˆ†èƒ½åŠ›ï¼Œåæ˜ æ¨¡å‹è´¨é‡ä¸Šç•Œ
- **AåŸŸå‡†ç¡®ç‡**: åœ¨è®­ç»ƒæ•°æ®åˆ†å¸ƒä¸Šçš„æ•´ä½“é¢„æµ‹æ­£ç¡®ç‡
- **AåŸŸF1åˆ†æ•°**: åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„ç²¾ç¡®ç‡å’Œå¬å›ç‡å¹³è¡¡
- **ç”¨é€”**: è¯„ä¼°æ¨¡å‹æ˜¯å¦å……åˆ†å­¦ä¹ äº†æºåŸŸçš„æ¨¡å¼

#### BåŸŸï¼ˆç›®æ ‡åŸŸï¼‰æ€§èƒ½æŒ‡æ ‡ - **æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡**
- **BåŸŸAUC**: **ä¸»è¦ä¼˜åŒ–ç›®æ ‡**ï¼Œåæ˜ æ¨¡å‹è·¨åŸŸæ³›åŒ–èƒ½åŠ›
- **BåŸŸå‡†ç¡®ç‡**: åœ¨ç›®æ ‡åŒ»é™¢æ•°æ®ä¸Šçš„æ•´ä½“é¢„æµ‹æ­£ç¡®ç‡
- **BåŸŸF1åˆ†æ•°**: åœ¨ç›®æ ‡åŸŸä¸Šçš„ç²¾ç¡®ç‡å’Œå¬å›ç‡å¹³è¡¡
- **BåŸŸç±»åˆ«ç‰¹å¼‚æ€§å‡†ç¡®ç‡**: å„ç±»åˆ«åœ¨ç›®æ ‡åŸŸä¸Šçš„é¢„æµ‹å‡†ç¡®ç‡
- **ç”¨é€”**: è¯„ä¼°æ¨¡å‹åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­çš„æ€§èƒ½

#### ç¨³å®šæ€§æŒ‡æ ‡
- **æ ‡å‡†å·®**: åæ˜ æ¨¡å‹åœ¨10æŠ˜äº¤å‰éªŒè¯ä¸­çš„æ€§èƒ½ç¨³å®šæ€§
- **å˜å¼‚ç³»æ•°**: æ ‡å‡†å·®ä¸å‡å€¼çš„æ¯”å€¼ï¼Œè¯„ä¼°ç›¸å¯¹ç¨³å®šæ€§

### æŒ‡æ ‡ä¼˜å…ˆçº§å’Œç”¨é€”æ€»ç»“

| æŒ‡æ ‡ç±»å‹ | æ•°æ®åŸŸ | æŒ‡æ ‡åç§° | ä¼˜å…ˆçº§ | ä¸»è¦ç”¨é€” |
|----------|--------|----------|--------|----------|
| **ä¼˜åŒ–ç›®æ ‡** | BåŸŸ | AUC | â­â­â­â­â­ | è¶…å‚æ•°ä¼˜åŒ–çš„ç›®æ ‡å‡½æ•° |
| **æ ¸å¿ƒè¯„ä¼°** | BåŸŸ | å‡†ç¡®ç‡ã€F1 | â­â­â­â­ | è¯„ä¼°å®é™…åº”ç”¨æ€§èƒ½ |
| **åŸºçº¿å‚è€ƒ** | AåŸŸ | AUCã€å‡†ç¡®ç‡ | â­â­â­ | è¯„ä¼°æ¨¡å‹å­¦ä¹ è´¨é‡ |
| **ç¨³å®šæ€§** | AåŸŸ+BåŸŸ | æ ‡å‡†å·® | â­â­â­ | è¯„ä¼°æ¨¡å‹å¯é æ€§ |
| **ç»†èŠ‚åˆ†æ** | BåŸŸ | ç±»åˆ«ç‰¹å¼‚æ€§å‡†ç¡®ç‡ | â­â­ | åˆ†æå„ç±»åˆ«æ€§èƒ½ |

**å…³é”®ç‚¹**: 
- ğŸ¯ **BåŸŸAUCæ˜¯å”¯ä¸€çš„ä¼˜åŒ–ç›®æ ‡**ï¼Œæ‰€æœ‰å‚æ•°è°ƒä¼˜éƒ½ä»¥æ­¤ä¸ºå‡†
- ğŸ“Š **AåŸŸæŒ‡æ ‡ç”¨äºå‚è€ƒ**ï¼Œç¡®ä¿æ¨¡å‹åœ¨æºåŸŸä¸Šå­¦ä¹ å……åˆ†
- ğŸ”„ **ç¨³å®šæ€§æŒ‡æ ‡**ç¡®ä¿ç»“æœå¯é ï¼Œé¿å…è¿‡æ‹Ÿåˆåˆ°ç‰¹å®šæ•°æ®åˆ’åˆ†

### å‚æ•°é‡è¦æ€§åˆ†æ
æ ¹æ®å®éªŒç»“æœï¼Œå…³é”®å‚æ•°çš„å½±å“ï¼š
1. **max_time**: å½±å“æ¨¡å‹å¤æ‚åº¦å’Œè®­ç»ƒè´¨é‡
2. **ges_scoring**: å½±å“æ¨¡å‹é€‰æ‹©ç­–ç•¥
3. **max_models**: å½±å“é›†æˆæ•ˆæœ
4. **use_categorical**: å½±å“ç‰¹å¾è¡¨ç¤ºè´¨é‡

### æœ€ä½³å®è·µå»ºè®®
1. **ä¼˜å…ˆä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–**: æ›´é«˜æ•ˆçš„å‚æ•°æœç´¢
2. **åˆç†è®¾ç½®è¯•éªŒæ¬¡æ•°**: å¹³è¡¡æœç´¢è´¨é‡å’Œè®¡ç®—æˆæœ¬
3. **å…³æ³¨AUCå’Œç¨³å®šæ€§**: ä¸ä»…çœ‹å¹³å‡æ€§èƒ½ï¼Œä¹Ÿè¦çœ‹æ ‡å‡†å·®
4. **éªŒè¯åˆ†ç±»ç‰¹å¾æ•ˆæœ**: å¯¹æ¯”ä½¿ç”¨å’Œä¸ä½¿ç”¨åˆ†ç±»ç‰¹å¾çš„ç»“æœ

## ğŸš€ æ‰©å±•å’Œæ”¹è¿›

### å¯èƒ½çš„æ‰©å±•æ–¹å‘
1. **å¤šç›®æ ‡ä¼˜åŒ–**: åŒæ—¶ä¼˜åŒ–AUCã€F1å’Œç¨³å®šæ€§
2. **æ—©åœæœºåˆ¶**: åŸºäºéªŒè¯æ€§èƒ½çš„æ—©åœç­–ç•¥
3. **é›†æˆæ–¹æ³•**: å¤šä¸ªæœ€ä½³æ¨¡å‹çš„é›†æˆ
4. **ç‰¹å¾é€‰æ‹©é›†æˆ**: å°†ç‰¹å¾é€‰æ‹©çº³å…¥ä¼˜åŒ–è¿‡ç¨‹

### æ€§èƒ½ä¼˜åŒ–å»ºè®®
1. **å¹¶è¡ŒåŒ–**: åˆ©ç”¨å¤šGPUæˆ–å¤šè¿›ç¨‹åŠ é€Ÿ
2. **ç¼“å­˜æœºåˆ¶**: ç¼“å­˜é‡å¤çš„æ¨¡å‹è®­ç»ƒç»“æœ
3. **å¢é‡ä¼˜åŒ–**: åŸºäºå†å²ç»“æœçš„å¢é‡æœç´¢
4. **è‡ªé€‚åº”æœç´¢**: æ ¹æ®æœç´¢å†å²è°ƒæ•´æœç´¢ç­–ç•¥

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [AutoTabPFN å®˜æ–¹æ–‡æ¡£](https://github.com/automl/TabPFN)
- [scikit-optimize æ–‡æ¡£](https://scikit-optimize.github.io/stable/)
- [åŒ»ç–—æ•°æ®é¢„å¤„ç†æŒ‡å—](../preprocessing/medical_data_guide.md)
- [è·¨åŸŸéªŒè¯æœ€ä½³å®è·µ](../evaluation/cross_domain_validation.md)

## ğŸ”— ä¾èµ–é¡¹

### å¿…éœ€ä¾èµ–
```bash
pip install numpy pandas scikit-learn matplotlib
pip install tabpfn-extensions  # AutoTabPFN
```

### å¯é€‰ä¾èµ– (è´å¶æ–¯ä¼˜åŒ–)
```bash
pip install scikit-optimize
```

### ç¡¬ä»¶è¦æ±‚
- **GPU**: æ¨èä½¿ç”¨CUDAå…¼å®¹GPUåŠ é€Ÿè®­ç»ƒ
- **å†…å­˜**: è‡³å°‘8GB RAM
- **å­˜å‚¨**: è‡³å°‘2GBå¯ç”¨ç©ºé—´ç”¨äºç»“æœå­˜å‚¨

## ğŸ“‹ æ•°æ®å¤„ç†æœ€ä½³å®è·µ

### 1. æ•°æ®é¢„å¤„ç†æ£€æŸ¥æ¸…å•
- [ ] **æ•°æ®æ–‡ä»¶å®Œæ•´æ€§**: ç¡®è®¤æ‰€æœ‰å¿…éœ€çš„æ•°æ®æ–‡ä»¶å­˜åœ¨ä¸”å¯è¯»
- [ ] **ç‰¹å¾ä¸€è‡´æ€§**: éªŒè¯è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç‰¹å¾åç§°å’Œæ•°é‡ä¸€è‡´
- [ ] **æ•°æ®ç±»å‹**: æ£€æŸ¥æ•°å€¼ç‰¹å¾å’Œåˆ†ç±»ç‰¹å¾çš„æ•°æ®ç±»å‹æ­£ç¡®
- [ ] **ç¼ºå¤±å€¼å¤„ç†**: ç»Ÿè®¡å¹¶åˆç†å¤„ç†ç¼ºå¤±å€¼
- [ ] **å¼‚å¸¸å€¼æ£€æµ‹**: è¯†åˆ«å’Œå¤„ç†æ˜æ˜¾çš„å¼‚å¸¸å€¼
- [ ] **æ ‡ç­¾åˆ†å¸ƒ**: åˆ†ææ ‡ç­¾åˆ†å¸ƒï¼Œè¯„ä¼°ç±»åˆ«ä¸å¹³è¡¡ç¨‹åº¦

### 2. è·¨åŸŸæ•°æ®å¤„ç†åŸåˆ™
- **æ ‡å‡†åŒ–é¡ºåº**: å§‹ç»ˆåœ¨æºåŸŸä¸Šæ‹Ÿåˆæ ‡å‡†åŒ–å™¨ï¼Œç„¶ååº”ç”¨åˆ°ç›®æ ‡åŸŸ
- **ç‰¹å¾é€‰æ‹©**: ä½¿ç”¨åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šç¨³å®šçš„ç‰¹å¾
- **åˆ†ç±»ç‰¹å¾**: è°¨æ…å¤„ç†åˆ†ç±»ç‰¹å¾ï¼Œç¡®ä¿ç¼–ç æ–¹å¼ä¸€è‡´
- **æ•°æ®æ³„éœ²é˜²æŠ¤**: ä¸¥æ ¼åˆ†ç¦»è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ï¼Œé¿å…ä¿¡æ¯æ³„éœ²

### 3. è´¨é‡æ§åˆ¶è¦ç‚¹
- **åˆ†å¸ƒå·®å¼‚ç›‘æ§**: å®šæœŸæ£€æŸ¥æºåŸŸå’Œç›®æ ‡åŸŸçš„ç‰¹å¾åˆ†å¸ƒå·®å¼‚
- **æ€§èƒ½åŸºçº¿**: å»ºç«‹æ— åŸŸé€‚åº”æƒ…å†µä¸‹çš„æ€§èƒ½åŸºçº¿
- **ç¨³å®šæ€§éªŒè¯**: ä½¿ç”¨å¤šæ¬¡éšæœºç§å­éªŒè¯ç»“æœç¨³å®šæ€§
- **å¯é‡ç°æ€§**: è®°å½•æ‰€æœ‰éšæœºç§å­å’Œå¤„ç†å‚æ•°

### 4. å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

#### é—®é¢˜1: ç‰¹å¾åˆ†å¸ƒå·®å¼‚è¿‡å¤§
**ç—‡çŠ¶**: æŸäº›ç‰¹å¾åœ¨ä¸åŒæ•°æ®é›†é—´æ•°å€¼èŒƒå›´å·®å¼‚è¶…è¿‡2å€
**è§£å†³æ–¹æ¡ˆ**: 
- ä½¿ç”¨é²æ£’æ ‡å‡†åŒ–æ–¹æ³•ï¼ˆå¦‚RobustScalerï¼‰
- è€ƒè™‘ç‰¹å¾å˜æ¢ï¼ˆå¦‚å¯¹æ•°å˜æ¢ï¼‰
- è¯„ä¼°æ˜¯å¦éœ€è¦ç§»é™¤è¯¥ç‰¹å¾

#### é—®é¢˜2: æ ‡ç­¾åˆ†å¸ƒä¸å¹³è¡¡
**ç—‡çŠ¶**: æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹åœ¨ä¸åŒæ•°æ®é›†é—´å·®å¼‚è¾ƒå¤§
**è§£å†³æ–¹æ¡ˆ**:
- ä½¿ç”¨åˆ†å±‚é‡‡æ ·ç¡®ä¿è®­ç»ƒé›†å¹³è¡¡
- è°ƒæ•´æ¨¡å‹çš„class_weightå‚æ•°
- è€ƒè™‘ä½¿ç”¨SMOTEç­‰è¿‡é‡‡æ ·æŠ€æœ¯

#### é—®é¢˜3: ç¼ºå¤±å€¼æ¨¡å¼ä¸ä¸€è‡´
**ç—‡çŠ¶**: ä¸åŒæ•°æ®é›†çš„ç¼ºå¤±å€¼æ¨¡å¼å·®å¼‚è¾ƒå¤§
**è§£å†³æ–¹æ¡ˆ**:
- åˆ†æç¼ºå¤±å€¼çš„åŸå› å’Œæ¨¡å¼
- ä½¿ç”¨æ›´å¤æ‚çš„æ’è¡¥æ–¹æ³•ï¼ˆå¦‚KNNæ’è¡¥ï¼‰
- è€ƒè™‘å°†ç¼ºå¤±å€¼ä½œä¸ºç‹¬ç«‹çš„ç±»åˆ«å¤„ç†

#### é—®é¢˜4: åˆ†ç±»ç‰¹å¾ç¼–ç ä¸ä¸€è‡´
**ç—‡çŠ¶**: åŒä¸€åˆ†ç±»ç‰¹å¾åœ¨ä¸åŒæ•°æ®é›†ä¸­æœ‰ä¸åŒçš„å–å€¼
**è§£å†³æ–¹æ¡ˆ**:
- å»ºç«‹ç»Ÿä¸€çš„ç¼–ç æ˜ å°„è¡¨
- ä½¿ç”¨ç›®æ ‡ç¼–ç ç­‰æ›´é²æ£’çš„ç¼–ç æ–¹æ³•
- è€ƒè™‘å°†ç½•è§ç±»åˆ«åˆå¹¶ä¸º"å…¶ä»–"ç±»åˆ«

### 5. æ•°æ®å¤„ç†æµç¨‹éªŒè¯
```python
def validate_preprocessing_pipeline():
    """éªŒè¯æ•°æ®é¢„å¤„ç†æµç¨‹çš„æ­£ç¡®æ€§"""
    
    # 1. éªŒè¯æ•°æ®å½¢çŠ¶
    assert X_train_scaled.shape[1] == X_external_scaled.shape[1], "ç‰¹å¾æ•°é‡ä¸ä¸€è‡´"
    
    # 2. éªŒè¯æ ‡å‡†åŒ–æ•ˆæœ
    train_mean = np.mean(X_train_scaled, axis=0)
    train_std = np.std(X_train_scaled, axis=0)
    assert np.allclose(train_mean, 0, atol=1e-10), "è®­ç»ƒé›†æ ‡å‡†åŒ–åå‡å€¼ä¸ä¸º0"
    assert np.allclose(train_std, 1, atol=1e-10), "è®­ç»ƒé›†æ ‡å‡†åŒ–åæ ‡å‡†å·®ä¸ä¸º1"
    
    # 3. éªŒè¯æ— æ•°æ®æ³„éœ²
    assert len(set(y_train.index) & set(y_external.index)) == 0, "è®­ç»ƒé›†å’Œæµ‹è¯•é›†å­˜åœ¨é‡å "
    
    # 4. éªŒè¯åˆ†ç±»ç‰¹å¾ç´¢å¼•
    if categorical_indices:
        assert max(categorical_indices) < X_train_scaled.shape[1], "åˆ†ç±»ç‰¹å¾ç´¢å¼•è¶…å‡ºèŒƒå›´"
    
    logging.info("æ•°æ®é¢„å¤„ç†æµç¨‹éªŒè¯é€šè¿‡")
```

è¿™äº›æœ€ä½³å®è·µç¡®ä¿äº†å‚æ•°è°ƒä¼˜å®éªŒçš„æ•°æ®è´¨é‡å’Œç»“æœå¯é æ€§ï¼Œä¸ºåç»­çš„æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚ 