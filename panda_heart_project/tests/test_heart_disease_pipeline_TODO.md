# æµ‹è¯•æ–¹æ¡ˆ TODO

## æ¨¡å—æ¦‚è¿°
**æ–‡ä»¶**: `tests/test_heart_disease_pipeline.py`
**åŠŸèƒ½**: PANDA-Hearté¡¹ç›®å®Œæ•´æµ‹è¯•æ¡†æ¶
**è´Ÿè´£äºº**: [å¾…åˆ†é…]
**é¢„è®¡å·¥æ—¶**: 24å°æ—¶

---

## ğŸ“‹ è¯¦ç»†ä»»åŠ¡æ¸…å•

### TASK-022: å•å…ƒæµ‹è¯•å®ç°
**ä¼˜å…ˆçº§**: ğŸ”¥ High | **é¢„è®¡å·¥æ—¶**: 8å°æ—¶ | **æˆªæ­¢**: Week 5

#### å­ä»»åŠ¡
- [ ] **TASK-022-1**: æ•°æ®å¤„ç†æµ‹è¯•
  - **æ•°æ®åŠ è½½**: UCI 4ä¸­å¿ƒæ•°æ®åŠ è½½æ­£ç¡®æ€§
  - **ç‰¹å¾ç¼–ç **: 14ä¸ªä¸´åºŠç‰¹å¾ç¼–ç éªŒè¯
  - **ç¼ºå¤±å€¼å¤„ç†**: å„ä¸­å¿ƒç¼ºå¤±å€¼ç­–ç•¥éªŒè¯
  - **æ•°æ®è´¨é‡**: æ•°æ®èŒƒå›´å’Œç±»å‹éªŒè¯

- [ ] **TASK-022-2**: æ¨¡å‹æµ‹è¯•
  - **TabPFNæ¨¡å‹**: é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å’Œæ¨ç†
  - **åŸŸé€‚åº”ç®—æ³•**: TCA/CORAL/SAç®—æ³•æ­£ç¡®æ€§
  - **åŸºçº¿æ¨¡å‹**: ä¼ ç»ŸMLæ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹
  - **é›†æˆç­–ç•¥**: PANDA_TabPFNé›†æˆéªŒè¯

- [ ] **TASK-022-3**: è¯„ä¼°æµ‹è¯•
  - **åŒ»å­¦æŒ‡æ ‡**: AUCã€æ•æ„Ÿæ€§ã€ç‰¹å¼‚æ€§è®¡ç®—
  - **æ ¡å‡†æŒ‡æ ‡**: Brier scoreã€ECEè®¡ç®—éªŒè¯
  - **è·¨åŸŸæŒ‡æ ‡**: æ€§èƒ½ä¿æŒç‡ã€é€‚åº”å¢ç›Šè®¡ç®—
  - **ç»Ÿè®¡æ£€éªŒ**: æ˜¾è‘—æ€§æ£€éªŒå’Œæ•ˆåº”é‡è®¡ç®—

#### éªŒæ”¶æ ‡å‡†
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡>90%
- [ ] æ‰€æœ‰å…³é”®åŠŸèƒ½æµ‹è¯•é€šè¿‡
- [ ] è¾¹ç•Œæ¡ä»¶æµ‹è¯•å®Œæ•´

#### æŠ€æœ¯è¦æ±‚
```python
# ä¼ªä»£ç ç¤ºä¾‹
import pytest
import numpy as np
from unittest.mock import Mock, patch
from sklearn.datasets import make_classification
from sklearn.metrics import roc_auc_score

class TestDataProcessing:
    """æ•°æ®å¤„ç†æ¨¡å—æµ‹è¯•"""

    @pytest.fixture
    def sample_heart_data(self):
        """ç”Ÿæˆç¤ºä¾‹å¿ƒè„ç—…æ•°æ®"""
        np.random.seed(42)
        n_samples = 200
        n_features = 14

        X = np.random.randn(n_samples, n_features)
        X[:, 0] = np.abs(X[:, 0] * 20 + 50)  # age: 30-90
        X[:, 1] = np.random.randint(0, 2, n_samples)  # sex: 0/1
        X[:, 2] = np.random.randint(1, 5, n_samples)  # cp: 1-4

        y = np.random.randint(0, 2, n_samples)
        feature_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',
                        'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num']

        return X, y, feature_names

    def test_feature_encoding_correctness(self, sample_heart_data):
        """æµ‹è¯•ç‰¹å¾ç¼–ç æ­£ç¡®æ€§"""
        X, y, feature_names = sample_heart_data
        encoder = ClinicalFeatureEncoder()

        # æµ‹è¯•ç¼–ç 
        X_encoded = encoder.fit_transform(X, y)

        # éªŒè¯ç‰¹å¾ç»´åº¦
        assert X_encoded.shape[0] == X.shape[0], "æ ·æœ¬æ•°é‡ä¸åº”æ”¹å˜"

        # éªŒè¯å¹´é¾„èŒƒå›´
        age_idx = feature_names.index('age')
        assert np.all(X_encoded[:, age_idx] >= -3) and np.all(X_encoded[:, age_idx] <= 3), "å¹´é¾„åº”åœ¨åˆç†èŒƒå›´å†…"

        # éªŒè¯äºŒè¿›åˆ¶ç‰¹å¾
        sex_idx = feature_names.index('sex')
        assert np.all(np.isin(X_encoded[:, sex_idx], [0, 1])), "æ€§åˆ«åº”ä¸º0/1"

    def test_missing_value_handling(self):
        """æµ‹è¯•ç¼ºå¤±å€¼å¤„ç†"""
        # åˆ›å»ºå¸¦ç¼ºå¤±å€¼çš„æ•°æ®
        X = np.random.randn(100, 14)
        X[::5, 2] = np.nan  # 20%ç¼ºå¤±ç‡
        y = np.random.randint(0, 2, 100)

        processor = DataProcessor()
        X_processed = processor.fit_transform(X, y)

        # éªŒè¯æ— ç¼ºå¤±å€¼
        assert not np.isnan(X_processed).any(), "å¤„ç†åä¸åº”æœ‰ç¼ºå¤±å€¼"

    def test_clinical_feature_validation(self, sample_heart_data):
        """æµ‹è¯•ä¸´åºŠç‰¹å¾éªŒè¯"""
        X, y, feature_names = sample_heart_data
        validator = ClinicalFeatureValidator()

        # æµ‹è¯•æ­£å¸¸æ•°æ®
        assert validator.validate_features(X, feature_names), "æ­£å¸¸æ•°æ®åº”é€šè¿‡éªŒè¯"

        # æµ‹è¯•å¼‚å¸¸å¹´é¾„
        X_invalid = X.copy()
        X_invalid[0, 0] = 150  # å¼‚å¸¸å¹´é¾„
        assert not validator.validate_features(X_invalid, feature_names), "å¼‚å¸¸å¹´é¾„åº”è¢«æ£€æµ‹"

class TestDomainAdaptation:
    """åŸŸé€‚åº”æ¨¡å—æµ‹è¯•"""

    @pytest.fixture
    def source_target_data(self):
        """ç”ŸæˆæºåŸŸå’Œç›®æ ‡åŸŸæ•°æ®"""
        # æºåŸŸæ•°æ®
        X_source, y_source = make_classification(
            n_samples=300, n_features=14, n_informative=10,
            n_redundant=2, random_state=42
        )

        # ç›®æ ‡åŸŸæ•°æ®ï¼ˆåˆ†å¸ƒåç§»ï¼‰
        X_target, y_target = make_classification(
            n_samples=200, n_features=14, n_informative=10,
            n_redundant=2, shift=0.5, random_state=123
        )

        return X_source, y_source, X_target, y_target

    def test_tca_algorithm(self, source_target_data):
        """æµ‹è¯•TCAç®—æ³•"""
        X_source, y_source, X_target, y_target = source_target_data
        tca = HeartDiseaseTCA(mu=0.1, n_components=10)

        # æµ‹è¯•è®­ç»ƒ
        tca.fit(X_source, y_source, X_target)
        assert hasattr(tca, 'components_'), "è®­ç»ƒååº”æœ‰components_å±æ€§"

        # æµ‹è¯•å˜æ¢
        X_source_tca, X_target_tca = tca.transform(X_source, X_target)
        assert X_source_tca.shape[1] == 10, "å˜æ¢åç»´åº¦åº”ä¸º10"
        assert X_target_tca.shape[1] == 10, "å˜æ¢åç»´åº¦åº”ä¸º10"

        # æµ‹è¯•MMDè·ç¦»å‡å°
        original_mmd = compute_mmd_distance(X_source, X_target)
        adapted_mmd = compute_mmd_distance(X_source_tca, X_target_tca)
        assert adapted_mmd < original_mmd, "åŸŸé€‚åº”åº”å‡å°MMDè·ç¦»"

    def test_coral_algorithm(self, source_target_data):
        """æµ‹è¯•CORALç®—æ³•"""
        X_source, y_source, X_target, y_target = source_target_data
        coral = HeartDiseaseCORAL(reg_param=1e-3)

        # æµ‹è¯•è®­ç»ƒ
        coral.fit(X_source, y_source, X_target)
        assert hasattr(coral, 'transformation_matrix_'), "è®­ç»ƒååº”æœ‰å˜æ¢çŸ©é˜µ"

        # æµ‹è¯•å˜æ¢
        X_source_coral = coral.transform(X_source)
        X_target_coral = coral.transform(X_target)
        assert X_source_coral.shape == X_source.shape, "CORALä¸åº”æ”¹å˜ç»´åº¦"

    def test_sa_algorithm(self, source_target_data):
        """æµ‹è¯•SAç®—æ³•"""
        X_source, y_source, X_target, y_target = source_target_data
        sa = HeartDiseaseSA(n_components=0.9)

        # æµ‹è¯•è®­ç»ƒ
        sa.fit(X_source, y_source, X_target)
        assert hasattr(sa, 'alignment_matrix_'), "è®­ç»ƒååº”æœ‰å¯¹é½çŸ©é˜µ"

        # æµ‹è¯•å˜æ¢
        X_source_sa = sa.transform(X_source)
        X_target_sa = sa.transform(X_target)
        assert X_source_sa.shape[1] <= X_source.shape[1], "SAåº”é™ç»´"

class TestModelEvaluation:
    """æ¨¡å‹è¯„ä¼°æµ‹è¯•"""

    @pytest.fixture
    def model_predictions(self):
        """ç”Ÿæˆæ¨¡å‹é¢„æµ‹ç»“æœ"""
        n_samples = 200
        y_true = np.random.randint(0, 2, n_samples)
        y_prob = np.random.rand(n_samples)
        y_pred = (y_prob > 0.5).astype(int)

        return y_true, y_pred, y_prob

    def test_medical_metrics_computation(self, model_predictions):
        """æµ‹è¯•åŒ»å­¦æŒ‡æ ‡è®¡ç®—"""
        y_true, y_pred, y_prob = model_predictions
        evaluator = MedicalMetrics()

        metrics = evaluator.compute_core_metrics(y_true, y_pred, y_prob)

        # éªŒè¯æŒ‡æ ‡èŒƒå›´
        assert 0 <= metrics['auc_roc'] <= 1, "AUCåº”åœ¨0-1èŒƒå›´å†…"
        assert 0 <= metrics['sensitivity'] <= 1, "æ•æ„Ÿæ€§åº”åœ¨0-1èŒƒå›´å†…"
        assert 0 <= metrics['specificity'] <= 1, "ç‰¹å¼‚æ€§åº”åœ¨0-1èŒƒå›´å†…"

    def test_calibration_metrics(self, model_predictions):
        """æµ‹è¯•æ ¡å‡†æŒ‡æ ‡è®¡ç®—"""
        y_true, _, y_prob = model_predictions
        evaluator = MedicalMetrics()

        calibration_metrics = evaluator.compute_calibration_metrics(y_true, y_prob)

        # éªŒè¯æ ¡å‡†æŒ‡æ ‡
        assert calibration_metrics['brier_score'] >= 0, "Brieråˆ†æ•°åº”ä¸ºéè´Ÿ"
        assert 0 <= calibration_metrics['ece'] <= 1, "ECEåº”åœ¨0-1èŒƒå›´å†…"
```

---

### TASK-023: é›†æˆæµ‹è¯•å®ç°
**ä¼˜å…ˆçº§**: ğŸ”¥ High | **é¢„è®¡å·¥æ—¶**: 8å°æ—¶ | **æˆªæ­¢**: Week 6

#### å­ä»»åŠ¡
- [ ] **TASK-023-1**: ç«¯åˆ°ç«¯æµç¨‹æµ‹è¯•
  - **å®Œæ•´æµç¨‹**: æ•°æ®åŠ è½½â†’æ¨¡å‹è®­ç»ƒâ†’è¯„ä¼°â†’å¯è§†åŒ–
  - **å¤šæ¨¡å‹å¯¹æ¯”**: 7ç§æ¨¡å‹çš„å®Œæ•´å¯¹æ¯”æµç¨‹
  - **å¤šåŸŸé€‚åº”**: 6ç§åŸŸé€‚åº”æ–¹æ³•çš„é›†æˆæµ‹è¯•
  - **é”™è¯¯å¤„ç†**: æµç¨‹ä¸­å¼‚å¸¸æƒ…å†µçš„å¤„ç†

- [ ] **TASK-023-2**: è·¨ä¸­å¿ƒé›†æˆæµ‹è¯•
  - **LOCO-CVæµç¨‹**: Leave-One-Center-Outå®Œæ•´æµ‹è¯•
  - **å¤šä¸­å¿ƒæ•°æ®**: 4ä¸ªä¸­å¿ƒæ•°æ®çš„å…¼å®¹æ€§æµ‹è¯•
  - **ç»“æœä¸€è‡´æ€§**: é‡å¤å®éªŒçš„ç»“æœä¸€è‡´æ€§éªŒè¯
  - **æ€§èƒ½åŸºå‡†**: ä¸é¢„æœŸæ€§èƒ½åŸºå‡†çš„å¯¹æ¯”

- [ ] **TASK-023-3**: ç³»ç»Ÿé›†æˆæµ‹è¯•
  - **é…ç½®ç®¡ç†**: é…ç½®æ–‡ä»¶å’Œå‚æ•°ç®¡ç†æµ‹è¯•
  - **èµ„æºç®¡ç†**: å†…å­˜ã€è®¡ç®—èµ„æºä½¿ç”¨æµ‹è¯•
  - **å¹¶å‘å¤„ç†**: å¤šè¿›ç¨‹/å¤šçº¿ç¨‹å¹¶å‘æµ‹è¯•
  - **æŒä¹…åŒ–**: æ¨¡å‹ä¿å­˜å’ŒåŠ è½½æµ‹è¯•

#### éªŒæ”¶æ ‡å‡†
- [ ] ç«¯åˆ°ç«¯æµç¨‹æµ‹è¯•è¦†ç›–æ‰€æœ‰åœºæ™¯
- [ ] è·¨ä¸­å¿ƒé›†æˆæµ‹è¯•éªŒè¯æ³›åŒ–èƒ½åŠ›
- [ ] ç³»ç»Ÿé›†æˆæµ‹è¯•ç¡®ä¿ç¨³å®šæ€§å’Œæ€§èƒ½

#### æŠ€æœ¯è¦æ±‚
```python
# ä¼ªä»£ç ç¤ºä¾‹
class TestEndToEndPipeline:
    """ç«¯åˆ°ç«¯æµç¨‹æµ‹è¯•"""

    @pytest.mark.integration
    def test_complete_single_center_experiment(self):
        """æµ‹è¯•å®Œæ•´å•ä¸­å¿ƒå®éªŒ"""
        # è®¾ç½®å®éªŒé…ç½®
        config = ExperimentConfig()
        config.experiment_type = 'single_center'
        config.models = ['PANDA_TabPFN', 'LASSO_LR']
        config.uda_methods = ['No_UDA']

        # è¿è¡Œå®éªŒ
        runner = ExperimentRunner(config)
        results = runner.run_experiment()

        # éªŒè¯ç»“æœç»“æ„
        assert 'single_center' in results, "åº”åŒ…å«å•ä¸­å¿ƒç»“æœ"
        assert len(results['single_center']) > 0, "åº”æœ‰å®éªŒç»“æœ"

        # éªŒè¯ç»“æœå®Œæ•´æ€§
        for center_result in results['single_center'].values():
            assert 'cv_results' in center_result, "åº”åŒ…å«äº¤å‰éªŒè¯ç»“æœ"
            assert 'model_performance' in center_result, "åº”åŒ…å«æ¨¡å‹æ€§èƒ½"

    @pytest.mark.integration
    def test_complete_two_center_experiment(self):
        """æµ‹è¯•å®Œæ•´ä¸¤ä¸­å¿ƒè·¨åŸŸå®éªŒ"""
        config = ExperimentConfig()
        config.experiment_type = 'two_center'
        config.source_centers = ['Cleveland', 'Hungarian']
        config.target_centers = ['VA', 'Switzerland']
        config.models = ['PANDA_TabPFN', 'TabPFN_Only']
        config.uda_methods = ['TCA', 'No_UDA']

        runner = ExperimentRunner(config)
        results = runner.run_experiment()

        # éªŒè¯è·¨åŸŸç»“æœ
        assert 'domain_adaptation' in results, "åº”åŒ…å«åŸŸé€‚åº”ç»“æœ"
        assert len(results['domain_adaptation']) > 0, "åº”æœ‰åŸŸé€‚åº”å®éªŒç»“æœ"

        # éªŒè¯é€‚åº”æ•ˆæœ
        for domain_result in results['domain_adaptation']:
            if 'TCA' in domain_result:
                assert 'adaptation_gain' in domain_result['TCA'], "åº”åŒ…å«é€‚åº”å¢ç›Š"

    @pytest.mark.integration
    def test_complete_multi_center_experiment(self):
        """æµ‹è¯•å®Œæ•´å¤šä¸­å¿ƒLOCO-CVå®éªŒ"""
        config = ExperimentConfig()
        config.experiment_type = 'multi_center'
        config.validation_method = 'loco_cv'
        config.models = ['PANDA_TabPFN', 'LASSO_LR', 'XGBoost']
        config.uda_methods = ['TCA', 'CORAL', 'No_UDA']

        runner = ExperimentRunner(config)
        results = runner.run_experiment()

        # éªŒè¯LOCO-CVç»“æœ
        assert 'multi_center' in results, "åº”åŒ…å«å¤šä¸­å¿ƒç»“æœ"
        assert len(results['multi_center']['loco_results']) == 4, "åº”æœ‰4ä¸ªLOCOå®éªŒ"

        # éªŒè¯æ€§èƒ½å¯¹æ¯”
        performance_comparison = results['multi_center']['performance_comparison']
        assert len(performance_comparison) > 0, "åº”æœ‰æ€§èƒ½å¯¹æ¯”ç»“æœ"

class TestSystemIntegration:
    """ç³»ç»Ÿé›†æˆæµ‹è¯•"""

    @pytest.mark.system
    def test_configuration_management(self):
        """æµ‹è¯•é…ç½®ç®¡ç†"""
        config_manager = HeartDiseaseConfigManager()

        # æµ‹è¯•é…ç½®åŠ è½½
        config = config_manager.get_config('data')
        assert 'feature_names' in config, "æ•°æ®é…ç½®åº”åŒ…å«ç‰¹å¾åç§°"

        # æµ‹è¯•é…ç½®æ›´æ–°
        config_manager.update_config('experiment', {'n_repetitions': 10})
        updated_config = config_manager.get_config('experiment')
        assert updated_config['n_repetitions'] == 10, "é…ç½®æ›´æ–°åº”ç”Ÿæ•ˆ"

    @pytest.mark.system
    def test_resource_management(self):
        """æµ‹è¯•èµ„æºç®¡ç†"""
        # ç›‘æ§å†…å­˜ä½¿ç”¨
        import psutil
        process = psutil.Process()
        initial_memory = process.memory_info().rss

        # è¿è¡Œå¤§æ•°æ®é›†å®éªŒ
        X_large = np.random.randn(10000, 14)
        y_large = np.random.randint(0, 2, 10000)

        model = PANDATabPFN()
        model.fit(X_large[:8000], y_large[:8000])
        _ = model.predict_proba(X_large[8000:])

        final_memory = process.memory_info().rss
        memory_increase = final_memory - initial_memory

        # éªŒè¯å†…å­˜ä½¿ç”¨åˆç†ï¼ˆ<2GBï¼‰
        assert memory_increase < 2 * 1024**3, "å†…å­˜å¢é•¿åº”æ§åˆ¶åœ¨2GBä»¥å†…"

    @pytest.mark.system
    def test_model_persistence(self):
        """æµ‹è¯•æ¨¡å‹æŒä¹…åŒ–"""
        # è®­ç»ƒæ¨¡å‹
        X_train, y_train = make_classification(n_samples=1000, n_features=14, random_state=42)
        model = PANDATabPFN()
        model.fit(X_train, y_train)

        # ä¿å­˜æ¨¡å‹
        save_path = "test_model.pkl"
        model.save_model(save_path)
        assert os.path.exists(save_path), "æ¨¡å‹æ–‡ä»¶åº”è¢«ä¿å­˜"

        # åŠ è½½æ¨¡å‹
        loaded_model = PANDATabPFN.load_model(save_path)
        assert isinstance(loaded_model, PANDATabPFN), "åŠ è½½çš„åº”ä¸ºPANDATabPFNå®ä¾‹"

        # éªŒè¯é¢„æµ‹ä¸€è‡´æ€§
        X_test = np.random.randn(100, 14)
        original_pred = model.predict_proba(X_test)
        loaded_pred = loaded_model.predict_proba(X_test)
        np.testing.assert_array_almost_equal(original_pred, loaded_pred, decimal=6)

        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        os.remove(save_path)
```

---

### TASK-024: æ€§èƒ½æµ‹è¯•å®ç°
**ä¼˜å…ˆçº§**: ğŸ”¥ Medium | **é¢„è®¡å·¥æ—¶**: 8å°æ—¶ | **æˆªæ­¢**: Week 7

#### å­ä»»åŠ¡
- [ ] **TASK-024-1**: è®¡ç®—æ€§èƒ½æµ‹è¯•
  - **è®­ç»ƒæ—¶é—´**: å„æ¨¡å‹è®­ç»ƒæ—¶é—´åŸºå‡†æµ‹è¯•
  - **æ¨ç†æ—¶é—´**: æ¨¡å‹é¢„æµ‹æ—¶é—´æ€§èƒ½æµ‹è¯•
  - **å†…å­˜ä½¿ç”¨**: ä¸åŒæ•°æ®è§„æ¨¡çš„å†…å­˜æ¶ˆè€—
  - **å¹¶å‘æ€§èƒ½**: å¤šè¿›ç¨‹/å¤šçº¿ç¨‹æ€§èƒ½æå‡

- [ ] **TASK-024-2**: å¯æ‰©å±•æ€§æµ‹è¯•
  - **æ•°æ®è§„æ¨¡**: ä¸åŒæ ·æœ¬æ•°é‡çš„æ€§èƒ½å˜åŒ–
  - **ç‰¹å¾æ•°é‡**: ä¸åŒç‰¹å¾ç»´åº¦çš„æ€§èƒ½å½±å“
  - **æ¨¡å‹å¤æ‚åº¦**: TabPFNé›†æˆå¤§å°å¯¹æ€§èƒ½çš„å½±å“
  - **åŸŸé€‚åº”å¤æ‚åº¦**: ä¸åŒåŸŸé€‚åº”æ–¹æ³•çš„è®¡ç®—å¼€é”€

- [ ] **TASK-024-3**: ç¨³å®šæ€§æµ‹è¯•
  - **é•¿æ—¶é—´è¿è¡Œ**: è¿ç»­è¿è¡Œçš„ç¨³å®šæ€§éªŒè¯
  - **å†…å­˜æ³„æ¼**: é•¿æœŸä½¿ç”¨çš„å†…å­˜æ³„æ¼æ£€æµ‹
  - **å¼‚å¸¸æ¢å¤**: å¼‚å¸¸æƒ…å†µä¸‹çš„ç³»ç»Ÿæ¢å¤
  - **è¾¹ç•Œæ¡ä»¶**: æç«¯å‚æ•°ä¸‹çš„ç³»ç»Ÿè¡Œä¸º

#### éªŒæ”¶æ ‡å‡†
- [ ] æ€§èƒ½åŸºå‡†ç¬¦åˆé¢„æœŸè¦æ±‚
- [ ] å¯æ‰©å±•æ€§æµ‹è¯•è¦†ç›–å®é™…ä½¿ç”¨åœºæ™¯
- [ ] ç¨³å®šæ€§æµ‹è¯•ç¡®ä¿ç³»ç»Ÿå¯é æ€§

#### æŠ€æœ¯è¦æ±‚
```python
# ä¼ªä»£ç ç¤ºä¾‹
class TestPerformance:
    """æ€§èƒ½æµ‹è¯•"""

    @pytest.mark.performance
    def test_training_time_benchmark(self):
        """æµ‹è¯•è®­ç»ƒæ—¶é—´åŸºå‡†"""
        data_sizes = [100, 500, 1000, 2000]
        models = ['PANDA_TabPFN', 'LASSO_LR', 'XGBoost']
        training_times = {}

        for size in data_sizes:
            X, y = make_classification(n_samples=size, n_features=14, random_state=42)
            training_times[size] = {}

            for model_name in models:
                model = create_model(model_name)

                start_time = time.time()
                model.fit(X, y)
                end_time = time.time()

                training_time = end_time - start_time
                training_times[size][model_name] = training_time

                # éªŒè¯è®­ç»ƒæ—¶é—´åˆç†æ€§
                if model_name == 'PANDA_TabPFN':
                    assert training_time < 300, f"PANDA_TabPFNè®­ç»ƒæ—¶é—´åº”å°äº5åˆ†é’Ÿ (å®é™…: {training_time:.2f}s)"
                elif model_name in ['LASSO_LR', 'XGBoost']:
                    assert training_time < 60, f"ä¼ ç»Ÿæ¨¡å‹è®­ç»ƒæ—¶é—´åº”å°äº1åˆ†é’Ÿ (å®é™…: {training_time:.2f}s)"

        # æ‰“å°æ€§èƒ½åŸºå‡†
        print("è®­ç»ƒæ—¶é—´åŸºå‡† (ç§’):")
        for size, times in training_times.items():
            print(f"æ ·æœ¬æ•° {size}: {times}")

    @pytest.mark.performance
    def test_inference_time_benchmark(self):
        """æµ‹è¯•æ¨ç†æ—¶é—´åŸºå‡†"""
        X_test = np.random.randn(1000, 14)
        models = ['PANDA_TabPFN', 'LASSO_LR', 'XGBoost']
        inference_times = {}

        for model_name in models:
            # é¢„è®­ç»ƒæ¨¡å‹
            X_train, y_train = make_classification(n_samples=1000, n_features=14, random_state=42)
            model = create_model(model_name)
            model.fit(X_train, y_train)

            # æµ‹è¯•æ¨ç†æ—¶é—´
            start_time = time.time()
            for _ in range(100):  # é‡å¤100æ¬¡
                _ = model.predict(X_test)
            end_time = time.time()

            avg_inference_time = (end_time - start_time) / 100
            inference_times[model_name] = avg_inference_time

            # éªŒè¯æ¨ç†æ—¶é—´
            assert avg_inference_time < 1.0, f"{model_name}æ¨ç†æ—¶é—´åº”å°äº1ç§’ (å®é™…: {avg_inference_time:.4f}s)"

        print("æ¨ç†æ—¶é—´åŸºå‡† (ç§’):", inference_times)

    @pytest.mark.performance
    def test_memory_usage_scaling(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æ‰©å±•æ€§"""
        import psutil
        process = psutil.Process()

        data_sizes = [1000, 2000, 5000, 10000]
        memory_usage = {}

        for size in data_sizes:
            # è®°å½•åˆå§‹å†…å­˜
            initial_memory = process.memory_info().rss

            # åˆ›å»ºå’Œå¤„ç†æ•°æ®
            X, y = make_classification(n_samples=size, n_features=14, random_state=42)
            model = PANDATabPFN()
            model.fit(X, y)

            # è®°å½•å³°å€¼å†…å­˜
            peak_memory = process.memory_info().rss
            memory_increase = (peak_memory - initial_memory) / 1024**2  # MB
            memory_usage[size] = memory_increase

            # æ¸…ç†å†…å­˜
            del model, X, y
            gc.collect()

        # éªŒè¯å†…å­˜ä½¿ç”¨çº¿æ€§å¢é•¿
        sizes = np.array(data_sizes)
        memories = np.array([memory_usage[size] for size in data_sizes])
        correlation = np.corrcoef(sizes, memories)[0, 1]
        assert correlation > 0.8, "å†…å­˜ä½¿ç”¨åº”ä¸æ•°æ®è§„æ¨¡æ­£ç›¸å…³"

        print("å†…å­˜ä½¿ç”¨æƒ…å†µ (MB):", memory_usage)

    @pytest.mark.stability
    def test_long_running_stability(self):
        """æµ‹è¯•é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§"""
        errors = []
        n_iterations = 50

        for i in range(n_iterations):
            try:
                # éšæœºæ•°æ®
                X, y = make_classification(n_samples=200, n_features=14, random_state=i)

                # è®­ç»ƒå’Œé¢„æµ‹
                model = PANDATabPFN()
                model.fit(X, y)
                pred = model.predict(X)
                prob = model.predict_proba(X)

                # éªŒè¯ç»“æœ
                assert len(pred) == len(y), "é¢„æµ‹é•¿åº¦åº”åŒ¹é…"
                assert prob.shape == (len(y), 2), "æ¦‚ç‡è¾“å‡ºå½¢çŠ¶åº”æ­£ç¡®"
                assert np.all(np.isclose(prob.sum(axis=1), 1.0)), "æ¦‚ç‡å’Œåº”ä¸º1"

            except Exception as e:
                errors.append(f"Iteration {i}: {str(e)}")

        # éªŒè¯é”™è¯¯ç‡
        error_rate = len(errors) / n_iterations
        assert error_rate < 0.1, f"é•¿æ—¶é—´è¿è¡Œé”™è¯¯ç‡åº”å°äº10% (å®é™…: {error_rate:.2%})"

        if errors:
            print("å‘ç°çš„é”™è¯¯:", errors[:5])  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯

class TestScalability:
    """å¯æ‰©å±•æ€§æµ‹è¯•"""

    @pytest.mark.scalability
    def test_data_size_scaling(self):
        """æµ‹è¯•æ•°æ®è§„æ¨¡æ‰©å±•æ€§"""
        data_sizes = [500, 1000, 2000, 5000]
        performance_metrics = []

        for size in data_sizes:
            X, y = make_classification(n_samples=size, n_features=14, random_state=42)

            # è®­ç»ƒæ—¶é—´å’Œæ€§èƒ½
            start_time = time.time()
            model = PANDATabPFN()
            model.fit(X, y)
            training_time = time.time() - start_time

            # è¯„ä¼°æ€§èƒ½
            scores = cross_val_score(model, X, y, cv=5, scoring='roc_auc')
            mean_auc = scores.mean()

            performance_metrics.append({
                'size': size,
                'training_time': training_time,
                'auc': mean_auc,
                'time_per_sample': training_time / size
            })

        # éªŒè¯æ€§èƒ½æ‰©å±•æ€§
        for i in range(1, len(performance_metrics)):
            prev_perf = performance_metrics[i-1]
            curr_perf = performance_metrics[i]

            # è®­ç»ƒæ—¶é—´ä¸åº”æŒ‡æ•°å¢é•¿
            time_ratio = curr_perf['training_time'] / prev_perf['training_time']
            size_ratio = curr_perf['size'] / prev_perf['size']
            assert time_ratio < size_ratio ** 1.5, "è®­ç»ƒæ—¶é—´å¢é•¿åº”æ…¢äºæ•°æ®è§„æ¨¡çš„1.5æ¬¡æ–¹"

        print("æ‰©å±•æ€§æµ‹è¯•ç»“æœ:")
        for metric in performance_metrics:
            print(f"æ ·æœ¬æ•°: {metric['size']}, è®­ç»ƒæ—¶é—´: {metric['training_time']:.2f}s, "
                  f"AUC: {metric['auc']:.3f}, æ—¶é—´/æ ·æœ¬: {metric['time_per_sample']:.4f}s")
```

---

## ğŸ”§ å®ç°ç»†èŠ‚

### æµ‹è¯•é…ç½®
```python
# pytest.ini
[tool:pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
markers =
    unit: å•å…ƒæµ‹è¯•
    integration: é›†æˆæµ‹è¯•
    system: ç³»ç»Ÿæµ‹è¯•
    performance: æ€§èƒ½æµ‹è¯•
    scalability: å¯æ‰©å±•æ€§æµ‹è¯•
    stability: ç¨³å®šæ€§æµ‹è¯•
    slow: æ…¢é€Ÿæµ‹è¯•ï¼ˆå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰
addopts =
    --strict-markers
    --strict-config
    --verbose
    --tb=short
    --cov=panda_heart
    --cov-report=html
    --cov-report=term-missing
    --cov-fail-under=80

# conftest.py
import pytest
import numpy as np
from sklearn.datasets import make_classification

@pytest.fixture(scope="session")
def random_seed():
    """å…¨å±€éšæœºç§å­"""
    np.random.seed(42)
    return 42

@pytest.fixture
def sample_heart_dataset():
    """ç¤ºä¾‹å¿ƒè„ç—…æ•°æ®é›†"""
    X, y = make_classification(
        n_samples=1000,
        n_features=14,
        n_informative=10,
        n_redundant=2,
        n_repeated=0,
        n_classes=2,
        n_clusters_per_class=2,
        weights=[0.6, 0.4],  # æ¨¡æ‹Ÿç±»åˆ«ä¸å¹³è¡¡
        flip_y=0.01,
        random_state=42
    )
    return X, y

@pytest.fixture
def mock_tabpfn_model():
    """æ¨¡æ‹ŸTabPFNæ¨¡å‹"""
    class MockTabPFN:
        def __init__(self):
            self.is_fitted = False

        def fit(self, X, y):
            self.is_fitted = True
            return self

        def predict(self, X):
            if not self.is_fitted:
                raise ValueError("Model not fitted")
            return np.random.randint(0, 2, len(X))

        def predict_proba(self, X):
            if not self.is_fitted:
                raise ValueError("Model not fitted")
            prob = np.random.rand(len(X), 2)
            prob = prob / prob.sum(axis=1, keepdims=True)
            return prob

    return MockTabPFN()
```

---

## ğŸ§ª æµ‹è¯•æ‰§è¡Œè®¡åˆ’

### è‡ªåŠ¨åŒ–æµ‹è¯•æµç¨‹
```python
# scripts/run_tests.py
#!/usr/bin/env python3
"""è‡ªåŠ¨åŒ–æµ‹è¯•æ‰§è¡Œè„šæœ¬"""

import subprocess
import sys
import argparse

def run_test_suite(test_type="all"):
    """è¿è¡Œæµ‹è¯•å¥—ä»¶"""

    test_commands = {
        "unit": "pytest tests/ -m unit",
        "integration": "pytest tests/ -m integration",
        "system": "pytest tests/ -m system",
        "performance": "pytest tests/ -m performance",
        "all": "pytest tests/",
        "coverage": "pytest tests/ --cov=panda_heart --cov-report=html",
        "quick": "pytest tests/ -m 'not slow and not performance'"
    }

    if test_type not in test_commands:
        print(f"Unknown test type: {test_type}")
        print(f"Available types: {list(test_commands.keys())}")
        return False

    command = test_commands[test_type]
    print(f"Running: {command}")

    result = subprocess.run(command, shell=True)
    return result.returncode == 0

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run PANDA-Heart test suite")
    parser.add_argument("--type", default="all", choices=["unit", "integration", "system", "performance", "all", "coverage", "quick"])
    args = parser.parse_args()

    success = run_test_suite(args.type)
    sys.exit(0 if success else 1)
```

---

## ğŸ“Š é¢„æœŸè¾“å‡º

### æµ‹è¯•æŠ¥å‘Š
- `tests/reports/unit_test_report.html` - å•å…ƒæµ‹è¯•æŠ¥å‘Š
- `tests/reports/integration_test_report.html` - é›†æˆæµ‹è¯•æŠ¥å‘Š
- `tests/reports/performance_benchmark.json` - æ€§èƒ½åŸºå‡†æŠ¥å‘Š
- `tests/reports/coverage_report/` - ä»£ç è¦†ç›–ç‡æŠ¥å‘Š

### æµ‹è¯•æ•°æ®
- `tests/data/sample_heart_data.csv` - æµ‹è¯•ç”¨å¿ƒè„ç—…æ•°æ®
- `tests/data/mock_models/` - æ¨¡æ‹Ÿæ¨¡å‹æ–‡ä»¶
- `tests/fixtures/` - æµ‹è¯•å¤¹å…·å’Œå·¥å…·å‡½æ•°

---

## ğŸš¨ é£é™©ä¸ç¼“è§£

### é£é™©è¯†åˆ«
1. **æµ‹è¯•è¦†ç›–ä¸å…¨** (å…³é”®åŠŸèƒ½é—æ¼)
2. **æµ‹è¯•ç¯å¢ƒå·®å¼‚** (å¼€å‘/ç”Ÿäº§ç¯å¢ƒä¸ä¸€è‡´)
3. **æ€§èƒ½æµ‹è¯•ä¸ç¨³å®š** (ç¯å¢ƒå› ç´ å½±å“)

### ç¼“è§£ç­–ç•¥
1. **ä»£ç è¦†ç›–ç‡ç›‘æ§ + å…³é”®è·¯å¾„æµ‹è¯•**
2. **å®¹å™¨åŒ–æµ‹è¯•ç¯å¢ƒ + ç¯å¢ƒæ ‡å‡†åŒ–**
3. **å¤šæ¬¡è¿è¡Œ + ç»Ÿè®¡åˆ†æ**

---

## ğŸ“ è”ç³»ä¿¡æ¯
**è´Ÿè´£äºº**: [å¾…åˆ†é…]
**æµ‹è¯•å·¥ç¨‹å¸ˆ**: [QAå·¥ç¨‹å¸ˆ]
**æ€§èƒ½å·¥ç¨‹å¸ˆ**: [æ€§èƒ½ä¼˜åŒ–ä¸“å®¶]

*æœ€åæ›´æ–°: 2025-11-18*