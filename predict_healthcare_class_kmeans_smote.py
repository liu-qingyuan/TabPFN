import warnings
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

import pandas as pd
import matplotlib.pyplot as plt
import time
import os
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix
from tabpfn import TabPFNClassifier
from typing import Dict, Tuple, Any
import numpy as np

# å¯¼å…¥imbalanced-learnä¸­çš„KMeans-SMOTE
try:
    from imblearn.over_sampling import KMeansSMOTE
except ImportError:
    print("æ­£åœ¨å®‰è£… imbalanced-learn...")
    os.system("pip install imbalanced-learn")
    from imblearn.over_sampling import KMeansSMOTE

def analyze_with_kmeans_smote_and_feature_selection(
    device: str = 'cuda',
    n_estimators: int = 32,
    softmax_temperature: float = 0.9,
    balance_probabilities: bool = False,
    average_before_softmax: bool = False,
    ignore_pretraining_limits: bool = True,
    random_state: int = 42,
    k_best_features: int = 8,
    base_path: str = './results'
) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    ä½¿ç”¨KMeans-SMOTEæ•°æ®å¹³è¡¡å’Œç‰¹å¾é€‰æ‹©çš„TabPFNç™Œç—‡é¢„æµ‹åˆ†æ
    """
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs(base_path, exist_ok=True)
    
    # ==============================
    # 1. è¯»å–æ•°æ®
    # ==============================
    print("åŠ è½½æ²³å—ç™Œç—‡åŒ»é™¢æ•°æ®...")
    df = pd.read_excel("data/HenanCancerHospital_translated_english.xlsx")
    
    # é€‰æ‹©ç‰¹å¾åˆ—
    features = [c for c in df.columns if c.startswith("Feature")]
    X = df[features].copy()
    y = df["Label"].copy()
    
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {X.shape}")
    print(f"åŸå§‹æ ‡ç­¾åˆ†å¸ƒ: é˜³æ€§(ç™Œç—‡)={y.sum()}, é˜´æ€§(å¥åº·)={len(y)-y.sum()}")
    print(f"åŸå§‹é˜³æ€§æ¯”ä¾‹: {y.mean():.1%}")
    
    # ==============================
    # 2. ä½¿ç”¨é¢„å®šä¹‰çš„æœ€ä½³8ç‰¹å¾
    # ==============================
    print(f"\n{'='*50}")
    print("ä½¿ç”¨é¢„å®šä¹‰çš„æœ€ä½³8ç‰¹å¾ (åŸºäºRFEé¢„ç­›é€‰ç»“æœ)")
    print(f"{'='*50}")
    
    # é¢„å®šä¹‰çš„æœ€ä½³8ç‰¹å¾
    BEST_8_FEATURES = [
        'Feature63', 'Feature2', 'Feature46', 'Feature61', 
        'Feature56', 'Feature42', 'Feature39', 'Feature43'
    ]
    
    # æ£€æŸ¥è¿™äº›ç‰¹å¾æ˜¯å¦å­˜åœ¨äºæ•°æ®ä¸­
    available_features = [f for f in BEST_8_FEATURES if f in features]
    missing_features = [f for f in BEST_8_FEATURES if f not in features]
    
    if missing_features:
        print(f"âš ï¸  ç¼ºå¤±ç‰¹å¾: {missing_features}")
        print(f"âœ… å¯ç”¨ç‰¹å¾: {available_features}")
    else:
        print(f"âœ… æ‰€æœ‰é¢„å®šä¹‰ç‰¹å¾å‡å¯ç”¨")
    
    # é€‰æ‹©å¯ç”¨çš„ç‰¹å¾
    selected_features = available_features
    feature_indices = [features.index(f) for f in selected_features]
    X_selected = X.iloc[:, feature_indices].values
    
    # è®¡ç®—è¿™äº›ç‰¹å¾çš„F-score (ç”¨äºåç»­åˆ†æ)
    from sklearn.feature_selection import f_classif
    feature_scores, _ = f_classif(X_selected, y)
    
    print(f"ä½¿ç”¨çš„ {len(selected_features)} ä¸ªç‰¹å¾åŠå…¶F-score:")
    for i, (feature, score) in enumerate(zip(selected_features, feature_scores)):
        print(f"  {i+1:2d}. {feature}: F-score={score:.2f}")
    
    # ä¸è¿›è¡Œæ•°æ®æ ‡å‡†åŒ–ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹ç‰¹å¾æ•°æ®
    
    # ==============================
    # 3. 10æŠ˜äº¤å‰éªŒè¯ (åœ¨æ¯æŠ˜è®­ç»ƒé›†ä¸Šå•ç‹¬åº”ç”¨KMeans-SMOTE)
    # ==============================
    print(f"\n{'='*50}")
    print("10æŠ˜äº¤å‰éªŒè¯ (æ¯æŠ˜è®­ç»ƒé›†å•ç‹¬åº”ç”¨KMeans-SMOTE)")
    print(f"{'='*50}")
    
    kf = KFold(n_splits=10, shuffle=True, random_state=random_state)
    
    # å­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœ
    all_predictions = []
    all_probabilities = []
    all_true_labels = []
    all_indices = []
    
    overall_scores = []
    
    for fold, (train_idx, test_idx) in enumerate(kf.split(X_selected), 1):
        print(f"å¤„ç† Fold {fold}...")
        
        # ä½¿ç”¨é€‰æ‹©çš„ç‰¹å¾æ•°æ®è¿›è¡Œåˆ’åˆ†
        X_train_fold, X_test_fold = X_selected[train_idx], X_selected[test_idx]
        y_train_fold, y_test_fold = y.iloc[train_idx], y.iloc[test_idx]
        
        print(f"  åŸå§‹è®­ç»ƒé›†: é˜³æ€§={y_train_fold.sum()}, é˜´æ€§={len(y_train_fold)-y_train_fold.sum()}")
        
        # ä»…åœ¨è®­ç»ƒé›†ä¸Šåº”ç”¨KMeans-SMOTEï¼Œè°ƒæ•´å‚æ•°ä»¥å¤„ç†å°æ ·æœ¬é—®é¢˜
        try:
            kmeans_smote_fold = KMeansSMOTE(
                k_neighbors=4,  # å‡å°‘é‚»å±…æ•°é‡
                cluster_balance_threshold='auto',  # è‡ªåŠ¨è°ƒæ•´èšç±»å¹³è¡¡é˜ˆå€¼
                random_state=random_state,
                n_jobs=1  # å•çº¿ç¨‹é¿å…å¹¶å‘é—®é¢˜
            )
        except Exception as e:
            print(f"  âš ï¸  KMeans-SMOTEå¤±è´¥ï¼Œæ”¹ç”¨SMOTE: {e}")
            from imblearn.over_sampling import SMOTE
            kmeans_smote_fold = SMOTE(
                k_neighbors=4,
                random_state=random_state
            )
        
        X_train_resampled, y_train_resampled = kmeans_smote_fold.fit_resample(X_train_fold, y_train_fold)
        X_train_resampled = np.array(X_train_resampled)
        y_train_resampled = np.array(y_train_resampled)
        
        print(f"  å¹³è¡¡åè®­ç»ƒé›†: é˜³æ€§={y_train_resampled.sum()}, é˜´æ€§={len(y_train_resampled)-y_train_resampled.sum()}")
        
        # è®­ç»ƒæ¨¡å‹ (ä½¿ç”¨å¹³è¡¡åçš„è®­ç»ƒé›†)
        clf = TabPFNClassifier(
            device=device,
            n_estimators=n_estimators,
            softmax_temperature=softmax_temperature,
            balance_probabilities=balance_probabilities,
            average_before_softmax=average_before_softmax,
            ignore_pretraining_limits=ignore_pretraining_limits,
            random_state=random_state
        )
        clf.fit(X_train_resampled, y_train_resampled)
        
        # åœ¨åŸå§‹æµ‹è¯•é›†ä¸Šé¢„æµ‹ (é‡è¦ï¼šä¸ä½¿ç”¨å¹³è¡¡åçš„æµ‹è¯•é›†)
        y_pred = clf.predict(X_test_fold)
        y_pred_proba = clf.predict_proba(X_test_fold)
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        all_predictions.extend(y_pred)
        all_probabilities.extend(y_pred_proba[:, 1])
        all_true_labels.extend(y_test_fold.values)
        all_indices.extend(test_idx)
        
        # è®¡ç®—foldæ€§èƒ½
        acc = accuracy_score(y_test_fold, y_pred)
        auc = roc_auc_score(y_test_fold, y_pred_proba[:, 1])
        f1 = f1_score(y_test_fold, y_pred)
        
        overall_scores.append({
            'fold': fold,
            'accuracy': acc,
            'auc': auc,
            'f1': f1
        })
        
        print(f"  æµ‹è¯•é›†æ€§èƒ½ - AUC: {auc:.4f}, ACC: {acc:.4f}, F1: {f1:.4f}")
    
    # ==============================
    # 6. é‡å»ºå®Œæ•´çš„é¢„æµ‹ç»“æœ
    # ==============================
    print(f"\n{'='*50}")
    print("é‡å»ºå®Œæ•´é¢„æµ‹ç»“æœ")
    print(f"{'='*50}")
    
    # åˆ›å»ºå®Œæ•´çš„é¢„æµ‹ç»“æœDataFrame (æŒ‰åŸå§‹ç´¢å¼•é¡ºåº)
    prediction_results = pd.DataFrame({
        'Original_Index': all_indices,
        'True_Label': all_true_labels,
        'Predicted_Label': all_predictions,
        'Malignant_Probability': all_probabilities
    })
    
    # æŒ‰åŸå§‹ç´¢å¼•æ’åº
    prediction_results = prediction_results.sort_values('Original_Index').reset_index(drop=True)
    
    # è®¡ç®—æ•´ä½“æ€§èƒ½
    overall_acc = accuracy_score(prediction_results['True_Label'], prediction_results['Predicted_Label'])
    overall_auc = roc_auc_score(prediction_results['True_Label'], prediction_results['Malignant_Probability'])
    overall_f1 = f1_score(prediction_results['True_Label'], prediction_results['Predicted_Label'])
    
    print(f"äº¤å‰éªŒè¯æ•´ä½“æ€§èƒ½:")
    print(f"  AUC: {overall_auc:.4f}")
    print(f"  ACC: {overall_acc:.4f}")
    print(f"  F1: {overall_f1:.4f}")
    
    # ==============================
    # 7. ç™Œç—‡ç±»å‹åˆ†æ
    # ==============================
    print(f"\n{'='*50}")
    print("ç™Œç—‡ç±»å‹æ£€å‡ºç‡åˆ†æ (åŸºäºäº¤å‰éªŒè¯é¢„æµ‹)")
    print(f"{'='*50}")
    
    # åˆ›å»ºåˆ†æç»“æœDataFrame
    analysis_results = pd.DataFrame({
        'True_Label': prediction_results['True_Label'].values,
        'Predicted_Label': prediction_results['Predicted_Label'].values,
        'Malignant_Probability': prediction_results['Malignant_Probability'].values,
        'Cancer_Type': df['Type_Raw_English'].values,
        'T_Stage': df['T_stage'].values,
        'Stage_Raw': df['Stage_Raw'].values
    })
    
    analysis_results['Correct_Prediction'] = (analysis_results['True_Label'] == analysis_results['Predicted_Label'])
    
    # åªåˆ†æçœŸå®çš„é˜³æ€§æ ·æœ¬
    positive_results = analysis_results[analysis_results['True_Label'] == 1].copy()
    
    cancer_analysis = []
    cancer_counts = positive_results['Cancer_Type'].value_counts()
    all_cancer_types = cancer_counts.index.tolist()
    
    for cancer_type in all_cancer_types:
        subset = positive_results[positive_results['Cancer_Type'] == cancer_type]
        
        total_samples = len(subset)
        correct_predictions = subset['Correct_Prediction'].sum()
        wrong_predictions = total_samples - correct_predictions
        accuracy = correct_predictions / total_samples
        avg_malignant_prob = subset['Malignant_Probability'].mean()
        
        cancer_analysis.append({
            'Cancer_Type': cancer_type,
            'Total_Samples': total_samples,
            'Correctly_Predicted_as_Positive': correct_predictions,
            'Wrongly_Predicted_as_Negative': wrong_predictions,
            'Detection_Rate': accuracy,
            'Avg_Malignant_Probability': avg_malignant_prob
        })
        
        print(f"\n{cancer_type}:")
        print(f"  æ€»æ ·æœ¬: {total_samples}")
        print(f"  æ­£ç¡®è¯†åˆ«ä¸ºç™Œç—‡: {correct_predictions} ({accuracy:.3f})")
        print(f"  é”™è¯¯åˆ¤æ–­ä¸ºå¥åº·: {wrong_predictions}")
        print(f"  å¹³å‡æ¶æ€§æ¦‚ç‡: {avg_malignant_prob:.3f}")
        if accuracy < 0.8:
            print(f"  âš ï¸  è¯¥ç™Œç—‡ç±»å‹æ£€å‡ºç‡è¾ƒä½ï¼")
        elif accuracy > 0.95:
            print(f"  âœ… è¯¥ç™Œç—‡ç±»å‹æ£€å‡ºç‡å¾ˆé«˜")
    
    # ==============================
    # 8. å¯¹æ¯”åˆ†æ
    # ==============================
    print(f"\n{'='*50}")
    print("æ”¹è¿›æ•ˆæœå¯¹æ¯”")
    print(f"{'='*50}")
    
    # æ··æ·†çŸ©é˜µ
    conf_matrix = confusion_matrix(prediction_results['True_Label'], prediction_results['Predicted_Label'])
    tn, fp, fn, tp = conf_matrix.ravel()
    
    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    
    print(f"æ··æ·†çŸ©é˜µ:")
    print(f"çœŸé˜´æ€§(TN): {tn:3d} | å‡é˜³æ€§(FP): {fp:3d}")
    print(f"å‡é˜´æ€§(FN): {fn:3d} | çœŸé˜³æ€§(TP): {tp:3d}")
    print(f"\næ•æ„Ÿåº¦(Sensitivity): {sensitivity:.3f}")
    print(f"ç‰¹å¼‚åº¦(Specificity): {specificity:.3f}")
    
    # ==============================
    # 9. ä¿å­˜ç»“æœ
    # ==============================
    
    overall_df = pd.DataFrame(overall_scores)
    
    # ä¿å­˜ç‰¹å¾é€‰æ‹©ç»“æœ
    feature_selection_df = pd.DataFrame({
        'Feature': selected_features,
        'F_Score': feature_scores
    }).sort_values('F_Score', ascending=False)
    feature_selection_df.to_csv(f'{base_path}/Selected_Features_Top{k_best_features}.csv', index=False)
    
    # ä¿å­˜äº¤å‰éªŒè¯ç»“æœ
    overall_df.to_csv(f'{base_path}/KMeans_SMOTE_CV_Results.csv', index=False)
    
    # ä¿å­˜äº¤å‰éªŒè¯é¢„æµ‹ç»“æœ
    analysis_results.to_csv(f'{base_path}/KMeans_SMOTE_CrossValidation_Predictions.csv', index=False)
    
    # ä¿å­˜ç™Œç—‡ç±»å‹åˆ†æ
    cancer_analysis_df = pd.DataFrame(cancer_analysis)
    cancer_analysis_df.to_csv(f'{base_path}/KMeans_SMOTE_Cancer_Type_Analysis.csv', index=False)
    
    # ==============================
    # 10. å¯è§†åŒ–
    # ==============================
    
    if len(cancer_analysis) > 0:
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # å­å›¾1: ç‰¹å¾é‡è¦æ€§
        axes[0,0].bar(range(len(selected_features)), feature_scores)
        axes[0,0].set_title(f'Top {k_best_features} Feature Scores')
        axes[0,0].set_ylabel('F-Score')
        axes[0,0].set_xticks(range(len(selected_features)))
        axes[0,0].set_xticklabels([f[:8] for f in selected_features], rotation=45)
        
        # å­å›¾2: æ•°æ®å¹³è¡¡å‰åå¯¹æ¯”
        categories = ['Before SMOTE', 'After SMOTE (avg per fold)']
        # ä½¿ç”¨ç†è®ºä¸Šçš„å¹³è¡¡æ•°æ®è¿›è¡Œæ¯”è¾ƒï¼ˆæ¯ä¸ªç±»åˆ«æ ·æœ¬æ•°ç›¸ç­‰ï¼‰
        original_pos = y.sum()
        original_neg = len(y) - y.sum()
        # ä¼°ç®—å¹³è¡¡åçš„æ ·æœ¬æ•°ï¼ˆå–è¾ƒå¤§çš„ç±»åˆ«æ•°é‡ï¼‰
        balanced_count = max(original_pos, original_neg)
        positive_counts = [original_pos, balanced_count]
        negative_counts = [original_neg, balanced_count]
        
        x = np.arange(len(categories))
        width = 0.35
        axes[0,1].bar(x - width/2, positive_counts, width, label='Positive', alpha=0.8)
        axes[0,1].bar(x + width/2, negative_counts, width, label='Negative', alpha=0.8)
        axes[0,1].set_title('Data Balance Comparison')
        axes[0,1].set_ylabel('Sample Count')
        axes[0,1].set_xticks(x)
        axes[0,1].set_xticklabels(categories)
        axes[0,1].legend()
        
        # å­å›¾3: ç™Œç—‡ç±»å‹æ£€å‡ºç‡
        cancer_analysis_df = pd.DataFrame(cancer_analysis)
        axes[0,2].bar(range(len(cancer_analysis_df)), cancer_analysis_df['Detection_Rate'])
        axes[0,2].set_title('Cancer Detection Rate by Type')
        axes[0,2].set_ylabel('Detection Rate')
        axes[0,2].set_xticks(range(len(cancer_analysis_df)))
        axes[0,2].set_xticklabels([ct[:10] + '...' if len(ct) > 10 else ct for ct in cancer_analysis_df['Cancer_Type']], rotation=45)
        axes[0,2].axhline(y=0.8, color='r', linestyle='--', alpha=0.5)
        
        # å­å›¾4: äº¤å‰éªŒè¯æ€§èƒ½
        metrics = ['accuracy', 'auc', 'f1']
        metric_means = [overall_df[metric].mean() for metric in metrics]
        metric_stds = [overall_df[metric].std() for metric in metrics]
        
        axes[1,0].bar(metrics, metric_means, yerr=metric_stds, capsize=5)
        axes[1,0].set_title('Cross-Validation Performance')
        axes[1,0].set_ylabel('Score')
        
        # å­å›¾5: æ··æ·†çŸ©é˜µ
        axes[1,1].imshow(conf_matrix, cmap='Blues', alpha=0.7)
        for i in range(2):
            for j in range(2):
                axes[1,1].text(j, i, conf_matrix[i, j], ha='center', va='center', fontsize=20)
        axes[1,1].set_title('Confusion Matrix (Original Data)')
        axes[1,1].set_xlabel('Predicted')
        axes[1,1].set_ylabel('Actual')
        axes[1,1].set_xticks([0, 1])
        axes[1,1].set_yticks([0, 1])
        axes[1,1].set_xticklabels(['Negative', 'Positive'])
        axes[1,1].set_yticklabels(['Negative', 'Positive'])
        
        # å­å›¾6: é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
        axes[1,2].hist(analysis_results[analysis_results['True_Label']==0]['Malignant_Probability'], 
                      bins=20, alpha=0.5, label='Healthy', color='blue')
        axes[1,2].hist(analysis_results[analysis_results['True_Label']==1]['Malignant_Probability'], 
                      bins=20, alpha=0.5, label='Cancer', color='red')
        axes[1,2].axvline(x=0.5, color='black', linestyle='--', alpha=0.5, label='Threshold')
        axes[1,2].set_title('Probability Distribution')
        axes[1,2].set_xlabel('Malignant Probability')
        axes[1,2].set_ylabel('Count')
        axes[1,2].legend()
        
        plt.tight_layout()
        plt.savefig(f'{base_path}/KMeans_SMOTE_Analysis_Plots.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    return overall_df, {
        'selected_features': selected_features,
        'feature_scores': feature_scores,
        'cv_performance': {
            'auc': overall_auc,
            'accuracy': overall_acc,
            'f1': overall_f1
        },
        'cancer_analysis': cancer_analysis,
        'confusion_matrix': conf_matrix,
        'selected_features_shape': X_selected.shape
    }

# ==============================
# è¿è¡Œåˆ†æ
# ==============================
if __name__ == "__main__":
    print("TabPFN + KMeans-SMOTE + ç‰¹å¾é€‰æ‹© ç™Œç—‡é¢„æµ‹åˆ†æ")
    print("="*60)
    
    # è¿è¡Œåˆ†æ
    results, analysis_data = analyze_with_kmeans_smote_and_feature_selection(
        device='cuda',
        n_estimators=32,
        softmax_temperature=0.9,
        balance_probabilities=False,
        average_before_softmax=False,
        ignore_pretraining_limits=True,
        random_state=42,
        k_best_features=8,
        base_path='./results/KMeans_SMOTE_Best8_Analysis'
    )
    
    print(f"\nåˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° ./results/KMeans_SMOTE_Best8_Analysis/")
    
    # è¾“å‡ºæ€»ç»“
    print("\n" + "="*60)
    print("æ”¹è¿›æ•ˆæœæ€»ç»“")
    print("="*60)
    
    print(f"é€‰æ‹©çš„æœ€ä½³8ä¸ªç‰¹å¾:")
    for i, (feature, score) in enumerate(zip(analysis_data['selected_features'], analysis_data['feature_scores'])):
        print(f"  {i+1:2d}. {feature}: {score:.2f}")
    
    print(f"\né€‰æ‹©çš„ç‰¹å¾å½¢çŠ¶: {analysis_data['selected_features_shape']}")
    
    cv_perf = analysis_data['cv_performance']
    print(f"\näº¤å‰éªŒè¯æœ€ç»ˆæ€§èƒ½:")
    print(f"  AUC: {cv_perf['auc']:.4f}")
    print(f"  ACC: {cv_perf['accuracy']:.4f}")
    print(f"  F1: {cv_perf['f1']:.4f}")
    
    cv_results = results
    print(f"\näº¤å‰éªŒè¯å¹³å‡æ€§èƒ½:")
    print(f"  AUC: {cv_results['auc'].mean():.4f} Â± {cv_results['auc'].std():.4f}")
    print(f"  ACC: {cv_results['accuracy'].mean():.4f} Â± {cv_results['accuracy'].std():.4f}")
    print(f"  F1: {cv_results['f1'].mean():.4f} Â± {cv_results['f1'].std():.4f}")
    
    cancer_analysis = analysis_data['cancer_analysis']
    if cancer_analysis:
        print(f"\nç™Œç—‡ç±»å‹æ£€å‡ºç‡:")
        for analysis in cancer_analysis:
            status = "âš ï¸ è¾ƒä½" if analysis['Detection_Rate'] < 0.8 else "âœ… è‰¯å¥½" if analysis['Detection_Rate'] > 0.95 else "ğŸ”¶ ä¸­ç­‰"
            print(f"  {analysis['Cancer_Type']}: {analysis['Detection_Rate']:.3f} {status}")
        
        total_cancer_samples = sum([a['Total_Samples'] for a in cancer_analysis])
        total_detected = sum([a['Correctly_Predicted_as_Positive'] for a in cancer_analysis])
        overall_detection_rate = total_detected / total_cancer_samples
        print(f"\næ•´ä½“ç™Œç—‡æ£€å‡ºç‡: {overall_detection_rate:.3f} ({total_detected}/{total_cancer_samples})") 