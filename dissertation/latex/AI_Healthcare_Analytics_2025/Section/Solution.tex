\section{Solution}
\label{sec:solution}
\label{sec:sol-start}

To bridge the gap between advanced tabular foundation models and the practical constraints of cross-hospital medical AI, we introduce PANDA (\textbf{P}retrained \textbf{A}daptation \textbf{N}etwork with \textbf{D}omain \textbf{A}lignment). PANDA is a composite algorithmic framework designed to predict pulmonary nodule malignancy with high stability across heterogeneous clinical environments. It explicitly addresses the tripartite challenge of small-sample scarcity, distribution shift, and feature heterogeneity through a tightly integrated pipeline.

We formalize the PANDA solution as a composite function $\pandafunc: \inputspace \to [0, 1]$ mapping raw, heterogeneous input space to a calibrated malignancy probability. The framework consists of four sequential stages: (1) Domain-Aware Feature Alignment and Selection, (2) Foundation Model Feature Extraction, (3) Latent Space Domain Adaptation, and (4) Multi-View Ensemble Classification.

\subsection{Architectural Overview}
\label{subsec:sol-architecture}

The PANDA framework operates as a directed acyclic graph (DAG) of data transformations, guiding raw clinical inputs through a carefully orchestrated sequence to produce a calibrated malignancy probability. Table \ref{tab:data-flow} details this journey, illustrating how feature representations evolve at each stage.

\begin{table}[htbp]
    \centering
    \caption{Data flow and transformations in the PANDA framework. The pipeline progressively refines the data from raw, high-dimensional inputs to low-dimensional, domain-invariant embeddings, and finally to a calibrated probability. In our pulmonary nodule experiments, $k = 8$, $d_{\cap} = 8$, and $m = 15$, but these dimensions are dataset-dependent.}
    \label{tab:data-flow}
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{lllll}
            \toprule
            \textbf{Stage} & \textbf{Component} & \textbf{Input Space} & \textbf{Core Operation} & \textbf{Output Space} \\
            \midrule
            1 & Cross-Domain RFE & $\mathbb{R}^{\rawdim}$ & Iterative elimination via $\rfeimportance(\func)$ & $\mathbb{R}^{k}$ ($\rfeselectedfeatures$, $k = |\rfeselectedfeatures|$) \\
            2 & Feature Alignment & $\mathbb{R}^{k}$ & Schema intersection on $\sharedfeatures$ & $\mathbb{R}^{d_{\cap}}$ ($d_{\cap} = |\sharedfeatures|$) \\
            3 & Latent TCA & $\mathbb{R}^{d_{\cap}}$ & Projection $\featurevec' = \tcaprojectionmatrix^\top \featurevec$ & $\mathbb{R}^{m}$ ($m$ = latent dimension) \\
            4 & TabPFN Classifier & $\mathbb{R}^{m}$ & Classification $\tabpfnfunc(\featurevec')$ & $[0, 1]$ (Prob.) \\
            5 & Ensemble & $[0, 1]^{\numpreprocessbranches \times \numrandomseeds}$ & Temperature-scaled averaging & $[0, 1]$ (Final) \\
            \bottomrule
        \end{tabular}%
    }
\end{table}



The data journey begins with \textbf{Stage 1: Cross-Domain Recursive Feature Elimination (RFE)}. From the raw feature set of $\rawdim$ dimensions, this stage systematically reduces the dimensionality to a concise set of $k$ clinically impactful features, denoted as $\rfeselectedfeatures$ (where $k = |\rfeselectedfeatures|$). This reduction serves to minimize the joint error ($\adaptabilityterm$) across domains and focuses on the most diagnostically relevant biomarkers for malignancy prediction. In our pulmonary nodule experiments, the cost-effectiveness criterion selects $k = 8$, but $k$ is dataset-dependent.

Following feature selection, \textbf{Stage 2: Feature Alignment} performs a robust schema intersection ($\sharedfeatures$) between the RFE-selected features and those available in each target hospital's dataset. This yields a shared feature schema of dimensionality $d_{\cap} = |\sharedfeatures|$ that is tailored to each pair of hospitals (for our cross-hospital pulmonary nodule cohort, $d_{\cap} = 8$). This crucial step effectively addresses the "missingness shift" by ensuring that only universally available and relevant variables proceed through the pipeline, adapting the feature set to the specific constraints of each clinical site.

Subsequently, in \textbf{Stage 3: Latent Transfer Component Analysis (TCA)}, the refined feature set $\mathbb{R}^{d_{\cap}}$ is adapted. TCA projects these features into a lower-dimensional, domain-invariant space $\mathbb{R}^{m}$, with $m$ treated as a tunable latent dimension (we use $m = 15$ in the reported experiments). This projection, $\featurevec' = \tcaprojectionmatrix^\top \featurevec$, is learned to minimize the Maximum Mean Discrepancy (MMD) between the source and target domain distributions in this latent space, thereby mitigating covariate shift.

\textbf{Stage 4: TabPFN Classifier} then processes the TCA-transformed features to predict the initial malignancy probability. In this stage, the TabPFN acts as a robust, pre-trained classifier on the domain-adapted data. Its in-context learning capabilities allow it to infer complex, non-linear decision boundaries without extensive gradient updates on small medical datasets.

Finally, \textbf{Stage 5: Ensemble} aggregates predictions from multiple TabPFN Classifier instances. This involves temperature scaling of the individual TabPFN outputs, followed by averaging across $\numpreprocessbranches \times \numrandomseeds$ ensemble members, yielding the final, calibrated malignancy probability. The temperature scaling is crucial for adjusting predicted probabilities to account for potential label shifts between domains, ensuring more reliable and interpretable outputs.

\subsubsection*{Mapping to Formal Problem Formulation}
To establish a clear connection between the PANDA framework's architectural stages and its formal definition in Equation \ref{eq:panda-formalization}, we explicitly map the components of $\pandafunc(\featurevec) = \hypothesis \circ \adaptmap \circ \pi_{\cap} \circ \rfeop (\featurevec)$ to the corresponding stages:
\begin{itemize}
    \item $\rfeselectedfeatures \leftarrow$ Stage 1 (Cross-Domain RFE)
    \item $\sharedfeatures \leftarrow$ Stage 2 (Feature Alignment)
    \item $\adaptmap \leftarrow$ Stage 3 (Latent TCA)
    \item $\hypothesis \leftarrow$ Stages 4--5 (TabPFN Classifier + Ensemble/Calibration)
\end{itemize}
This mapping clarifies how the abstract components of the PANDA function are instantiated through the concrete processing steps, providing readers with a direct link between theory and implementation.

\subsection{Core Component I: TabPFN as a Robust Classifier}
\label{subsec:sol-tabpfn}

The backbone of PANDA is the TabPFN (Tabular Prior-Data Fitted Network) foundation model. While TabPFN is capable of extracting embeddings, in the PANDA framework, we leverage it primarily as a robust \textbf{Classifier} operating on the domain-adapted features. This strategic choice is driven by TabPFN's unique ability to generalize well on small tabular datasets without extensive domain-specific fine-tuning.
Mathematically, after the feature set $\featurevec$ has been reduced and aligned (Stages 1-2) and then adapted by TCA into $\featurevec'$ (Stage 3), the TabPFN classifier $\tabpfnfunc(\cdot)$ directly maps these transformed features to a malignancy probability:
\begin{equation}
    \tabpfnfunc(\featurevec') \in [0, 1]
\end{equation}
Here, the entire pre-trained TabPFN model, including its Transformer encoder and classification head, is used to make predictions.

\textbf{Theoretical Advantage (Prior-Data Fitted Learning):}
The core innovation here is leveraging the \textbf{Prior-Data Fitted (PFN)} nature of TabPFN. TabPFN was meta-trained on millions of synthetic datasets generated from Structural Causal Models (SCMs). This meta-training instills a strong Bayesian prior $\priorfunc$ that favors simple, causal explanations over spurious correlations. In the context of medical data, where sample sizes are often small ($\sourcedatasize \approx 300$), traditional deep learning models typically overfit. However, TabPFN's meta-learned prior allows it to infer complex, non-linear decision boundaries (e.g., non-linear interactions between Age and Nodule Size) on the adapted features without requiring gradient updates on the small medical dataset. This directly addresses the "Small Sample Size" constraint defined in Equation \ref{eq:small-sample-constraint}. By applying TabPFN to TCA-transformed features, we benefit from both domain invariance and strong generalization capabilities on small cohorts.

\subsection{Core Component II: Cross-Domain RFE with Cost-Effectiveness Index}
\label{subsec:sol-rfe}

Medical datasets often contain high-dimensional noise (e.g., irrelevant radiomics features) and "concept shift" (features whose predictive value changes across hospitals). To handle this, we employ a Cross-Domain Recursive Feature Elimination (RFE) mechanism.
The selection process is not merely about maximizing \auc; it is governed by a global \textbf{Cost-Effectiveness Index (CEI)} that balances predictive power against stability, clinical acquisition cost, and model complexity. We formulate the optimization problem for the optimal feature subset size $\rfecurrentdim^*$ as:
\begin{equation}
    \rfeselectedfeatures = \argmax_{\rfecurrentdim} \left( \rfecomponentweights_1 \rfeperformance(\rfecurrentdim) + \rfecomponentweights_2 \rfeefficiency(\rfecurrentdim) + \rfecomponentweights_3 \rfestability(\rfecurrentdim) + \rfecomponentweights_4 \rfesimplicity(\rfecurrentdim) \right)
\end{equation}
where the components are defined as:
\begin{itemize}
    \item \textbf{Performance ($\rfeperformance$):} A weighted sum of discriminative metrics on the source validation folds: $\rfeperformance = 0.5 \cdot \auc + 0.3 \cdot \accuracy + 0.2 \cdot \fonescore$.
    \item \textbf{Efficiency ($\rfeefficiency$):} Measures the computational efficiency, defined as $1 - \frac{\bar{T} - T_{\min}}{T_{\max} - T_{\min}}$, where $\bar{T}$ is the mean training time for the feature subset.
    \item \textbf{Stability ($\rfestability$):} Quantifies the robustness across multiple metrics, defined as $1 - \frac{1}{3} (\sigma_{\text{AUC}} + \sigma_{\text{Acc}} + \sigma_{\text{F1}})$, where $\sigma$ denotes the normalized standard deviation across folds.
    \item \textbf{Simplicity ($\rfesimplicity$):} A sparsity penalty $\exp(-\sparsityparam \rfecurrentdim)$ to prefer smaller, clinically manageable subsets (typically $\rfecurrentdim \in [5, 10]$).
\end{itemize}

Implementation-wise, we use `TabPFNClassifier` as the base estimator for RFE. In each iteration, we calculate the \textbf{Permutation Importance} $\rfeimportance(\featurevecj)$ of each feature $\featurevecj$. The feature with the lowest importance is pruned, and the model is re-evaluated. This recursive process ensures that the retained features $\rfeselectedfeatures$ are not only predictive but also stable against the noise inherent in small cohorts. In our experiments, this process consistently identifies a "Best-8" subset (Age, Spiculation, Lobulation, Diameter, etc.) that is robust across hospitals.

\subsection{Core Component III: Latent Space TCA Adaptation}
\label{subsec:sol-tca}

To explicitly align the source and target distributions, we apply Transfer Component Analysis (TCA) to the feature space reduced by RFE (Stage 2). Standard TCA is often applied to raw features, but this is suboptimal for complex medical data where the manifold structure might be noisy or high-dimensional. Instead, we construct the kernel matrix $\kernelmatrix$ using the RFE-selected features $\featurevec$. Specifically, we employ a \textbf{Linear Kernel} on these features:
\begin{equation}
    \kernelmatrix_{ij} = \langle \featurevec_i, \featurevec_j \rangle
\end{equation}
This projection, $\featurevec' = \tcaprojectionmatrix^\top \featurevec$, learns a domain-invariant feature representation that is then fed into the TabPFN Classifier (Stage 4).

\textbf{Justification for Linear Kernel on RFE-Selected Features:}
After Cross-Domain RFE, the selected features are assumed to reside in a more disentangled and clinically relevant space. In this context, a simple linear alignment via TCA is often sufficient to match the first-order moments of the distributions, thereby reducing covariate shift. This approach avoids the complexity and hyperparameter sensitivity (e.g., $\gammaK$ in RBF kernels) that often plagues non-linear kernel methods, especially on small datasets. The subsequent TabPFN Classifier is then capable of modeling complex non-linear relationships within this linearly adapted feature space.

The optimization objective minimizes the Maximum Mean Discrepancy (MMD) trace:
\begin{equation}
    \min_{\tcaprojectionmatrix} \quad \tr({\tcaprojectionmatrix}^\top \kernelmatrix \mmdmatrix \kernelmatrix \tcaprojectionmatrix) + \mu \tr({\tcaprojectionmatrix}^\top \tcaprojectionmatrix)
\end{equation}
where $\mmdmatrix$ is the MMD matrix defined block-wise as $\mmdmatrix_{ij} = 1/\sourcedatasize^2$ if $\featurevec_i, \featurevec_j \in \sourcedata$, $1/\targetdatasize^2$ if $\featurevec_i, \featurevec_j \in \targetdata$, and $-1/(\sourcedatasize \targetdatasize)$ otherwise. The projection $\tcaprojectionmatrix$ aligns the marginals $\marginalsourcedist(\mathbf{z})$ and $\marginaltargetdist(\mathbf{z})$, directly addressing the Covariate Shift challenge.
Figure~\ref{fig:tca_dimensionality} visualizes how the TCA projection compresses the RFE-reduced features into a compact latent subspace while shrinking the MMD divergence between the source and target domains, highlighting the balance controlled by $\mu$.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{img/cross_hospital/TCA_dimensionality_reduction.pdf}
\caption{\textbf{TCA-based domain adaptation visualization.} a,b PCA views before and after TCA transformation, showing the tightened alignment between source and target. c,d t-SNE views before and after TCA, demonstrating improved cluster center alignment while preserving local structure.}
    \label{fig:tca_dimensionality}
\end{figure}


\subsection{Core Component IV: Multi-View Ensemble and Calibration}
\label{subsec:sol-ensemble}

To mitigate the variance associated with small-sample learning and label shift, PANDA employs a \textbf{Strategic Multi-Branch Ensemble}. Instead of a single model, we construct an ensemble of $N=32$ members, derived from $\numpreprocessbranches=4$ distinct preprocessing strategies expanded by $\numrandomseeds=8$ random seeds (which control feature shuffling and rotational invariants).

The four strategic branches are designed to present different "views" of the data to the foundation model:
\begin{enumerate}
    \item \textbf{High-Complexity Ordinal (Raw+Quantile):} Features are mapped to a uniform distribution via `QuantileTransformer` (handling outliers) and concatenated with raw features. Categorical variables are Ordinal Encoded.
    \item \textbf{Low-Complexity Ordinal (Raw):} Raw features are used directly (relying on TabPFN's internal robustness). Categorical variables are Ordinal Encoded.
    \item \textbf{High-Complexity Numeric:} Similar to Branch 1, but categorical variables are treated as numeric.
    \item \textbf{Low-Complexity Numeric:} Raw features with numeric encoding for categoricals.
\end{enumerate}

\textbf{Rotational Invariance:}
For each branch, we train 8 versions. In each version, the feature columns are cyclically permuted (Rotated). This is critical because Transformers can exhibit positional bias; rotating the features ensures that the model's attention mechanism attends to all features equally, regardless of their column index.

\textbf{Temperature Scaling and Aggregation:}
The final probability is obtained via \textbf{Temperature Scaling} to calibrate the predictions from the ensemble of TabPFN Classifiers. This is crucial when the target domain prevalence differs from the source (Label Shift), as uncalibrated models often produce overconfident probabilities. The aggregated prediction is:
\begin{equation}
    \hat{\labelval}(\featurevec) = \frac{1}{\numpreprocessbranches \times \numrandomseeds} \sum_{i=1}^{\numpreprocessbranches \times \numrandomseeds} \activation\left(\frac{\logitoutput}{\temperature}\right)
\end{equation}
where $\logitoutput$ is the logit output of the $i$-th TabPFN Classifier member, and $\temperature=0.9$ is the temperature parameter empirically tuned to soften predictions.

\subsection{Theoretical Justification and Feasibility}
\label{subsec:sol-justification}

\textbf{Addressing Small Sample Size:}
The use of a frozen, pre-trained encoder $\tabpfnencoder$ avoids the need to train a deep network from scratch on $\sourcedatasize \approx 300$ samples. The "data hunger" of standard Transformers is satisfied by the pre-training on synthetic data, not the downstream medical data.

\textbf{Addressing Distribution Shift:}
The framework attacks shift at three levels:
1.  \textbf{Feature Level:} `QuantileTransformer` aligns the marginal distributions of individual features (Covariate Shift).
2.  \textbf{Latent Level:} TCA explicitly minimizes the divergence term $\domaindivergence$ in the generalization bound by aligning the joint embedding distributions.
3.  \textbf{Output Level:} Temperature scaling calibrates the posterior probabilities against Label Shift.

\textbf{Real-time Feasibility:}
Despite the ensemble complexity ($N=32$), the inference involves only forward passes of the Transformer (which are highly parallelizable) and linear projections. Empirical tests on a standard CPU (Intel i7) show an average inference latency of $< 200$ ms per patient. This sub-second latency is well within the requirements for real-time clinical decision support systems, where a delay of even a few seconds is acceptable.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{img/cross_hospital/Pre-trained Tabular Foundation Mode Pipeline_new.pdf}
    \caption{\textbf{The PANDA framework architecture.}
    (a) Compositional pipeline: from original tabular data through ensemble training, prediction aggregation, class imbalance adjustment, to final classification output.
    (b) Multi-branch ensemble with $\numpreprocessbranches=4$ preprocessing strategies,
    each generating $\numrandomseeds=8$ ensemble members via different random seeds.}
    \label{fig:model_details}
\end{figure}
\label{sec:sol-end}
