\section{Solution: The PANDA Framework}
\label{sec:solution}
\label{sec:sol-start}

To bridge the gap between advanced tabular foundation models and the practical constraints of cross-hospital medical AI, we introduce PANDA (\textbf{P}re-trained t\textbf{A}bular fou\textbf{N}dation model with \textbf{D}omain \textbf{A}daptation). PANDA is a composite algorithmic framework designed to predict pulmonary nodule malignancy with high stability across heterogeneous clinical environments. It explicitly addresses the tripartite challenge of small-sample scarcity, distribution shift, and feature heterogeneity through a tightly integrated pipeline.

We formalize the PANDA solution as a composite function $\pandafunc: \inputspace \to [0, 1]$ mapping raw, heterogeneous input space to a calibrated malignancy probability. The framework consists of four sequential stages: (1) Domain-Aware Feature Alignment and Selection, (2) Foundation Model Feature Extraction, (3) Latent Space Domain Adaptation, and (4) Multi-View Ensemble Classification.

\subsection{Architectural Overview}
\label{subsec:sol-architecture}

The PANDA framework operates as a directed acyclic graph (DAG) of data transformations, guiding raw clinical inputs through a carefully orchestrated sequence to produce a calibrated malignancy probability. Table \ref{tab:data-flow} details this journey, illustrating how feature representations evolve at each stage.

\begin{table}[htbp]
    \centering
    \caption{Data flow and transformations in the PANDA framework. The pipeline progressively refines the data from raw, high-dimensional inputs to low-dimensional, domain-invariant embeddings, and finally to a calibrated probability.}
    \label{tab:data-flow}
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{lllll}
            \toprule
            \textbf{Stage} & \textbf{Component} & \textbf{Input Space} & \textbf{Core Operation} & \textbf{Output Space} \\
            \midrule
            1 & Feature Alignment & $\mathbb{R}^{\rawdim}$ & Schema intersection $\cap$ & $\mathbb{R}^{\intersectdim}$ \\
            2 & Cross-Domain RFE & $\mathbb{R}^{\intersectdim}$ & Iterative elimination via $\rfeimportance(\func)$ & $\mathbb{R}^{\rfecurrentdim}$ ($\rfecurrentdim \approx 8$) \\
            3 & Latent TCA & $\mathbb{R}^{\rfecurrentdim}$ & Projection $\featurevec' = \tcaprojectionmatrix^\top \featurevec$ & $\mathbb{R}^{\tcaprojectdim}$ ($\tcaprojectdim=15$) \\
            4 & TabPFN Classifier & $\mathbb{R}^{\tcaprojectdim}$ & Classification $\tabpfnfunc(\featurevec')$ & $[0, 1]$ (Prob.) \\
            5 & Ensemble & $[0, 1]^{\numpreprocessbranches \times \numrandomseeds}$ & Temperature-scaled averaging & $[0, 1]$ (Final) \\
            \bottomrule
        \end{tabular}%
    }
\end{table}



The data journey begins with \textbf{Stage 1: Feature Alignment}. Here, we perform a robust schema intersection ($\sharedfeatures$) between the features available in the source domain and each target hospital's dataset. This crucial step effectively addresses the "missingness shift" by identifying the maximal common feature set, ensuring that only universally collected and relevant variables proceed through the pipeline, even if certain examinations are not uniformly performed across all clinical sites.

Following alignment, \textbf{Stage 2: Cross-Domain Recursive Feature Elimination (RFE)} takes over. From the $\intersectdim$ features, this stage systematically reduces the dimensionality to a concise set of $\rfecurrentdim \approx 8$ clinically impactful features, denoted as $\rfeselectedfeatures$. This reduction is not arbitrary; it serves to minimize the joint error ($\adaptabilityterm$) across domains and guarantees that the selected features are practically acquirable by all participating hospitals. This process aligns directly with the feature set $\rfeselectedfeatures$ and the target feature space for domain $t$, $\rfeiterateset$, as introduced in Section 3.6.1.

Subsequently, in \textbf{Stage 3: Latent Transfer Component Analysis (TCA)}, the refined feature set $\mathbb{R}^{\rfecurrentdim}$ is adapted. TCA projects these features into a lower-dimensional, domain-invariant space $\mathbb{R}^{\tcaprojectdim}$ (typically $\tcaprojectdim=15$). This projection, $\featurevec' = \tcaprojectionmatrix^\top \featurevec$, is learned to minimize the Maximum Mean Discrepancy (MMD) between the source and target domain distributions in this latent space, thereby mitigating covariate shift.

\textbf{Stage 4: TabPFN Classifier} then processes the TCA-transformed features to predict the initial malignancy probability. In this stage, the TabPFN acts as a robust, pre-trained classifier on the domain-adapted data. Its in-context learning capabilities allow it to infer complex, non-linear decision boundaries without extensive gradient updates on small medical datasets.

Finally, \textbf{Stage 5: Ensemble} aggregates predictions from multiple TabPFN Classifier instances. This involves temperature scaling of the individual TabPFN outputs, followed by averaging across $\numpreprocessbranches \times \numrandomseeds$ ensemble members, yielding the final, calibrated malignancy probability. The temperature scaling is crucial for adjusting predicted probabilities to account for potential label shifts between domains, ensuring more reliable and interpretable outputs.

\subsubsection*{Mapping to Formal Problem Formulation}
To establish a clear connection between the PANDA framework's architectural stages and its formal problem formulation presented in Section 3.6, we explicitly map the components of $\pandafunc(\featurevec) = \hypothesis(\adaptmap(\featurevec; \rfeselectedfeatures))$ to the corresponding stages:
\begin{itemize}
    \item $\rfeselectedfeatures \leftarrow$ Stages 1--2 (Schema Intersection + Cross-Domain RFE)
    \item $\adaptmap \leftarrow$ Stage 3 (Latent TCA)
    \item $\hypothesis \leftarrow$ Stages 4--5 (TabPFN Classifier + Ensemble/Calibration)
\end{itemize}
This mapping clarifies how the abstract components of the PANDA function are instantiated through the concrete processing steps, providing readers with a direct link between theory and implementation.

\subsection{Core Component I: TabPFN as a Robust Classifier}
\label{subsec:sol-tabpfn}

The backbone of PANDA is the TabPFN (Tabular Prior-Data Fitted Network) foundation model. While TabPFN is capable of extracting embeddings, in the PANDA framework, we leverage it primarily as a robust \textbf{Classifier} operating on the domain-adapted features. This strategic choice is driven by TabPFN's unique ability to generalize well on small tabular datasets without extensive domain-specific fine-tuning.
Mathematically, after the feature set $\featurevec$ has been aligned and reduced (Stages 1-2) and then adapted by TCA into $\featurevec'$ (Stage 3), the TabPFN classifier $\tabpfnfunc(\cdot)$ directly maps these transformed features to a malignancy probability:
\begin{equation}
    \tabpfnfunc(\featurevec') \in [0, 1]
\end{equation}
Here, the entire pre-trained TabPFN model, including its Transformer encoder and classification head, is used to make predictions.

\textbf{Theoretical Advantage (Prior-Data Fitted Learning):}
The core innovation here is leveraging the \textbf{Prior-Data Fitted (PFN)} nature of TabPFN. TabPFN was meta-trained on millions of synthetic datasets generated from Structural Causal Models (SCMs). This meta-training instills a strong Bayesian prior $\priorfunc$ that favors simple, causal explanations over spurious correlations. In the context of medical data, where sample sizes are often small ($\sourcedatasize \approx 300$), traditional deep learning models typically overfit. However, TabPFN's meta-learned prior allows it to infer complex, non-linear decision boundaries (e.g., non-linear interactions between Age and Nodule Size) on the adapted features without requiring gradient updates on the small medical dataset. This directly addresses the "Small Sample Size" constraint defined in Section \ref{sec:problem-formulation}. By applying TabPFN to TCA-transformed features, we benefit from both domain invariance and strong generalization capabilities on small cohorts.

\subsection{Core Component II: Cross-Domain RFE with Cost-Effectiveness Index}
\label{subsec:sol-rfe}

Medical datasets often contain high-dimensional noise (e.g., irrelevant radiomics features) and "concept shift" (features whose predictive value changes across hospitals). To handle this, we employ a Cross-Domain Recursive Feature Elimination (RFE) mechanism.
The selection process is not merely about maximizing \auc; it is governed by a global \textbf{Cost-Effectiveness Index (CEI)} that balances predictive power against stability, clinical acquisition cost, and model complexity. We formulate the optimization problem for the optimal feature subset size $\rfecurrentdim^*$ as:
\begin{equation}
    \rfeselectedfeatures = \argmax_{\rfecurrentdim} \left( \rfecomponentweights_1 \rfeperformance(\rfecurrentdim) + \rfecomponentweights_2 \rfeefficiency(\rfecurrentdim) + \rfecomponentweights_3 \rfestability(\rfecurrentdim) + \rfecomponentweights_4 \rfesimplicity(\rfecurrentdim) \right)
\end{equation}
where the components are defined as:
\begin{itemize}
    \item \textbf{Performance ($\rfeperformance$):} A weighted sum of \auc and \fonescore on the source validation folds.
    \item \textbf{Efficiency ($\rfeefficiency$):} Measures the reduction in clinical burden, defined as $1 - \frac{\featsetcost}{\totalfeatcost}$, where Cost proxies for the difficulty of acquiring the feature (e.g., age is cheap, contrast CT is expensive).
    \item \textbf{Stability ($\rfestability$):} Quantifies the robustness of the feature subset across cross-validation folds, defined as $1 - \aucsd$.
    \item \textbf{Simplicity ($\rfesimplicity$):} A sparsity penalty $\exp(-\sparsityparam \rfecurrentdim)$ to prefer smaller, clinically manageable subsets (typically $\rfecurrentdim \in [5, 10]$).
\end{itemize}

Implementation-wise, we use `TabPFNClassifier` as the base estimator for RFE. In each iteration, we calculate the \textbf{Permutation Importance} $\rfeimportance(\featurevecj)$ of each feature $\featurevecj$. The feature with the lowest importance is pruned, and the model is re-evaluated. This recursive process ensures that the retained features $\rfeselectedfeatures$ are not only predictive but also stable against the noise inherent in small cohorts. In our experiments, this process consistently identifies a "Best-8" subset (Age, Spiculation, Lobulation, Diameter, etc.) that is robust across hospitals.

\subsection{Core Component III: Latent Space TCA Adaptation}
\label{subsec:sol-tca}

To explicitly align the source and target distributions, we apply Transfer Component Analysis (TCA) to the feature space reduced by RFE (Stage 2). Standard TCA is often applied to raw features, but this is suboptimal for complex medical data where the manifold structure might be noisy or high-dimensional. Instead, we construct the kernel matrix $\kernelmatrix$ using the RFE-selected features $\featurevec$. Specifically, we employ a \textbf{Linear Kernel} on these features:
\begin{equation}
    \kernelmatrix_{ij} = \langle \featurevec_i, \featurevec_j \rangle
\end{equation}
This projection, $\featurevec' = \tcaprojectionmatrix^\top \featurevec$, learns a domain-invariant feature representation that is then fed into the TabPFN Classifier (Stage 4).

\textbf{Justification for Linear Kernel on RFE-Selected Features:}
After Cross-Domain RFE, the selected features are assumed to reside in a more disentangled and clinically relevant space. In this context, a simple linear alignment via TCA is often sufficient to match the first-order moments of the distributions, thereby reducing covariate shift. This approach avoids the complexity and hyperparameter sensitivity (e.g., $\gammaK$ in RBF kernels) that often plagues non-linear kernel methods, especially on small datasets. The subsequent TabPFN Classifier is then capable of modeling complex non-linear relationships within this linearly adapted feature space.

The optimization objective minimizes the Maximum Mean Discrepancy (MMD) trace:
\begin{equation}
    \min_{\tcaprojectionmatrix} \quad \tr({\tcaprojectionmatrix}^\top \kernelmatrix \mmdmatrix \kernelmatrix \tcaprojectionmatrix) + \mu \tr({\tcaprojectionmatrix}^\top \tcaprojectionmatrix)
\end{equation}
where $\mmdmatrix$ is the MMD matrix defined block-wise as $\mmdmatrix_{ij} = 1/\sourcedatasize^2$ if $\featurevec_i, \featurevec_j \in \sourcedata$, $1/\targetdatasize^2$ if $\featurevec_i, \featurevec_j \in \targetdata$, and $-1/(\sourcedatasize \targetdatasize)$ otherwise. The projection $\tcaprojectionmatrix$ aligns the marginals $\marginalsourcedist(\mathbf{z})$ and $\marginaltargetdist(\mathbf{z})$, directly addressing the Covariate Shift challenge.
Figure~\ref{fig:tca_dimensionality} visualizes how the TCA projection compresses the RFE-reduced features into a compact latent subspace while shrinking the MMD divergence between the source and target domains, highlighting the balance controlled by $\mu$.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{img/cross_hospital/TCA_dimensionality_reduction.pdf}
    \caption{\textbf{TCA dimensionality reduction.} Left: projection of source/target points into the TCA subspace. Right: decreasing MMD divergence as more components are retained.}
    \label{fig:tca_dimensionality}
\end{figure}


\subsection{Core Component IV: Multi-View Ensemble and Calibration}
\label{subsec:sol-ensemble}

To mitigate the variance associated with small-sample learning and label shift, PANDA employs a \textbf{Strategic Multi-Branch Ensemble}. Instead of a single model, we construct an ensemble of $N=32$ members, derived from $\numpreprocessbranches=4$ distinct preprocessing strategies expanded by $\numrandomseeds=8$ random seeds (which control feature shuffling and rotational invariants).

The four strategic branches are designed to present different "views" of the data to the foundation model:
\begin{enumerate}
    \item \textbf{High-Complexity Ordinal (Raw+Quantile):} Features are mapped to a uniform distribution via `QuantileTransformer` (handling outliers) and concatenated with raw features. Categorical variables are Ordinal Encoded.
    \item \textbf{Low-Complexity Ordinal (Raw):} Raw features are used directly (relying on TabPFN's internal robustness). Categorical variables are Ordinal Encoded.
    \item \textbf{High-Complexity Numeric:} Similar to Branch 1, but categorical variables are treated as numeric (useful for ordered grades like "Spiculation 1-5").
    \item \textbf{Low-Complexity Numeric:} Raw features with numeric encoding for categoricals.
\end{enumerate}

\textbf{Rotational Invariance:}
For each branch, we train 8 versions. In each version, the feature columns are cyclically permuted (Rotated). This is critical because Transformers can exhibit positional bias; rotating the features ensures that the model's attention mechanism attends to all features equally, regardless of their column index.

\textbf{Temperature Scaling and Aggregation:}
The final probability is obtained via \textbf{Temperature Scaling} to calibrate the predictions from the ensemble of TabPFN Classifiers. This is crucial when the target domain prevalence differs from the source (Label Shift), as uncalibrated models often produce overconfident probabilities. The aggregated prediction is:
\begin{equation}
    \hat{\labelval}(\featurevec) = \frac{1}{\numpreprocessbranches \times \numrandomseeds} \sum_{i=1}^{\numpreprocessbranches \times \numrandomseeds} \activation\left(\frac{\logitoutput}{\temperature}\right)
\end{equation}
where $\logitoutput$ is the logit output of the $i$-th TabPFN Classifier member, and $\temperature=0.9$ is the temperature parameter empirically tuned to soften predictions.

\subsection{Theoretical Justification and Feasibility}
\label{subsec:sol-justification}

\textbf{Addressing Small Sample Size:}
The use of a frozen, pre-trained encoder $\featuremap_encoder$ avoids the need to train a deep network from scratch on $\sourcedatasize \approx 300$ samples. The "data hunger" of standard Transformers is satisfied by the pre-training on synthetic data, not the downstream medical data.

\textbf{Addressing Distribution Shift:}
The framework attacks shift at three levels:
1.  \textbf{Feature Level:} `QuantileTransformer` aligns the marginal distributions of individual features (Covariate Shift).
2.  \textbf{Latent Level:} TCA explicitly minimizes the divergence term $\domaindivergence$ in the generalization bound by aligning the joint embedding distributions.
3.  \textbf{Output Level:} Temperature scaling calibrates the posterior probabilities against Label Shift.

\textbf{Real-time Feasibility:}
Despite the ensemble complexity ($N=32$), the inference involves only forward passes of the Transformer (which are highly parallelizable) and linear projections. Empirical tests on a standard CPU (Intel i7) show an average inference latency of $< 200$ ms per patient. This sub-second latency is well within the requirements for real-time clinical decision support systems, where a delay of even a few seconds is acceptable.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{img/cross_hospital/Pre-trained Tabular Foundation Mode Pipeline_new.pdf}
    \caption{\textbf{The PANDA framework architecture.}
    (a) Compositional pipeline: from original tabular data through ensemble training, prediction aggregation, class imbalance adjustment, to final classification output.
    (b) Multi-branch ensemble with $\numpreprocessbranches=4$ preprocessing strategies,
    each generating $\numrandomseeds=8$ ensemble members via different random seeds.}
    \label{fig:model_details}
\end{figure}
\label{sec:sol-end}
