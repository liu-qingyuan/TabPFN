\section{Problem Formulation}
\label{sec:problem-formulation}
\label{sec:pf-start}

Cross-hospital diagnostic risk prediction can be viewed as a constrained machine learning problem at
the intersection of small-sample tabular learning and unsupervised domain adaptation (UDA). Rather
than treating pulmonary nodule malignancy prediction as a purely clinical task, we explicitly formalize
it as learning a robust classifier that transfers from a source hospital with limited labels to a target
hospital with unlabeled data, under heterogeneous feature schemas and distribution shift.

Concretely, PANDA instantiates an AI system composed of three interacting components: a pre-trained
tabular foundation model that supplies strong small-sample priors, a cross-domain feature selection
module that enforces schema overlap and discards unstable biomarkers, and a kernel-based alignment
module that reduces distributional discrepancy between hospitals. In this section, we formalize the
underlying learning problem and connect it to standard UDA theory, so that the architectural choices
of PANDA can be interpreted as explicit responses to the terms in a domain adaptation bound.

\subsection{Task definition and notation}
\label{subsec:notation}

We consider a binary classification problem with a $d$-dimensional tabular input space
$\mathcal{X} \subseteq \mathbb{R}^d$ comprising mixed numerical and categorical variables
(e.g., age, nodule diameter, smoking status), and a label space
$\mathcal{Y} = \{0,1\}$ indicating benign or malignant nodules. A \emph{domain} is defined
as a joint distribution $P(X,Y)$ over $\mathcal{X} \times \mathcal{Y}$.

From an AI perspective, cross-hospital deployment naturally induces two domains:

\begin{itemize}
    \item \textbf{Source domain (Hospital A).} We observe a labeled dataset
    $\sourcedata = \{(x_i^s, y_i^s)\}_{i=1}^{\sourcedatasize}$ drawn i.i.d.\ from a source distribution
    $\sourcedomaindist$ over $(X,Y)$. This is the only domain in which labels are available.
    \item \textbf{Target domain (Hospital B).} We observe an unlabeled dataset
    $\targetdata = \{x_j^t\}_{j=1}^{\targetdatasize}$ drawn i.i.d.\ from the marginal $P_T(X)$ of a target
    distribution $\targetdomaindist$. Labels $Y_T$ are not available during training, reflecting
    realistic privacy and annotation constraints in new hospitals.  
\end{itemize}

Feature heterogeneity is modeled explicitly. Let $F_S$ and $F_T$ denote the feature-index sets
available in the source and target hospitals, respectively, and define the shared schema
$F_\cap = F_S \cap F_T$ with effective dimensionality $d_\cap = |F_\cap|$.
Site-specific variables in $F_\setminus = (F_S \cup F_T) \setminus F_\cap$ are
treated as non-transferable and are removed or marginalized before alignment.

We write $\mathcal{H}$ for a hypothesis class of classifiers $h : \mathcal{X} \rightarrow \{0,1\}$
(or $h : \mathcal{X} \rightarrow [0,1]$ for probabilistic outputs). For any $h \in \mathcal{H}$,
the expected risk on source and target domains under a loss function $L$ is
\[
  \sourceerror(\hypothesis) = \mathbb{E}_{(X,Y) \sim \sourcedomaindist}[\loss(\hypothesis(X),Y)], \quad
  \targeterror(\hypothesis) = \mathbb{E}_{(X,Y) \sim \targetdomaindist}[\loss(\hypothesis(X),Y)].
\]
The empirical risks $\sourcesampleerror(\hypothesis)$ and $\targetsampleerror(\hypothesis)$ are defined
analogously over $\sourcedata$ and $\targetdata$.

\textbf{Objective.} The AI task considered in this thesis is to learn a predictor
$h^\star \in \mathcal{H}$ using labeled source data $S$ and unlabeled target data $T$ such that:

\begin{enumerate}
  \item $\epsilon_T(h^\star)$ is minimized (high AUC and clinically acceptable sensitivity on
  the target hospital),
  \item calibration on the target domain is preserved under prevalence and covariate shift, and
  \item the solution respects privacy and data-governance constraints (no sharing of raw target labels
  and only lightweight alignment on shared features).
\end{enumerate}

Table~\ref{tab:notation} summarizes the key mathematical symbols used throughout the thesis.

\renewcommand{\arraystretch}{1.3}
\begin{longtable}{p{0.25\textwidth} p{0.50\textwidth} p{0.15\textwidth}}
  \caption{Unified Mathematical Notation System}
  \label{tab:notation} \\
  \toprule
  \textbf{Symbol} & \textbf{Definition} & \textbf{Dimensions} \\
  \midrule
  \endhead

  \bottomrule
  \endfoot
    \multicolumn{3}{l}{\textit{Domains and Data}} \\
    $\inputspace$ & Input feature space (mixed numerical/categorical) & $\subseteq \mathbb{R}^d$ \\
    $\labelspace$ & Label space (0: Benign, 1: Malignant) & $\{0, 1\}$ \\
    $\sourcedomaindist, \targetdomaindist$ & Source and Target domain distributions & over $\inputspace \times \labelspace$ \\
    $\sourcedata, \targetdata$ & Empirical datasets drawn from $\sourcedomaindist, \targetdomaindist$ & Sets of size $\sourcedatasize, \targetdatasize$ \\
    $\featurevec, \labelval$ & Feature vector and corresponding label & $\featurevec \in \mathbb{R}^d, \labelval \in \labelspace$ \\
    $\dimnum, \dimcat$ & Dimensionality of numerical and categorical features & Scalar (integers) \\
    $\prevalence$ & Prevalence of positive class $P(Y=1)$ & Probability $\in [0,1]$ \\
    $\sharedfeatures, \rfeselectedfeatures$ & Shared feature schema and RFE-selected subset & Subsets of indices \\
    
    \multicolumn{3}{l}{\textit{Learning Theory}} \\
    $\hypothesis, \hypothesisclass$ & Hypothesis function (classifier) and hypothesis class & $\hypothesis: \inputspace \to \labelspace$ \\
    $\sourceerror, \targeterror$ & Expected risk (error) on Source and Target distributions & Scalar $\in [0,1]$ \\
    $\sourcesampleerror, \targetsampleerror$ & Empirical risk on datasets $\sourcedata$ and $\targetdata$ & Scalar $\in [0,1]$ \\
    $\domaindivergence$ & $\mathcal{H}\Delta\mathcal{H}$-Divergence (Domain Discrepancy) & Scalar $\ge 0$ \\
    $\adaptabilityterm$ & Ideal joint hypothesis error (Adaptability term) & Scalar $\ge 0$ \\
    $\vcdim$ & VC Dimension of hypothesis class $\hypothesisclass$ & Scalar $\ge 1$ \\
    $\confidence$ & Confidence parameter for generalization bound & Scalar $\in (0,1)$ \\
    $\loss$ & Loss function & $\labelspace \times \labelspace \to \mathbb{R}_{\ge 0}$ \\
    $\modelparams$ & Model parameters (weights) & Vector $\in \mathbb{R}^k$ \\
    $\Hdeep$ & Deep Neural Network hypothesis class & $\Hdeep \subseteq \hypothesisclass$ \\
    $\Sperf$ & Performance-optimal hypothesis class & Subset of $\hypothesisclass$ maximizing accuracy \\
    $\Seff$ & Efficiency-optimal hypothesis class & Subset of $\hypothesisclass$ minimizing computational cost \\
    $\Sstab$ & Stability-optimal hypothesis class & Subset of $\hypothesisclass$ minimizing variance \\
    $\Ssimp$ & Simplicity-optimal hypothesis class & Subset of $\hypothesisclass$ minimizing complexity \\
    $\ns, \nt$ & Alternative notation for source/target domain sizes & $\ns = \sourcedatasize$, $\nt = \targetdatasize$ \\
    
    \multicolumn{3}{l}{\textit{PANDA Architecture}} \\
    $\rfeimportance$ & Permutation Importance (Feature Saliency) & Scalar $\in \mathbb{R}$ \\
    $\rfeop(\cdot)$ & RFE operation that restricts features to $\rfeselectedfeatures$ & $\mathbb{R}^{d} \to \mathbb{R}^{\rfecurrentdim}$ \\
    $\adaptmap(\cdot)$ & Domain adaptation mapping (TCA projection) & $\mathbb{R}^{\rfecurrentdim} \to \mathbb{R}^{\tcaprojectdim}$ \\
    $\tcaprojectionmatrix$ & TCA Projection Matrix & $\mathbb{R}^{\rfecurrentdim \times \tcaprojectdim}$ \\
    $\kernelmatrix$ & Kernel Matrix (Linear kernel on features) & $\mathbb{R}^{(\sourcedatasize+\targetdatasize)^2}$ \\
    $\mmdmatrix$ & MMD Indicator Matrix & $\mathbb{R}^{(\sourcedatasize+\targetdatasize)^2}$ \\
    $\centeringmatrix$ & Centering Matrix & $\mathbb{R}^{(\sourcedatasize+\targetdatasize)^2}$ \\
    $\regularizationparam$ & TCA Regularization Parameter & Scalar $> 0$ \\
    $\gammaK$ & RBF Kernel Bandwidth Parameter & Scalar $> 0$ \\
    $\featuremap(\cdot)$ & Implicit Feature Map to RKHS & $\inputspace \to \rkhs$ \\
    $\ppd$ & Posterior Predictive Distribution $P(\labelval|\featurevec, \sourcedata)$ & Probability \\
    $\tabpfnfunci{i}(\cdot)$ & i-th TabPFN classifier function (ensemble member) & $\mathbb{R}^{\tcaprojectdim} \to [0,1]$ \\
$\tabpfnfunc(\cdot)$ & TabPFN classifier function (general) & $\mathbb{R}^{\tcaprojectdim} \to [0,1]$ \\
    $\tabpfnencoder$ & TabPFN Transformer Encoder & $\mathbb{R}^{\tcaprojectdim} \to \mathbb{R}^{\text{hidden}}$ \\
    $\featureembed$ & General feature embedding vector & $\mathbb{R}^{\text{embed\_dim}}$ \\
    $\samplesembed$ & Full sample embedding (concatenated feature embeddings) & $\mathbb{R}^{\text{hidden}}$ \\
    $\posencoding$ & Positional encoding for feature sequence & $\mathbb{R}^{\text{hidden}}$ \\
    $\numpreprocessbranches$ & Number of preprocessing branches & Scalar (integer) \\
    $\numrandomseeds$ & Number of random seeds (ensemble size per branch) & Scalar (integer) \\
    $\pandafunc(\cdot)$ & PANDA composite function & $\inputspace \to [0,1]$ \\
    $\temperature$ & Temperature scaling parameter for calibration & Scalar $> 0$ \\
    $\activation(\cdot)$ & Activation function (e.g., sigmoid) & $\mathbb{R} \to [0,1]$ \\
\end{longtable}

\subsection{Prior-data fitted networks as tabular foundation models}
\label{subsec:pfn-generation}

Traditional machine learning assumes a fixed parametric form for the data generation process (e.g., a separating hyperplane for SVMs). In contrast, the TabPFN framework posits that the dataset $\mathcal{D} = \{(\featurevec_i, \labelval_i)\}_{i=1}^N$ is generated from a \textbf{prior distribution over functions}, denoted as $\priorfunc$.

Formally, a dataset $\mathcal{D}$ is sampled in two steps:
\begin{enumerate}
    \item A structural equation model (SEM) or a data-generating function $\func$ is sampled from the prior: $\func \sim \priorfunc(\cdot)$. In TabPFN, this prior is constructed explicitly using a large mixture of synthetic structural causal models (SCMs), including Bayesian Neural Networks and causal graphs with varying sparsity and non-linearity.
    \item Data points are sampled conditioned on this function: $\labelval_i = \func(\featurevec_i) + \noise$, or $\labelval_i \sim P(\labelspace|\func(\featurevec_i))$.
\end{enumerate}

The learning objective of a Prior-Data Fitted Network (PFN) is to approximate the \textbf{posterior predictive distribution} (\ppd) for a query sample $\queryvec$ given the context dataset $\sourcedata$:
\begin{equation}
    \ppd(\labelvalq \mid \queryvec, \sourcedata) = \int P(\labelvalq \mid \queryvec, \func) \, P(\func \mid \sourcedata) \, d\func
\end{equation}
TabPFN approximates this integral using a Transformer-based architecture that attends to the entire context $\sourcedata$ (In-Context Learning). This perspective is particularly advantageous for the medical small-sample setting ($\sourcedatasize < 500$) because:
\begin{itemize}
    \item It avoids iterative gradient descent on the small dataset, mitigating the risk of overfitting to noise.
    \item It leverages the "knowledge" encoded in the prior $\priorfunc$, effectively transferring inductive biases about tabular structures (e.g., decision boundaries are often aligned with axes, sparsity is common) to the medical task.
\end{itemize}

However, the standard PFN assumes that the query sample $\queryvec$ comes from the same distribution as $\sourcedata$ (i.e., same $\func$). In our cross-hospital setting, the target query $\featurevect$ comes from a shifted distribution $\targetdomaindist$, violating the exchangeability assumption of the posterior approximation.

\subsection{Formalizing Domain Shift}
\label{subsec:domain-shift}

The core challenge in our research is that $\sourcedomaindist \neq \targetdomaindist$. This joint distribution shift can be decomposed into three primary components relevant to medical AI:

\subsubsection{Covariate Shift: The Acquisition Gap}
Covariate shift occurs when the marginal feature distributions differ, $\marginalsourcedist \neq \marginaltargetdist$, while the conditional probability of the label remains constant, $\conditionalps = \conditionalpt$.
\begin{equation}
    \marginalsourcedist \neq \marginaltargetdist \quad \text{and} \quad \conditionalps = \conditionalpt
\end{equation}
In pulmonary nodule diagnosis, this is often driven by technological heterogeneity. For instance, CT scanners use different reconstruction kernels (e.g., "Sharp" vs. "Smooth"). A nodule scanned with a sharp kernel will systematically exhibit higher values for texture features like "entropy" or "spiculation" compared to the same nodule scanned with a smooth kernel, shifting the probability density function $P(\featurevectexture)$ without changing the underlying malignancy risk. TabPFN is particularly sensitive to this because its attention mechanism relies on finding similar examples in the support set; if the target $\featurevect$ lies in a region unsupported by $\marginalsourcedist$, the attention weights become diffuse.

\subsubsection{Label Shift: The Prevalence Gap}
Label shift, or prior probability shift, is defined by a change in the marginal label distribution:
\begin{equation}
    \marginalsourcelabel \neq \marginaltargetlabel
\end{equation}
This is endemic to healthcare referrals. A tertiary cancer center (Source) typically receives high-risk referrals with a malignancy prevalence of $P(\labelspace=1) \approx 60\%$. In contrast, a community screening program (Target) encounters a broader population with many benign incidental findings, where $P(\labelspace=1) \approx 5\%-20\%$. A model trained on the balanced source will learn a prior $\prevalence_S$ and systematically overestimate risk on the target, leading to poor calibration and excessive false positives.

\subsubsection{Concept Shift: The Definition Gap}
Concept shift implies a fundamental change in the relationship between features and labels:
\begin{equation}
    \conditionalps \neq \conditionalpt
\end{equation}

In pulmonary medicine, apparent model failures often reflect latent confounders
arising from regional disease ecology. In regions such as the Mississippi and
Ohio River Valleys in North America or parts of East Asia, granulomatous
infections (e.g., histoplasmosis, tuberculosis) are endemic and generate a high
burden of benign pulmonary nodules and granulomas%
~\cite{barros_pulmonary_2023,yang_comparison_2018}. 
These benign lesions frequently mimic malignant nodules on imaging, including
solid or spiculated nodules with appreciable metabolic uptake on PET--CT, and
therefore constitute an important source of diagnostic ambiguity in lung cancer
screening and workup~\cite{shipe_validation_2021,lang_asymptomatic_2017}. 
Consequently, radiographic patterns that are highly predictive of malignancy in
low-endemic Western cohorts may correspond to a substantially lower cancer
probability in these settings, representing a form of concept shift.

Formally, this means that the same feature vector $\featurevec$ can be
associated with markedly different posterior malignancy probabilities across
hospitals: a model calibrated in a non-endemic source hospital might assign
$P_S(Y=1 \mid \featurevec) \approx 0.9$, whereas the true probability in a
TB-endemic target hospital could be closer to
$P_T(Y=1 \mid \featurevec) \approx 0.4$~\cite{ben2010theory}.

\subsubsection{Theoretical Bound on Generalization Error}
Following the seminal theory by Ben-David et al. \cite{ben2010theory}, the expected error of a hypothesis $\hypothesis$ on the target domain, $\targeterror$, is bounded by:
\begin{equation}
    \label{eq:ben-david}
    \targeterror \leq \sourceerror + \frac{1}{2} \domaindivergence(\sourcedomaindist, \targetdomaindist) + \adaptabilityterm
\end{equation}
where:
\begin{itemize}
    \item $\sourceerror$ is the source domain error, minimized via supervised training.
    \item $\domaindivergence(\sourcedomaindist, \targetdomaindist)$ is the $\mathcal{H}\Delta\mathcal{H}$-divergence between the two domains.
    \item $\adaptabilityterm = \min_{\hypothesis \in \hypothesisclass} [\sourceerror + \targeterror]$ is the error of the ideal joint hypothesis.
\end{itemize}
This bound highlights that minimizing source error alone is insufficient; an effective cross-hospital
system must simultaneously (i) keep $\sourceerror$ small, (ii) reduce the divergence term
$\domaindivergence(\sourcedomaindist, \targetdomaindist)$, and (iii) control the joint error
$\adaptabilityterm$ by avoiding features that admit no low-error classifier across both domains.

From this perspective, the three components of PANDA align directly with the three terms in
Eq.~\eqref{eq:ben-david}:

\begin{itemize}
  \item \textbf{Tabular foundation model (TabPFN) $\rightarrow \sourceerror$.}
  By treating the source cohort as a context set for a pre-trained PFN, PANDA reduces
  $\sourceerror(\hypothesis)$ under small-sample, imbalanced conditions without extensive
  hyperparameter tuning.
  \item \textbf{Transfer Component Analysis (TCA) $\rightarrow \domaindivergence$.}
  The TCA module learns a latent space in which the empirical Maximum Mean Discrepancy between
  source and target representations is minimized, directly targeting the divergence term
  $\domaindivergence(\sourcedomaindist, \targetdomaindist)$.
  \item \textbf{Cross-domain RFE $\rightarrow \adaptabilityterm$.}
  Recursive feature elimination explicitly searches for a subset of shared features
  $\rfeselectedfeatures \subseteq \sharedfeatures$ that support a low-error joint classifier
  across hospitals, thereby tightening the adaptability term $\adaptabilityterm$ by removing
  unstable or hospital-specific biomarkers.
\end{itemize}

Thus, PANDA can be interpreted as an instantiation of the domain adaptation bound rather than
an ad hoc composition of modules: each architectural choice is motivated by a specific term in
Eq.~\eqref{eq:ben-david}.

\subsection{Theoretical Constraints of Existing Models}
\label{subsec:model-constraints}

To justify the architecture of PANDA, we formally analyze why existing state-of-the-art models fail in this specific regime ($N \approx 300$, Unlabeled Target, Tabular Data).

\subsubsection{Gradient Boosted Decision Trees (GBDT)}
GBDTs (e.g., XGBoost, LightGBM) partition the feature space using hard, axis-aligned splits ($\mathbb{I}(\featurevecj < \threshold)$). They suffer from two critical limitations in UDA:
\begin{enumerate}
    \item \textbf{Non-Differentiability:} The piecewise constant decision boundary is non-differentiable with respect to input features. This precludes the use of gradient-based domain alignment techniques (like Adversarial Training or Gradient Reversal Layers) which require backpropagating a domain loss into the feature encoder.
    \item \textbf{Inability to Extrapolate:} Tree models cannot extrapolate beyond the range of the training data. If covariate shift pushes the target distribution $\marginaltargetdist$ outside the support of $\marginalsourcedist$, the tree maps all such points to the value of the nearest leaf node, often resulting in statistically invalid predictions.
\end{enumerate}

\subsubsection{Deep Tabular Models}
Deep learning models (e.g., TabNet, FT-Transformer) offer differentiability but lack the appropriate inductive bias for small tabular datasets:
\begin{enumerate}
    \item \textbf{Data Hunger:} Neural networks typically require large datasets ($N > 10^4$) to converge to a generalizable solution. With $\sourcedatasize \approx 300$, deep models are prone to severe overfitting or convergence to local minima.
    \item \textbf{Rotational Invariance:} Standard MLPs are rotationally invariant, but tabular features are not rotationally interchangeable (e.g., rotating "Age" and "Creatinine" axes creates a nonsensical feature space). This mismatch in inductive bias makes them less sample-efficient than tree-based or prior-fitted methods.
\end{enumerate}

\subsection{Transfer Component Analysis (TCA) Optimization Objective}
\label{subsec:tca-optimization}

To minimize the divergence term in Eq. \eqref{eq:ben-david}, we employ Transfer Component Analysis (TCA). TCA seeks a feature map $\featuremap: \inputspace \rightarrow \rkhs$ such that the Maximum Mean Discrepancy (MMD) between source and target distributions in the Reproducing Kernel Hilbert Space (RKHS) is minimized.

The empirical MMD distance is defined as:
\begin{equation}
    \text{MMD}(\sourcedomaindist, \targetdomaindist) = \left\| \frac{1}{\sourcedatasize} \sum_{i=1}^{\sourcedatasize} \featuremap(\featurevec_i^s) - \frac{1}{\targetdatasize} \sum_{j=1}^{\targetdatasize} \featuremap(\featurevec_j^t) \right\|_{\hypothesisclass}^2
\end{equation}

TCA aims to learn a transformation matrix $\tcaprojectionmatrix \in \mathbb{R}^{(\sourcedatasize+\targetdatasize) \times \tcaprojectdim}$ that reduces the data dimensionality to $\tcaprojectdim \ll \featuredim$ while minimizing MMD. The optimization problem is formally:

\begin{equation}
    \min_{\tcaprojectionmatrix} \tr({\tcaprojectionmatrix}^\top \kernelmatrix \mmdmatrix \kernelmatrix \tcaprojectionmatrix) + \mu \tr({\tcaprojectionmatrix}^\top \tcaprojectionmatrix) \,\text{s.t.}\, {\tcaprojectionmatrix}^\top \kernelmatrix \centeringmatrix \kernelmatrix \tcaprojectionmatrix = \identitymatrix
\end{equation}

where:
\begin{itemize}
    \item $\kernelmatrix \in \mathbb{R}^{(\sourcedatasize+\targetdatasize) \times (\sourcedatasize+\targetdatasize)}$ is the kernel matrix computed on the union of source and target data (specifically, the RFE-selected features). We specifically employ a \textbf{Linear Kernel} on these features: $\kernelmatrix_{ij} = \langle \featurevec_i, \featurevec_j \rangle$. This is justified because the RFE process aims to select a set of features that are already more linearly separable or amenable to linear transformation. Using a linear kernel provides a robust and computationally efficient alignment without introducing complex hyperparameter tuning for RBF bandwidths.

    \item $\mmdmatrix$ is the MMD coefficient matrix, with elements $\mmdmatrix_{ij} = 1/\sourcedatasize^2$ if $\featurevec_i, \featurevec_j \in \sourcedata$, $1/\targetdatasize^2$ if $\featurevec_i, \featurevec_j \in \targetdata$, and $-1/(\sourcedatasize \targetdatasize)$ otherwise.
    \item $\centeringmatrix = \identitymatrix - \frac{1}{\sourcedatasize+\targetdatasize}\onesvec\onesvec^\top$ is the centering matrix.
    \item The constraint ${\tcaprojectionmatrix}^\top \kernelmatrix \centeringmatrix \kernelmatrix \tcaprojectionmatrix = \identitymatrix$ ensures the variance of the projected data is preserved (maximizing information).
\end{itemize}

\subsection{The PANDA Framework: A Unified Formalization}
\label{subsec:panda-formalization}

We formalize our proposed \textbf{PANDA} (Pretrained Adaptation Network with Domain Alignment) framework as a composite function $\pandafunc: \inputspace \rightarrow \labelspace$. The inference process for a target sample $\featurevec$ is defined as:

\begin{equation}
    \label{eq:panda-formalization}
    f_{\text{PANDA}}(\featurevec) = \hypothesis \circ \adaptmap \circ \pi_{\cap} \circ \rfeop (\featurevec),
\end{equation}
where $\rfeop$ selects the discriminative feature subset $\rfeselectedfeatures$, and $\pi_{\cap}$ enforces the schema intersection with the target domain (Feature Alignment).

This composition involves distinct stages, grounded in the optimization of the feature subspace prior to alignment:

\subsubsection{Stage 1: Recursive Feature Elimination (RFE) with TabPFN}
\label{subsubsec:rfe-formalization}

Directly applying domain adaptation on high-dimensional, noisy feature spaces often leads to negative transfer. We employ Recursive Feature Elimination (RFE) to determine the optimal subspace $\rfeselectedfeatures \subseteq \sharedfeatures$.

Let $\rfeset^{(0)} = \sharedfeatures$ be the initial set of shared features. The RFE process generates a sequence of feature subsets $\rfeset^{(0)} \supset \rfeset^{(1)} \supset \dots \supset \rfeset^{(\featuredim-\rfecurrentdim)}$, where $\rfecurrentdim$ is the target dimensionality. At each iteration $t$:

\begin{enumerate}
    \item \textbf{Model Fitting:} We define the TabPFN posterior predictive distribution conditioned on the current subset $\rfeiterateset$ using the source data $\sourcedata$.
    \item \textbf{Importance Estimation ($\rfeimportance$):} Unlike linear models, TabPFN is a non-parametric meta-learned model. We approximate feature importance using \textbf{Permutation Importance}. For each feature $\featurevecj \in \rfeiterateset$, we compute the degradation in the \auc metric when feature $\featurevecj$ is randomly permuted in the validation set:
    \begin{equation}
        \rfeimportance(\featurevecj; \rfeiterateset) = \lossauc(\hypothesissubset, \sourcedata) - \lossauc(\hypothesissubset, \sourcedata^{\text{perm}(j)})
    \end{equation}
    
    \item \textbf{Elimination:} We identify and remove the feature with the minimal contribution:
    \begin{equation}
        \minfeature = \argmin_{\featurevecj \in \rfeiterateset} \rfeimportance(\featurevecj; \rfeiterateset)
        \quad \longrightarrow \quad
        \rfeset^{(t+1)} = \rfeiterateset \setminus \{\minfeature\}
    \end{equation}
\end{enumerate}

The final subset $\rfeselectedfeatures$ is selected to maximize stability and discriminative power, effectively reducing the $\adaptabilityterm$ term (joint error) in the Ben-David bound by removing concept-shifted features.

\subsubsection{Stage 2: Feature Alignment ($\pi_{\cap}$)}

Given the selected feature subset $\rfeselectedfeatures$, this stage enforces schema consistency between hospitals. In our specific experimental setup, RFE identifies an optimal subset of $|\rfeselectedfeatures| = 9$ features. However, due to missingness shift in the target domain, the alignment operator $\pi_{\cap}$ restricts this to the intersection of available schemas:
\begin{equation}
    \sharedfeatures = \rfeselectedfeatures \cap \targetfeatureschema
\end{equation}
This dimensionality reduction (from 9 to 8) explicitly handles the constraint where specific biomarkers are unrecorded in the target hospital, ensuring that subsequent stages operate only on the biologically common subspace.

\subsubsection{Stage 3: Domain Adaptation Mapping ($\adaptmap$)}

The aligned features $\sharedfeatures$ are then adapted using Transfer Component Analysis (TCA). The domain adaptation mapping $\adaptmap$ projects these features into a Reproducing Kernel Hilbert Space (RKHS) where the Maximum Mean Discrepancy (MMD) between source and target distributions is minimized.
\begin{equation}
    \featurevec' = \adaptmap(\featurevec; \sharedfeatures) = \tcaprojectionmatrix^\top \featurevec
\end{equation}
This linear projection $\tcaprojectionmatrix$ is learned to align the marginal distributions $P_S(\sharedfeatures)$ and $P_T(\sharedfeatures)$, thereby reducing the domain divergence term $\domaindivergence$ in the generalization bound.

\subsubsection{Stage 4: Classification and Ensemble ($\hypothesis$)}

Finally, the classification function $\hypothesis$ processes the adapted features $\featurevec'$. This stage leverages the pre-trained TabPFN as a robust classifier, augmented by an ensemble mechanism to handle predictive uncertainty and label shift.
\begin{equation}
    \hypothesis(\featurevec') = \frac{1}{N} \sum_{i=1}^{N} \activation\left(\frac{\tabpfnfunci{i}(\featurevec')}{\temperature}\right)
\end{equation}
where $\temperature$ is a temperature scaling parameter optimized to calibrate the output probabilities against the prevalence differences between hospitals. The result is a calibrated malignancy probability that is robust to both covariate and label shifts.

\subsection{Clinical-Statistical Mapping}
\label{subsec:clinical-mapping}

Table~\ref{tab:clinical-mapping} summarizes the correspondence between the clinical challenges observed in pulmonary nodule diagnosis, their statistical manifestations, and the corresponding PANDA solution components derived from our theoretical framework.

\begin{table}[htbp]
  \centering
  \caption{Mathematical Mapping of Clinical Problems to PANDA Components}
  \label{tab:clinical-mapping}
  \begin{tabular}{p{0.25\textwidth}p{0.25\textwidth}p{0.2\textwidth}p{0.2\textwidth}}
    \toprule
    \textbf{Clinical Challenge} & \textbf{Statistical Mechanism} & \textbf{PANDA Component} & \textbf{Theoretical Justification} \\
    \midrule
    Scanner Variance (Sharp vs. Smooth Kernels) & Covariate Shift: $\marginalsourcedist \neq \marginaltargetdist$ & Latent TCA on RFE-selected features & Minimizes MMD divergence $\domaindivergence$ in RKHS. \\
    \midrule
    Referral Patterns (Cancer Center vs. Screening) & Label Shift: $\marginalsourcelabel \neq \marginaltargetlabel$ & Ensemble Aggregation \& Temperature Scaling & Calibrates posteriors; smooths overconfidence from prior mismatch. \\
    \midrule
    Biological Confounders (TB vs. Cancer) & Concept Shift: $\conditionalps \neq \conditionalpt$ & Cross-Domain RFE & Minimizes joint error $\adaptabilityterm$ by removing unstable features. \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Problem Constraints and Research Scope}
\label{sec:problem-constraints}
\label{subsec:constraints}

Our formulation is bound by specific constraints inherent to the medical domain:
\begin{itemize}
    \item \textbf{Small Sample Size Constraint:} The sample sizes $\sourcedatasize, \targetdatasize$ are typically in the range of 100 to 1000, which is insufficient for training over-parameterized deep networks from scratch:
    \begin{equation}
        \label{eq:small-sample-constraint}
        \sourcedatasize \ll \vcdim(\deepmodelclass)
    \end{equation}
    where $\vcdim(\deepmodelclass)$ represents the VC dimension required for standard deep domain adaptation networks to generalize.
    \item \textbf{Privacy and Data Silos:} We assume source data $\sourcedomaindist$ and target data $\targetdomaindist$ cannot be physically merged.
    \item \textbf{Class Imbalance:} The prevalence of the positive class is often low ($\prevalence < 0.3$), requiring \auc-centric optimization.
\end{itemize}

This formalization sets the stage for the specific methodological implementations detailed in Chapter 4.
\label{sec:pf-end}
