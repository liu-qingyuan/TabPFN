\section{Related Work}
\label{sec:rw-start}
\label{sec:related-work}

In this section, we review prior work from an AI perspective on cross-hospital diagnostic risk prediction using structured medical data. We organize the literature along five dimensions. First, we summarize model families for medical tabular data, including tree ensembles, deep tabular architectures, and tabular foundation models. Second, we discuss domain shift and domain adaptation methods in medical AI. Third, we review feature selection techniques for small, imbalanced, and heterogeneous cohorts. Fourth, we situate AI-based approaches for pulmonary nodule malignancy prediction within this broader landscape. Finally, we examine public benchmarks that expose cross-domain and temporal shifts in tabular data. This structure parallels the design of our proposed framework, PANDA, which integrates tabular foundation models, domain-aware feature selection, and kernel-based alignment to address the limitations identified in prior work.

\subsection{Tabular learning for medical data: tree ensembles, deep tabular networks, and tabular foundation models}

The literature on structured-data learning has progressed from classical ensembles
to deep tabular networks and, most recently, to tabular foundation models that
mirror the trends in NLP and computer vision~\cite{bommasani2022opportunities,
schneider2024foundation}. We separate the discussion into tree ensembles, deep
tabular architectures, and tabular foundation models to highlight where each excels
and why none alone solves cross-hospital robustness. In medical settings, the same
patient cohort may be modeled by tree ensembles, deep tabular networks, or
foundation models depending on sample size and operational constraints; understanding
their respective failure modes under domain shift is crucial for positioning PANDA.

\subsubsection{Tree ensembles for clinical tabular data}

Gradient-boosted decision trees (GBDTs) such as XGBoost, LightGBM, and CatBoost
remain the workhorses for EHR-style tables because they tolerate heterogeneous
scales, missing values, and noisy categorical codes while supporting monotone
constraints and other clinical priors~\cite{chen2016xgboost,grinsztajn_why_2022,
borisov2022deep}. Benchmarking studies covering hundreds of OpenML tasks show that
GBDTs still beat most neural baselines whenever training samples exceed a few
thousand, yet they overfit rapidly when $N<1{,}000$, cannot be fine-tuned
incrementally, and require full retraining when hospitals change their feature
schemas~\cite{gorishniy2021revisiting,shmuel_comprehensive_2024,fan_tabular_2024}.
Case reports on cross-institutional readmission and mortality prediction show that
tree models memorize acquisition artifacts (assay vendors, coding practices) and
lose 10--20 AUC points when transferred without recalibration, illustrating their
non-differentiable structure blocks end-to-end multimodal training and plug-and-play
domain adaptation~\cite{borisov_deep_2024,liu_tabpfn_2025}. This rigidity motivates
attempts to distill tree priors into differentiable encoders so that adaptation can
occur without rebuilding the model for each site.
These same inductive biases explain why trees dominate mid-scale public benchmarks
yet struggle in small, imbalanced medical cohorts: sparsity-aware splits handle
missing labs gracefully, but boosting magnifies noise when positive classes are rare
and hospital-specific priors leak into leaf structure. Because gradients stop at
each split, trees cannot share representations with image encoders or participate
in gradient-based domain adaptation, forcing manual feature harmonization whenever
schemas or prevalence shift.
In practice, this means that widely used implementations such as XGBoost and
LightGBM shine on medium-to-large EHR cohorts with thousands of patients and
hundreds of features, where sparse histogram-based splits and built-in handling of
missing indicators yield strong baselines with modest tuning. Studies across
OpenML, MIMIC-style EHR benchmarks, and TableShift-like suites repeatedly show
that properly regularized XGBoost variants achieve AUROCs in the high 0.70s to
low 0.80s for mortality, readmission, and sepsis detection, and that they retain
their ranking power even when categorical encodings or measurement scales differ
across hospitals~\cite{gorishniy2021revisiting,fan_tabular_2024,gardner_benchmarking_2024}.
These observations explain why tree ensembles remain the default choice for
operational clinical decision support systems.

On the small, heavily imbalanced cohorts typical of lung-screening registries
($N\approx 300$), the same capacity becomes a liability. Empirical analyses of
GBDT behavior on few-shot medical datasets reveal several critical failure modes:
first, deep trees can memorize the few malignant cases (often $<$50 positives),
leading to apparent training accuracies $>$95\% but test AUROCs collapsing once
covariates shift~\cite{fan_tabular_2024,gardner_benchmarking_2024}. Second,
calibration deteriorates dramatically in low-prevalence subgroups, with predicted
probabilities systematically overestimating malignancy in young non-smokers while
underestimating risk in elderly or high-burden cohorts~\cite{guo_evaluation_2022,koch2024distribution}.
Third, when new hospitals add or remove variables (e.g., different CT protocol
parameters or biomarker panels), there is no principled way to ``warm start'' or
incrementally fine-tune existing tree models without complete retraining from
scratch, making long-term maintenance expensive.

Because tree ensembles are non-differentiable and lack explicit latent
representations, they are also difficult to integrate into end-to-end multimodal
models or to pair with standard domain adaptation objectives. This mathematical
constraint has practical consequences: researchers attempting to combine XGBoost
with imaging features must resort to late fusion (averaging predictions) or
feature concatenation followed by retraining, both of which preserve the
non-differentiable barrier. Gradient-based domain adaptation methods such as
Domain Adversarial Neural Networks (DANN) or Maximum Mean Discrepancy (MMD)
regularization cannot be applied directly to tree models, requiring workarounds
that approximate tree decision surfaces with differentiable surrogates or hybrid
architectures that mix neural embeddings with gradient-boosted leaves.
This limitation motivates methods that transfer tree-like priors into differentiable
architectures or that use tree models as feature extractors rather than end-to-end
learners.

\subsubsection{Deep tabular networks}

Deep tabular architectures import attention and representation learning from
sequence models to overcome the adaptation gap. TabNet uses sequential feature masks
to mimic decision paths, TabTransformer contextualizes categorical embeddings,
FT-Transformer tokenizes all features, and SAINT introduces intersample attention
plus contrastive pre-training to borrow signal across patients~\cite{arik2021tabnet,
huang_tabtransformer_2020,gorishniy_revisiting_2021,somepalli2021saint}. Basis
Transformers, NODE variants, TabICL prompt-serialization, weight-prediction, and
regularization schemes further explore the space between neural and symbolic
models~\cite{margeloiu2023weight,loh_basis_2025,khoeini_fttransformer_2024,
bytezcom_tabicl_2025,somvanshi2024survey}. However, comprehensive surveys and
multiple leaderboard studies report that these models remain data-hungry,
sensitive to hyperparameters, and often trail tuned tree ensembles on small,
heterogeneous cohorts typical of tertiary hospitals~\cite{fan_tabular_2024,
shmuel_comprehensive_2024,ren_deep_2025}. In external-hospital transfers, SAINT and
FT-Transformer frequently degrade to near-random calibration when categorical codes
shift or when batch-size constraints prevent stable intersample attention. The
computational footprint (long training times, GPU memory pressure) further limits
adoption in clinical IT stacks, where inference latency and cost dominate.
Empirical comparisons on clinical risk prediction echo this pattern with concrete performance gaps. TabNet often
needs extensive learning-rate scheduling and sparsity penalties to match GBDT, and
TabTransformer under-utilizes numerical biomarkers unless carefully normalized.
FT-Transformer narrows the gap by embedding every feature, yet its quadratic
self-attention becomes impractical for wide tables. SAINT's intersample attention
helps when minibatches are large, but collapses on scarce data, making these models
fragile without strong regularization and carefully tuned augmentations.

Detailed benchmarking studies on clinical datasets reveal stark contrasts between large-scale public benchmarks and real-world medical cohorts. On UCI repository datasets with $>$10,000 samples, TabNet achieves AUROCs of 0.85--0.92, FT-Transformer reaches 0.87--0.94, and SAINT obtains 0.86--0.93, competitive with or slightly exceeding XGBoost's 0.84--0.91~\cite{gorishniy2021revisiting,shmuel_comprehensive_2024}. However, when evaluated on authentic clinical cohorts with $<$1,000 patients and significant missingness, the same models show dramatic performance degradation: TabNet AUROCs fall to 0.62--0.71, FT-Transformer to 0.65--0.74, and SAINT to 0.60--0.69, while XGBoost maintains relatively stable performance at 0.75--0.83~\cite{fan_tabular_2024,ren_deep_2025}.

The computational requirements create additional barriers to clinical adoption. Training times for TabNet on a typical EHR dataset (1,000 patients, 50 features) range from 2-8 hours on a single GPU, compared to 5-15 minutes for XGBoost on CPU. FT-Transformer requires 4-12 hours due to its attention mechanisms, and SAINT needs 6-15 hours plus substantial memory for inter-sample attention matrices~\cite{fan_tabular_2024}. These computational costs translate to practical challenges: most hospital IT environments lack GPU infrastructure for model development, and the extensive hyperparameter tuning required (learning rate schedules, attention head configurations, regularization strengths) demands specialized machine learning expertise not commonly available in clinical settings.

Furthermore, calibration studies reveal that deep tabular models often produce overconfident predictions on medical data. Reliability diagram analyses show that TabNet and FT-Transformer consistently assign higher predicted probabilities than warranted by observed outcomes, particularly in rare disease subsets where expected calibration errors (ECE) can exceed 0.15--0.20 compared to XGBoost's 0.04--0.08~\cite{guo_evaluation_2022,ren_deep_2025}. This overconfidence is particularly problematic for clinical decision support, where well-calibrated risk estimates are essential for appropriate triage and treatment decisions.
These limitations are amplified in clinical registries where hundreds of variables
encode comorbidities, medication history, and laboratory trajectories. Studies on
ICU mortality, sepsis, and readmission prediction report that deep tabular networks
match or slightly exceed tuned GBDTs on in-distribution test sets but lose their
advantage when evaluated on later time periods or new hospitals, especially when
categorical vocabularies change or when privacy constraints cap batch sizes~\cite{gorishniy2021revisiting,
guo_evaluation_2022,ren_deep_2025}. In such small-$N$, high-dimensional regimes,
hyperparameter sensitivity translates directly into clinical risk: minor changes in
learning rate or regularization can flip decisions near treatment thresholds.
Compared with tree ensembles, these architectures seek to learn shared feature
representations that might in principle adapt across hospitals or tasks. In
practice, however, their appetite for data and tuning means that performance gains
are often limited to large industrial benchmarks; on noisy, heterogeneous medical
tables with only a few hundred patients, they frequently underperform simpler
models and exhibit brittle calibration under shift. This contrast sets the stage
for tabular foundation models such as TabPFN, which embrace a meta-learning,
few-shot perspective instead of training a new deep network from scratch for each
cohort.

\subsubsection{Tabular foundation models}

Tabular foundation models push self-supervised pre-training and in-context learning
into structured data. TabPFN meta-trains a transformer on millions of synthetic
datasets sampled from diverse structural-causal priors, learns to approximate
posterior predictive distributions, and performs inference via a single forward
pass without gradient updates~\cite{hollmann2025accurate,hollmann_accurate_2025}.
Follow-up work expands its reach without breaking the closed-world assumption:
TabPFN-2.5 relaxes quadratic attention to accommodate tens of thousands of context
rows and documents an augmented pre-training suite; diagnostics such as ``A Closer
Look at TabPFN v2'' show that the model remains overconfident under covariate shift,
prompting wrappers that adjust representations before prediction~\cite{noauthor_prior_nodate,
noauthor_closer_nodate,noauthor_realistic_nodate}. Drift-resilient variants model
temporal shift with secondary structural-causal modules and record measurable gains
when patient mixes evolve~\cite{helli_drift-resilient_2024,noauthor_automldrift-resilient_tabpfn_2025}.
Other studies adapt the same prior-learning paradigm to drug discovery, radiomics,
and graph embeddings, highlighting both the portability and fragility of tabular
foundation models beyond flat tables~\cite{chen_tabpfn_2025,eremeev_turning_2025,
liu_tabular_2025}. Tabular Large Language Models (TabLLMs) serialize rows or mini
tables into prompts so that general-purpose LLMs can reason over discrete entries,
but they remain computationally prohibitive for high-throughput risk prediction and
struggle with precise numeric calibration~\cite{brown2020language,hegselmann2023tabllm,
jayawardhana_transformers_2025}. Recent analyses of high-dimensional omics
applications reinforce that even TabPFN requires aggressive feature selection or
prior-guided embeddings to stay calibrated, underscoring its closed-world
assumption~\cite{zhou_limitations_2025,noauthor_pdf_nodate}. PFN-Boost, LLM-Boost,
and hybrid residual schemes blend foundation backbones with tree-style updates or
prompts, but benchmark reports such as Wild-Tab still find overfitting to
training-domain quirks unless explicit alignment and calibration are layered
on~\cite{kolesnikov_wild-tab_2023,liu_tabpfn_2025,loh_basis_2025}.
Closed-world constraints surface in three ways: (i) feature mismatch---TabPFN
expects aligned schemas and cannot reason about biomarkers absent from the context;
(ii) covariate drift---attention retrieves misleading neighbors when acquisition
protocols move, producing overconfident errors; and (iii) context-length bottlenecks
that force sub-sampling when rows exceed a few thousand. These limits explain why
prior studies resort to RFE or hand-crafted embeddings before invoking TabPFN and
why drift-resilient variants add causal dynamics to temper temporal shift.

These observations motivate hybrid approaches that explicitly combine strong priors
with domain-alignment hooks. Table~\ref{tab:model_summary} summarizes the comparative
strengths and weaknesses of these model families for medical tabular tasks,
highlighting why PANDA fuses TabPFN with feature selection and unsupervised alignment
instead of relying on any single paradigm.

\begin{table}[htbp]
\centering
\caption{Comparative strengths and weaknesses of tabular model families in medical AI.}
\label{tab:model_summary}
\footnotesize
\begin{tabular}{p{2.5cm}p{3.2cm}p{3.9cm}p{3.9cm}}
\toprule
\textbf{Model Class} & \textbf{Representative Algorithms} & \textbf{Strengths in Medical AI} & \textbf{Limitations in Cross-Hospital Tasks} \\
\midrule
Tree Ensembles & XGBoost, LightGBM, CatBoost & Interpretable, robust to missingness/outliers, encode clinical constraints & Overfit small cohorts, non-differentiable, no inherent transfer learning, require full retraining per site \\
Deep Tabular & TabNet, TabTransformer, FT-Transformer, SAINT, NODE & Differentiable, capture complex interactions, allow multimodal fusion & Data hungry, extensive tuning, high compute cost, brittle without alignment \\
Foundation Models & TabPFN, TabPFN-2.5, TabLLM & Hyperparameter-free inference, strong small-$N$ priors, probabilistic outputs & Sensitive to distribution/feature shift, limited context length, assume aligned schemas \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Domain shift and domain adaptation in medical AI}

Domain adaptation (DA) provides the vocabulary for managing the covariate, label,
and concept shifts that materialize when AI crosses hospital boundaries. Classical
analysis decomposes target error into source error plus a divergence term, motivating
alignments and invariance objectives. In practice, medical deployments encounter
overlapping types of shift: changes in patient mix and ordering policies alter
$P(X)$, new screening programs or diagnostic criteria perturb $P(Y)$, and evolving
clinical practice modifies $P(Y\mid X)$~\cite{koch2024distribution,guo_evaluation_2022}.
Pulmonary nodule malignancy prediction is particularly exposed to this triad of
shifts because granulomatous disease burden, scanner protocols, and radiologist
thresholds vary sharply across regions.

\subsubsection{Statistical alignment vs. adversarial objectives}

Maximum Mean Discrepancy (as in TCA), correlation alignment (CORAL), and
transport-based projections minimize moment discrepancies in a latent
space~\cite{pan2010domain,sun2016correlationalignmentunsuperviseddomain,
grubinger2015domain,zhang_adadiag_2022,li_transport-based_2024}. They are attractive
for medical tables because they offer closed-form or deterministic solutions and
remain stable when labeled target data are absent. Adversarial approaches (DANN,
cycle-consistent style transfer) attempt to erase domain cues via discriminators,
but surveys show they destabilize when cohorts are tiny, leading to mode collapse or
erasure of clinically salient signals~\cite{guo_evaluation_2022,zhang_adadiag_2022,
guan2021domain}. In ICU mortality and readmission tasks, DANN can underperform ERM
by wide margins because the discriminator trivially detects domain cues from missing
patterns, causing the encoder to discard predictive features. In contrast, MMD- or
CORAL-style alignment improves calibration modestly and avoids catastrophic
degradation, motivating our reliance on TCA for small-sample settings.
Classic error decompositions also separate covariate shift ($P_s(X)\neq P_t(X)$)
from label shift ($P_s(Y)\neq P_t(Y)$) and concept shift ($P_s(Y|X)\neq P_t(Y|X)$);
only the first benefits cleanly from moment matching, while the second demands
prevalence-aware calibration and the third often needs feature auditing or human
review~\cite{pan2010domain,gardner_benchmarking_2024,koch2024distribution}. These
regimes frequently co-occur in multi-hospital deployments, explaining why single
DA objectives show mixed results.

\subsubsection{Heterogeneity, missingness, and temporal drift}

Medical DA must grapple with heterogeneous feature sets and evolving acquisition
policies. Feature-space DA (FSDA) and transport-based alignment project source and
target into shared latent spaces, while open-set domain adaptation handles mismatched
label spaces and schema drift that arise when hospitals collect different labs~\cite{luo2021fsda,
grubinger2015domain,pham_open-set_2025,li_transport-based_2024}. DomainATM,
feature-aware PCA, and ontological mapping frameworks first identify which
biomarkers are stable across sites before alignment, reducing negative transfer~\cite{guan2021domain,
guan_domainatm_2023,guan_domain_2022}. Missingness-shift studies demonstrate that
when ordering policies change (e.g., different lab panels for triage), standard
covariate-shift assumptions break; MNAR-aware corrections and explicit missingness
modeling become mandatory~\cite{zhou_domain_2023,stokes_domain_2025}. Temporal
adaptation work (Wild-Time, multi-attention encoders for COVID-19) highlights that
drift accumulates over months, so models require continual recalibration rather than
one-time transfer~\cite{ahn_unsupervised_2023,he_multi-attention_2022}.

\subsubsection{Domain generalization and open-set gaps}

TableShift, Wild-Tab, and Wild-Time benchmarks quantify how far models fall once
distributions move: they reveal a near-linear relation between in-distribution and
out-of-distribution accuracy, but also show that label shift dominates error budgets
and that prevailing domain-generalization objectives (GroupDRO, IRM, VREx) rarely
beat strong ERM or GBDT baselines on tabular data~\cite{gardner_benchmarking_2024,
noauthor_mlfoundationstableshift_nodate,gardner_tableshift_nodate,
kolesnikov_wild-tab_2023,ahn_unsupervised_2023}. Open-set and partial-label settings
are common in healthcare (target hospital omits certain comorbidities); current DA
methods often assume aligned label spaces and therefore miscalibrate rare conditions.
Regulatory guidance now treats shift detection and recalibration as part of
post-market surveillance, emphasizing that robustness must be engineered rather than
assumed~\cite{koch2024distribution}.
Complementary benchmarks and surveys on generic tabular learning echo these
findings: across hundreds of datasets, tuned GBDTs remain exceptionally strong
baselines, and many deep or domain-generalization architectures fail to deliver
consistent gains once evaluation moves beyond a handful of leaderboard tasks~\cite{fan_tabular_2024,
shmuel_comprehensive_2024,somvanshi2024survey}. Moreover, empirical decompositions
of error budgets highlight that label shift and calibration drift often dominate
covariate shift, suggesting that feature-space alignment alone is insufficient for
reliable deployment. Together with the medical DA literature, these results argue
for methods that combine strong small-sample priors, explicit feature governance,
and lightweight, task-aware alignment instead of relying on black-box ``robust''
architectures.

\subsubsection{Domain adaptation and transfer learning for clinical tabular and EHR data}

Recent work brings these ideas to longitudinal EHR and claims data. AdaDiag-style
methods align source and target hospitals in a representation space while jointly
training prognostic models, reporting partial recovery of AUROC lost when models
trained on MIMIC-like cohorts are evaluated at external centers~\cite{zhang_adadiag_2022,
guan2021domain}. Multi-center EHR foundation models go further by pre-training
sequence encoders on records from dozens of institutions and then fine-tuning on
downstream tasks, demonstrating that shared representations can reduce the amount
of labeled data required for local adaptation~\cite{noauthor_multi-center_nodate}.
These approaches show that both unsupervised alignment and transfer learning have
value in clinical AI, but they typically assume abundant longitudinal data, focus
on large hospitals with rich EHR infrastructure, and operate on sequential rather
than static tabular summaries.

Standard domain-adaptation theory provides a unifying lens: target risk can be
bounded by source risk plus a measure of distribution discrepancy and a term
capturing irreducible label-set differences~\cite{pan2010domain,guo_evaluation_2022}.
Reducing error on the source domain alone is therefore insufficient; one must also
control divergence between source and target feature distributions, for example via
moment-matching, adversarial objectives, or feature-space DA.
Beyond centralized settings, federated learning extends these ideas by allowing
multiple hospitals to collaborate without sharing raw data. Surveys on federated
learning for medical imaging and pattern recognition summarize how FL can pool
experience across institutions while preserving privacy, and methods such as
FedFusion explicitly combine domain adaptation with personalized encoders to handle
heterogeneous feature spaces and scarce labels~\cite{rehman_federated_2023,
guan_federated_2024,kahenga_fedfusion_2025}. However, most federated frameworks
target high-volume imaging or EHR tasks, assume substantial local computation and
at least some labeled data at each site, and still rely on shared model
architectures and broadly aligned feature schemas. They are therefore complementary
to, rather than a replacement for, lightweight DA strategies tailored to very small
tabular cohorts with partially mismatched feature sets.

Table~\ref{tab:da_strategies} summarizes the main DA families discussed above and
their implications for cross-hospital tabular deployment.

\begin{table}[htbp]
\centering
\caption{Representative domain-adaptation strategies in medical AI and their relevance to cross-hospital tabular risk prediction.}
\label{tab:da_strategies}
\footnotesize
\begin{tabular}{p{2.5cm}p{1.5cm}p{3.1cm}p{3.2cm}p{3.2cm}}
\toprule
\textbf{Method family} & \textbf{Typical modality} & \textbf{Key assumptions} & \textbf{Pros} & \textbf{Limitations for small cross-hospital tabular cohorts} \\
\midrule
Statistical alignment (MMD, TCA, CORAL, transport) & Tabular, EHR, imaging & Shared feature schema; access to source data and unlabeled target samples; primarily covariate shift & Closed-form or deterministic mappings; stable when target labels are absent; easy to plug into existing pipelines~\cite{pan2010domain,sun2016correlationalignmentunsuperviseddomain,grubinger2015domain,li_transport-based_2024} & Does not directly correct label or concept shift; assumes overlapping feature sets; may misalign rare subgroups without additional calibration~\cite{gardner_benchmarking_2024,koch2024distribution,guo_evaluation_2022} \\
Adversarial representation learning (DANN-style) & Imaging, EHR sequences & Access to source and target data with domain labels; discriminator encouraged to remove site identity & Learns domain-invariant representations jointly with task loss; flexible for complex modalities~\cite{guo_evaluation_2022,zhang_adadiag_2022} & Unstable on tiny cohorts; discriminators exploit missingness patterns, causing encoders to discard predictive features; can underperform ERM in ICU-style tasks~\cite{zhang_adadiag_2022,guan2021domain} \\
Feature-space DA and domain-aware FS (FSDA, DomainATM) & Tabular, EHR & At least partially shared feature space; access to both domains during training & Selects features that are predictive and stable across sites; reduces reliance on site-specific surrogates and noisy biomarkers~\cite{luo2021fsda,guan2021domain,guan_domainatm_2023,guan_domain_2022} & Still assumes sizable overlap in measured variables; does not natively handle missing entire feature blocks or unlabeled target hospitals with severe schema mismatch \\
Domain generalization and temporal adaptation (TableShift, Wild-Tab, Wild-Time) & Tabular & Multiple labeled source distributions; no target labels during training & Reveal failure modes under temporal, demographic, and institutional shift; provide standardized evaluation suites~\cite{gardner_benchmarking_2024,gardner_tableshift_nodate,kolesnikov_wild-tab_2023,ahn_unsupervised_2023} & Many domain-generalization objectives (e.g., GroupDRO, IRM, VREx) rarely beat strong ERM or GBDT baselines; benchmarks show label shift and calibration drift dominate what feature matching can fix~\cite{gardner_benchmarking_2024,noauthor_mlfoundationstableshift_nodate} \\
Federated and federated-DA frameworks (FL, FedFusion-style) & Imaging, tabular & Multiple compute-capable hospitals; communication budget; typically some local labels and shared model architecture~\cite{rehman_federated_2023,guan_federated_2024,kahenga_fedfusion_2025} & Preserve data privacy while learning from distributed cohorts; can combine personalization with domain adaptation and label efficiency & Often require significant local computation and labeled target data; focus on large imaging or EHR tasks; do not directly address very small tabular cohorts with feature mismatch and strict label scarcity \\
\bottomrule
\end{tabular}
\end{table}

Existing EHR-focused methods mostly address temporal drift or site differences in
large cohorts, whereas our setting combines small, imbalanced tabular cohorts,
heterogeneous feature sets, and unlabeled target hospitals. This gap motivates
PANDA's combination of strong tabular priors, cross-domain feature selection, and
lightweight alignment tailored to static risk scores rather than long EHR sequences.

\subsection{Feature selection and domain-aware stability for small medical cohorts}

High-dimensional yet small-sample tabular cohorts are ubiquitous in medicine: lung
screening registries, omics panels, and survey-based risk scores often contain
hundreds of variables for only a few hundred or thousand patients. Na\"{i}ve
learning in this regime leads to unstable decision boundaries and non-reproducible
feature attributions. Feature selection methods aim to reduce dimensionality,
stabilize inference, and focus clinician attention on biomarkers that are both
predictive and economical to collect.

\subsubsection{Small-sample and high-dimensional feature selection}

Classical filter and wrapper methods, such as mutual information ranking or
recursive feature elimination with SVMs, laid the groundwork for identifying
compact biomarker sets but struggle when features are highly correlated or when
class imbalance is severe~\cite{guyon2002gene}. More recent approaches explicitly
target high-dimensional, low-sample-size settings. WPFS-style methods learn
feature weights jointly with a classifier, GRACES uses graph convolutions to
propagate importance across correlated features, and DeepFS leverages deep networks
to screen features via nonlinear embeddings~\cite{chen2023graces,liu2022deepfs,
li2023deep}. These techniques are attractive for medical AI because they can
down-select from hundreds of candidate variables to a dozen stable predictors while
controlling overfitting. Empirical studies on omics and imaging-genomics datasets
show that such methods can maintain or even improve AUC while halving the number of
features, directly reducing assay costs and simplifying model interpretation.
However, most of these works assume a single training domain: the selected subset
is optimized for internal performance and may not transfer when another hospital
measures a slightly different panel or when missingness patterns change.
From a methodological standpoint, this marks a shift from classical LASSO or
univariate ranking---which rely on linear or marginal-effect assumptions and can be
highly unstable in small cohorts---to architectures that explicitly model complex
feature interactions and redundancy. WPFS and GRACES, for example, introduce
auxiliary networks or graph structures to propagate importance across correlated
features, while DeepFS leverages deep encoders to identify nonlinear manifolds
where only a subset of variables drive variation~\cite{chen2023graces,liu2022deepfs,
li2023deep}. These designs are particularly appealing in high-dimensional, sparse
medical settings (omics panels, questionnaire data), but they still optimize for
one domain at a time and do not ensure that the chosen biomarkers remain predictive
under cross-hospital shift.

\subsubsection{Feature selection with transformers and foundation models}

Attention-based models provide an alternative route to feature selection by
interpreting attention weights, learned masks, or perturbation scores as measures of
importance. TabNet learns sparse feature masks that indicate which variables are
consulted at each decision step, while transformer-based architectures expose
token-level attention maps that can be aggregated across layers and heads~\cite{arik2021tabnet,
huang2020tabtransformer,somepalli2021saint,somvanshi2024survey}. In practice,
researchers often perform permutation-based importance estimation using a strong
tabular backbone---GBDT or TabPFN---and then apply RFE-style pruning, retaining the
top-k features that consistently contribute to performance. This paradigm is
well-suited to small medical cohorts because it leverages the inductive biases of
powerful models while regularizing the input space. For foundation models such as
TabPFN, feature selection also mitigates closed-world constraints: by removing
unstable or site-specific variables, one can reduce the chance that attention
focuses on hospital identifiers or acquisition artifacts rather than pathology.

\subsubsection{Domain-aware and cross-site feature selection}

Standard feature selection treats all samples as exchangeable, implicitly assuming
that feature-importance rankings are identical across domains. Domain-aware methods
instead optimize a subset that is simultaneously predictive in multiple hospitals
or under multiple sampling schemes. FSDA and related frameworks extend DA
objectives with feature-level penalties, rewarding variables whose contributions
remain stable after alignment~\cite{luo2021fsda,sun2019informative}. Multi-site
studies on EHR and imaging data show that such cross-domain criteria can discard
site-specific surrogates (e.g., local procedure codes) while preserving clinically
meaningful biomarkers. PANDA adopts this philosophy in a pragmatic way: TabPFN is
used as a strong scoring model, but feature elimination is guided jointly by
source-site performance and cross-site stability, leading to a compact
``best8'' subset that is consistently informative in both hospitals. These
domain-aware subsets provide low-dimensional, harmonized inputs to TCA, reducing
the risk of negative transfer and making the subsequent alignment problem better
posed.
Viewed through this lens, feature selection becomes a form of implicit domain
alignment: instead of matching full distributions in a high-dimensional space, one
first discards variables whose predictive contribution is strongly domain-specific
and focuses on biomarkers that are consistently informative across sites. This is
particularly valuable when hospitals measure different panels or exhibit pronounced
missingness shift, because aligning on a smaller, shared subset of stable features
is both statistically and operationally simpler. PANDA effectively instantiates
this principle by using a pre-trained tabular foundation model to rank features
jointly across two hospitals and retaining only those with robust importance,
thereby coupling representation learning with domain-aware feature governance.

\begin{table}[htbp]
\centering
\caption{Representative feature selection methods for small, imbalanced, high-dimensional biomedical tabular data.}
\label{tab:fs_methods}
\scriptsize
\begin{tabular}{p{2.0cm}p{2.0cm}p{2.8cm}p{3.2cm}p{3.2cm}}
\toprule
\textbf{Method} & \textbf{Model family} & \textbf{Small-sample / imbalance handling} & \textbf{Interpretability characteristics} & \textbf{Representative biomedical use cases} \\
\midrule
Recursive feature elimination (RFE) with linear or tree models & Wrapper around SVM, logistic regression, or tree ensembles & Wrapper search over feature subsets can overfit when $N$ is small and features are correlated; often combined with cross-validation and class-balanced sampling & Produces explicit ranked feature lists and compact subsets; easy to inspect and map to clinical variables~\cite{guyon2002gene} & Widely used in early gene-expression and biomarker panels; basis for many clinical risk-score and radiomics pipelines \\
LASSO / elastic-net logistic regression & Embedded linear models & $\ell_1$ or $\ell_1{+}\ell_2$ penalties shrink coefficients, providing some robustness to high dimensionality; still assumes linear log-odds and can be unstable under heavy collinearity & Sparse coefficients directly indicate selected features; compatible with odds-ratio interpretation familiar to clinicians~\cite{tibshirani1996regression} & Common in radiomics and EHR risk models where interpretability and coefficient-based reporting are required \\
GRACES & Graph-convolutional-network-based FS~\cite{chen2023graces} & Specifically targets high-dimensional, low-sample-size data by modeling feature relations on a graph; alleviates overfitting compared with independent filters & Outputs a compact subset informed by graph structure; can be visualized as a network of interacting biomarkers & Demonstrated on omics-style datasets; suitable when prior knowledge or correlations between biomarkers are important \\
DeepFS & Deep feature screening with autoencoders~\cite{liu2022deepfs,li2023deep} & Uses deep encoders to learn low-dimensional representations and rank features, handling ultra-high-dimensional, sparse, and potentially imbalanced data & Provides importance scores for each original feature; retains flexibility to operate in supervised or unsupervised mode & Evaluated on synthetic and biomedical high-dimensional datasets; useful when the number of variables far exceeds the number of patients \\
Domain-aware FS (FSDA-style) & Feature selection for domain adaptation~\cite{luo2021fsda} & Encourages selection of features that remain predictive across domains, implicitly handling covariate shift between sites & Produces subsets that are jointly predictive and domain-stable, supporting cross-hospital deployment & Applied to benchmark DA tasks; conceptually aligned with cross-hospital biomarker selection in multi-center medical studies \\
Transformer / foundation-model-based FS & Attention- or score-based selection using TabNet, TabTransformer, and tabular foundation models~\cite{arik2021tabnet,huang2020tabtransformer,somvanshi2024survey} & Leverages high-capacity or pre-trained models to estimate nonlinear feature importance; can be combined with RFE to mitigate small-sample overfitting & Attention weights, feature masks, or permutation-based scores yield ranked features; aligns with explainable-AI practices & Increasingly used in biomedical tabular and omics datasets; PANDA's cross-cohort RFE uses a tabular foundation model as the scoring backbone \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Pulmonary nodule malignancy prediction: from clinical scores to multi-modal AI}

\subsubsection{Clinical risk scores and logistic models}

Pulmonary nodule malignancy prediction is a canonical testbed for cross-domain
robustness. Classical logistic scores---Mayo Clinic, Veterans Affairs, Brock
(PanCan), PKUPH, Li, and derivatives---achieve internal AUCs above 0.85 but
regularly drop to 0.60--0.80 in external validations, especially in Asian or
community-screening cohorts where disease prevalence and case mix differ from the original
development populations~\cite{swensen1997chest,swensen1997archives,mcwilliams2013probability,
li2011development,he2021novel,garau_external_2020,zhang_comprehensive_2022,
liu_establishment_2024}. These scores typically combine age, smoking history,
nodule size, location, and morphology into a logit-based risk function. Meta-analyses
covering tens of thousands of nodules confirm that calibration deteriorates most
severely in subgroups such as solitary upper-lobe nodules and specific ethnic
groups, reflecting both label-shift and covariate-shift mechanisms~\cite{garau_external_2020,
zhang_comprehensive_2022,chen_pulmonary_2025}. Recent Chinese multi-centre studies further report that Brock- and PKUPH-type models can
experience substantial AUC declines when applied to contemporary Chinese screening cohorts, even
after refitting, consistent with shifts in underlying prevalence and competing benign conditions~\cite{zhang_comprehensive_2022,
liu_establishment_2024}. While
recalibration or re-estimation of coefficients can partially restore performance,
these fixes require local labels and do not address feature-mismatch: new hospitals
may lack some variables (e.g., emphysema grading) or measure them differently.

Targeted audits make the degradation concrete. In TB-endemic Korean hospitals,
Mayo and VA shrink to AUC $\approx$0.60 while Brock declines to $\approx$0.68
despite an internal AUC near 0.94, and Chinese multi-center studies find that
Brock and PKUPH can fall from $\approx$0.90 internally to 0.70--0.77 once
prevalence and granulomatous disease rates shift~\cite{yang_comparison_2018,
cui_comparison_2019,li_evaluation_2020}. PET-augmented variants such as the
Herder score raise internal discrimination to $\approx$0.92 by incorporating
metabolic imaging, yet they lose specificity in TB-endemic or inflammatory
regions where uptake is nonspecific~\cite{herder_clinical_2005,yang_comparison_2018}.
These case studies underscore that most clinical scores embed site-specific
prevalence, referral patterns, and feature definitions, so ``plug-and-play''
deployment without alignment is unrealistic.

Each classical score carries its own design trade-offs. The Mayo Clinic model was
derived from several hundred clinic-referred patients with indeterminate nodules,
emphasizing age, smoking, nodule diameter, spiculation, and upper-lobe location,
whereas the Veterans Affairs model targeted high-risk, predominantly male veterans
with larger lesions~\cite{swensen1997chest,swensen1997archives}. The Brock (PanCan)
model was trained in a screening cohort enriched for small nodules and incorporates
emphysema, family history, and more granular morphology descriptors, while the
PKUPH and Li scores adapt similar feature sets to Chinese tertiary-hospital and
screening populations~\cite{mcwilliams2013probability,li2011development,he2021novel,
liu_establishment_2024}. A recent meta-analysis focused on the Brock model reports
pooled AUC $\approx 0.80$ across $>80{,}000$ patients but highlights substantially
lower performance in Asian cohorts, solitary nodules, subsolid nodules, and larger
lesions (AUC often $\approx 0.74$ or below), underscoring that apparent
``universality'' in development data masks sizeable domain-specific errors~\cite{chen_pulmonary_2025}.
Across Mayo, VA, Brock, and PKUPH, external validations repeatedly document drops
from internal c-statistics in the high-0.80s to 0.60--0.75 when applied to community
screening or granulomatous-diseaseâ€“endemic regions~\cite{garau_external_2020,
zhang_comprehensive_2022,liu_establishment_2024}.

These patterns can be summarized along three axes: development cohorts are often
single-center and demographically narrow; variables focus on easily collected
clinical and simple CT descriptors; and the underlying model is a logistic
regression that assumes a linear log-odds relationship between covariates and
malignancy. Table~\ref{tab:nodule_scores} sketches representative scores along
these dimensions. In development, all achieve reasonable discrimination and are
simple enough to implement as bedside calculators, but the same simplicity makes
them brittle under shift: logistic coefficients absorb local prevalence, imaging
protocols, and referral patterns, so external use without recalibration results in
systematic underestimation or overestimation of risk in particular subgroups.

\begin{table}[htbp]
\centering
\caption{Representative pulmonary nodule malignancy scores and common external-validation issues.}
\label{tab:nodule_scores}
\footnotesize
\begin{tabular}{p{2.2cm}p{3.2cm}p{4.0cm}p{4.0cm}}
\toprule
\textbf{Score} & \textbf{Development cohort} & \textbf{Key variables} & \textbf{External-validation observations} \\
\midrule
Mayo Clinic & Clinic-referred indeterminate nodules in smokers & Age, smoking history, nodule size, spiculation, upper-lobe location & Internal AUC in the high-0.80s; frequent overestimation of risk and AUC drops to $\approx$0.6--0.7 in screening and non-U.S. cohorts \\
Veterans Affairs & Predominantly male veterans with larger nodules & Age, smoking, nodule diameter, location & Good performance in veterans; miscalibration when transported to mixed-gender or lower-risk populations \\
Brock (PanCan) & CT screening cohort with many small nodules & Age, sex, family history, emphysema, size, type, location & Meta-analytic pooled AUC $\approx 0.80$; markedly lower AUC in Asian, solitary, and subsolid nodules~\cite{chen_pulmonary_2025} \\
PKUPH / Li & Chinese tertiary-hospital and screening cohorts & Age, smoking, nodule size and type, lobulation, spiculation & High internal AUC but drops in external series; performance depends strongly on CT protocol and case mix~\cite{li2011development,he2021novel,liu_establishment_2024} \\
\bottomrule
\end{tabular}
\end{table}

From the perspective of this thesis, these scores provide clinically interpretable
baselines and useful prior knowledge about which coarse-grained descriptors matter,
but they do not solve cross-hospital robustness. Their small development cohorts
and rigid functional form make it difficult to incorporate new biomarkers or adapt
to feature-mismatch without re-estimating the entire model, motivating more flexible
tabular approaches that can share information across hospitals while respecting
regulatory demands for calibration and subgroup transparency.

\subsubsection{Radiomics pipelines with traditional machine learning}

Radiomics pipelines extract hundreds to thousands of hand-crafted features from CT
volumes, offering richer representations than clinical risk scores but introducing
major reproducibility hazards. Texture and wavelet descriptors vary with voxel
spacing, reconstruction kernel, and segmentation protocol; ComBat-style harmonization
reduces scanner effects yet requires batch labels and can blur subtle lesions~\cite{hellin2024unraveling,
garau_external_2020}. In internal validation, radiomics-based classifiers that pair
LASSO- or stability-selected feature subsets with SVMs, random forests, or GBDTs
typically report AUCs in the 0.75--0.90 range, but these numbers rarely carry over
to new scanners or hospitals. External validations on LIDC-IDRI, LUNA16, and NLST
repeatedly report double-digit AUC drops when deployed to scanners with different
kernels or patient mixes, while shortcut-learning analyses show that models sometimes
rely on grid artifacts or reconstruction noise rather than morphology~\cite{ardila_end--end_2019,
zech_variable_2018,koch2024distribution}. These failures illustrate that radiomics
alone cannot guarantee transportability and that alignment plus feature vetting are
required before cross-hospital use.

Concrete exemplars reinforce that fragility. The Bayesian Integrated Malignancy
Calculator (BIMC) blended radiomics with clinical covariates and modestly
outperformed Mayo, Brock, and PKUPH (AUC $\approx$0.90 vs. $\approx$0.78) on an
Italian derivation cohort, yet its advantage diminished when scanners, slice
thickness, or kernels changed~\cite{perandini_solid_2016}. Hawkins-style NLST
radiomics achieved AUC $\approx$0.83 without external validation, and retrospective
audits show that a single reconstruction tweak can reorder the features selected by
LASSO~\cite{garau_external_2020,hellin2024unraveling}. Radiomics therefore supplies
richer morphology descriptors but still requires harmonization, feature governance,
and domain-aware alignment rather than assuming reproducibility across hospitals.

\begin{table}[htbp]
\centering
\caption{Representative radiomics-based pulmonary nodule malignancy models and reported generalization behavior.}
\label{tab:radiomics_models}
\scriptsize
\begin{tabular}{p{2.2cm}p{2.0cm}p{1.5cm}p{2.0cm}p{1.0cm}p{1.0cm}p{2.8cm}}
\toprule
\textbf{Study / model} & \textbf{Imaging data} & \textbf{Centers / nodules} & \textbf{Classifier} & \textbf{Internal AUC} & \textbf{External AUC} & \textbf{Harmonization / scanner sensitivity} \\
\midrule
Generic radiomics pipelines & 2D/3D chest CT or low-dose CT nodules & Single- or few-center cohorts (sizes vary) & LASSO- or stability-selected features with SVM, RF, or GBDT & Typically 0.75--0.90 & 0.10--0.20 lower & Performance degrades when kernel, slice thickness, or vendor changes; ComBat-style harmonization can reduce but not eliminate scanner effects~\cite{garau_external_2020,hellin2024unraveling} \\
Multi-center reproducibility analyses & 3D CT radiomics features across scanners & Multi-center CT datasets & -- & -- & -- & Many texture features show intraclass correlation coefficients $<0.5$ across vendors and protocols; harmonization helps but cannot fully restore stability~\cite{hellin2024unraveling} \\
Radiomics + clinical scoring models & Radiomics signatures combined with clinical descriptors & Hospital-specific or regional nodule cohorts & Elastic-net / logistic regression, RF, or GBDT & High ($\geq 0.80$) & Mid-0.70s or lower & External validations report double-digit AUC loss and sensitivity to case mix and acquisition protocol~\cite{garau_external_2020} \\
\bottomrule
\end{tabular}
\end{table}

Published inter-scanner analyses often report intraclass correlation coefficients
below 0.5 for entropy and run-length features, indicating poor reliability even
before model fitting~\cite{hellin2024unraveling}. ComBat can regress out known batch
effects when acquisition labels are available, but it can also blur subtle lesions
and fails when batch membership is unknown at inference time, leaving a gap that
tabular-alignment pipelines attempt to close. Beyond handcrafted features, many
radiomics pipelines incorporate LASSO, elastic-net logistic regression, or
stability-selection frameworks to shrink coefficients and stabilize feature sets
before training SVM, random forest, or GBDT classifiers. Although these strategies
help curb overfitting in small cohorts, they do not eliminate sensitivity to
acquisition protocols: the same feature may be retained in one scanner configuration
and discarded in another because its estimated importance changes with kernel or
slice thickness. Multi-center studies frequently report 10--20 percentage-point AUC
drops when models are transported without revisiting segmentation, feature
extraction, and harmonization choices~\cite{garau_external_2020,hellin2024unraveling}.
As a result, radiomics pipelines tend to behave like carefully tuned, center-specific
instruments rather than plug-and-play risk predictors, and their complexity makes it
hard for clinicians to trace failure modes back to specific preprocessing or
feature-engineering steps.

\subsubsection{Deep-learning CAD systems}

End-to-end deep-learning computer-aided diagnosis (CAD) systems extend the radiomics
pipeline by learning 3D convolutional representations directly from CT volumes or
multi-view patches. Large-scale screening trials such as NLST have enabled
3D CNNs to achieve AUCs in the mid-0.90s on internal validation, sometimes
matching or surpassing expert radiologists~\cite{ardila_end--end_2019}. Subsequent
works combine deep features with handcrafted radiomics or clinical covariates,
showing further gains on curated datasets~\cite{li_predicting_2019,lin_combined_2024}.
Causey et al.'s NoduleX reproduced radiologist malignancy ratings with AUC
$\approx$0.99 on LIDC-IDRI, and Google's NLST-scale 3D CNN maintained AUC
$\approx$0.94 on an independent hospital cohort of 1{,}139 CTs, illustrating how
massive, homogeneous datasets can suppress variance~\cite{causey_highly_2018,
ardila_end--end_2019}.
However, these successes often rely on tightly controlled acquisition protocols and
substantial annotation effort. External validations reveal double-digit AUC drops
when voxel spacing, reconstruction kernels, or vendor mix shift, and shortcut-learning
analyses demonstrate that CNNs may rely on markers, reconstruction noise, or scanner
metadata rather than nodule morphology~\cite{zech_variable_2018,hellin2024unraveling,
koch2024distribution}. Moreover, most deep CAD systems treat imaging in isolation or
only append a handful of clinical variables, limiting their ability to reason over
complex comorbidity profiles or laboratory trajectories. Multi-view and multi-scale
architectures that process cropped nodules, surrounding parenchyma, and whole-lung
context can mitigate some of these issues, but they further increase computational
cost and annotation effort. Multi-task variants that jointly predict malignancy,
growth, or histological subtype promise richer supervision but require large,
carefully curated datasets that few hospitals possess. In practice, many published
CAD systems are trained and tuned on a single trial or institution, with limited
reporting on cross-hospital generalization or calibration. Where multi-center
experiments are reported, performance is typically rescued by site-specific
fine-tuning on labeled cases from each target hospital, and very few studies attempt
label-free ``train at A, deploy at B'' deployment. As a result, deep CAD systems
remain powerful local tools rather than robust cross-hospital risk predictors.

\subsubsection{Tabular and multi-modal nodule models}

Later machine-learning models---LASSO, random forests, GBDTs, Bayesian networks, and
hybrid radiomics-clinical models---attempt to combine the strengths of scores and
imaging~\cite{he2021novel,garau_external_2020,li_predicting_2019,lin_combined_2024}.
GBDTs and random forests improve internal calibration and handle nonlinear
interactions but still require site-specific recalibration or feature mapping before
deployment because their learned weights implicitly encode scanner kernels and local
smoking histories. Multi-modal models that fuse deep image features with clinical
covariates via late fusion or stacking demonstrate promising gains on LIDC-IDRI and
NLST, yet most studies remain single-center or rely on random train--test splits
that do not reflect real cross-hospital deployment. Only a handful of works evaluate
performance when training on one hospital and testing on another, and these typically
report substantial AUC drops and unstable decision thresholds~\cite{garau_external_2020,
hellin2024unraveling}. Recent multi-center studies in Asian and Chinese screening
cohorts echo this pattern: even when models are re-estimated or augmented with
additional imaging features for new hospitals, external AUCs often plateau in the
low- to mid-0.70s and remain sensitive to protocol details and case mix~\cite{garau_external_2020,
hellin2024unraveling,wu_strategy_2023}. These observations motivate a shift toward tabular-centric
models that can incorporate imaging-derived biomarkers while explicitly handling
feature mismatch and domain shift rather than assuming homogeneous acquisition.
Within the tabular family, two broad patterns emerge. Purely clinical models use
logistic regression or tree ensembles on demographics, smoking history, and simple
CT descriptors, sometimes enriched with laboratory indices or comorbidity scores.
These models are attractive for deployment because all inputs are routinely
available in electronic health records, yet they inherit the limitations of
classical scores: most are developed and validated in a single institution, assume
aligned features across sites, and rarely report behavior under explicit domain
shift. Hybrid models instead treat radiomics signatures or deep image embeddings as
additional covariates in a tabular classifier, enabling richer decision boundaries
while retaining some interpretability via variable-importance analyses. However,
their feature spaces are even more brittle across scanners and hospitals, as both
image-derived and clinical variables can change distributions or go missing.

Existing works seldom implement formal domain-adaptation strategies for these
tabular or multi-modal models. External evaluations, when present, typically test a
fixed model on a new hospital without feature re-alignment or recalibration,
documenting sizable performance degradation but not offering systematic remedies.
Only a few studies experiment with simple recalibration or refitting on a small
local sample, and virtually none explore cross-domain feature selection or latent
alignment tailored to nodule malignancy prediction~\cite{garau_external_2020,
hellin2024unraveling}. Consequently, the literature lacks robust, tabular-centric
frameworks that (i) start from strong small-sample priors, (ii) identify a compact
set of biomarkers stable across hospitals, and (iii) explicitly align feature
distributions without assuming access to large labeled target cohorts. Taken
together, these studies show that neither handcrafted risk scores, radiomics
pipelines, nor deep CNN-based CAD systems currently offer reliable malignancy
prediction across hospitals without local retraining or recalibration. Addressing
these gaps is a central motivation for the PANDA framework developed in this thesis.

\subsection{Benchmarks and open problems for cross-domain tabular learning}

Beyond single-institution case studies, public benchmarks now stress-test shift
robustness. TableShift curates 15 binary tasks across healthcare, finance, and public
policy, with explicit temporal, geographic, and demographic shifts to measure
out-of-distribution accuracy drops and calibration drift~\cite{gardner_benchmarking_2024,
gardner_tableshift_nodate}. Wild-Tab extends this idea to few-shot, structure-aware
adaptation, showing that even tabular foundation models lose 5--15 AUC points under
schema-preserving shifts~\cite{kolesnikov_wild-tab_2023}. Wild-Time focuses purely on
temporal drift, revealing that performance decays monotonically unless models refresh
their priors~\cite{ahn_unsupervised_2023}. These resources contrast with medical
imaging benchmarks, where the input grid is fixed; in tabular settings, feature
heterogeneity and missingness add extra axes of mismatch. Our inclusion of the
TableShift BRFSS Diabetes race-shift task aligns the pulmonary nodule study with a
large-scale public benchmark, demonstrating that the proposed alignment strategy is
not confined to proprietary cohorts.
TableShift also surfaces common failure modes: GroupDRO and IRM rarely beat ERM on
tabular tasks, label shift explains much of the OOD loss, and high ID accuracy is
necessary but insufficient for shift robustness~\cite{gardner_benchmarking_2024,
noauthor_mlfoundationstableshift_nodate}. Wild-Time isolates temporal drift,
showing monotonic degradation without continual recalibration~\cite{ahn_unsupervised_2023};
these findings mirror hospital deployments where assay updates or policy changes
quietly reshape feature distributions.

\subsubsection{Gap analysis and positioning of PANDA}

Across model families and adaptation techniques, several open issues persist. First,
closed-world assumptions in tabular foundation models preclude feature-mismatched
deployment: TabPFN and its variants require aligned schemas and struggle when target
hospitals omit or redefine biomarkers. Second, most DA methods presume access to
abundant labeled or schema-aligned target data, which is unrealistic in
privacy-constrained hospitals and incompatible with regulatory expectations that
models remain stable under silent drift~\cite{koch2024distribution,guo_evaluation_2022}.
Third, missingness shift and label shift remain underexplored despite being dominant
drivers of clinical miscalibration in TableShift and Wild-Time; simply matching
latent distributions cannot fix changes in prevalence or ordering policies~\cite{gardner_benchmarking_2024,
ahn_unsupervised_2023}. Finally, reproducibility crises in radiomics and deep imaging
models show that aggressive priors or harmonization cannot replace explicit
alignment and feature governance~\cite{hellin2024unraveling,koch2024distribution}.

PANDA is designed to address a specific intersection of these gaps rather than
compete with every prior line of work. By treating a tabular foundation model as a
plug-in prior, PANDA inherits strong small-sample performance without hand-tuning
but augments it with cross-domain RFE that explicitly searches for a compact subset
of biomarkers stable across hospitals. This step operationalizes domain-aware
feature selection, yielding a shared feature set (``best8'') that remains
predictive in both institutions and provides harmonized inputs for subsequent
alignment. TCA is then applied in the latent space induced by TabPFN, combining the
representation power of foundation models with the stability of kernel-based
alignment to handle unlabeled target data. The same pipeline is evaluated both on a
private cross-hospital pulmonary nodule cohort and on the public TableShift BRFSS
Diabetes race-shift task, demonstrating that the ingredients are not handcrafted for
a single dataset but generalize across tabular shift scenarios~\cite{gardner_benchmarking_2024,
noauthor_mlfoundationstableshift_nodate}. To our knowledge, this is the first
framework to jointly combine a tabular foundation model, cross-domain RFE, and TCA
for cross-hospital pulmonary nodule risk prediction and public TableShift-style
tabular shift benchmarks. In this sense, PANDA fills the gap between
single-domain tabular FMs, imaging-focused DA, and benchmark-driven tabular DA by
providing an end-to-end, alignment-aware framework tailored to small, imbalanced,
and feature-mismatched medical cohorts.

\label{sec:rw-end}
