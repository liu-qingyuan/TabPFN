\section{Evaluation}
\label{sec:eval-start}
\label{sec:evaluation}

We assess PANDA across cross-institutional performance, domain adaptation, interpretability, and clinical utility, using a protocol meant to resemble what deployment would actually look like.

\subsection{Evaluation Metrics and Statistical Analysis}

\subsubsection{Classification Performance Metrics}

Results are averaged over 10-fold stratified cross-validation to temper label imbalance. The metrics are:

\[
\begin{aligned}
&\text{True Positive Rate:} && \tpr(\tau) = \frac{\tp(\tau)}{\tp(\tau) + \fn(\tau)} \\
&\text{False Positive Rate:} && \fpr(\tau) = \frac{\fp(\tau)}{\fp(\tau) + \tn(\tau)} \\
&\text{AUC:} && \auc = \int_0^1 \tpr(\tau)\, d(\fpr(\tau)) \\
&\text{Accuracy:} && \frac{\tp + \tn}{\tp + \tn + \fp + \fn} \\
&\text{Precision:} && \frac{\tp}{\tp + \fp} \\
&\text{Recall (Sensitivity):} && \frac{\tp}{\tp + \fn} \\
&\text{F1 Score:} && \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} \\
&\text{Specificity:} && \frac{\tn}{\tn + \fp}
\end{aligned}
\]

Let $\mathcal{D} = \{(\featurevec_i, \labelval_i)\}_{i=1}^n$ denote the full dataset, and $\mathcal{D}_k$ be the $k$-th fold. For metric $M$, the mean and standard deviation over $\numfolds=10$ folds are:

\[
\meanmetric = \frac{1}{\numfolds}\sum_{k=1}^{\numfolds} \metrick, \quad \stdmetric = \sqrt{\frac{1}{\numfolds-1} \sum_{k=1}^{\numfolds} (\metrick - \meanmetric)^2}
\]

\subsubsection{Visualization-Based Evaluation}

\begin{itemize}
\item \textbf{ROC Curves:} Plot $\tpr(\tau)$ versus $\fpr(\tau)$ for $\tau \in [0,1]$ to see the sensitivity-specificity trade-off. 

\item \textbf{Calibration Curves:} Check agreement between predicted probability $\predprob$ and observed frequency $\labelval_i$. For $\numfolds$ equal-width bins $\calibbin = [k/\numfolds, (k+1)/\numfolds)$:

\[
\meanpredprob = \frac{1}{|\calibbin|} \sum_{i \in \calibbin} \predprob, \quad \meanobservedfreq = \frac{1}{|\calibbin|} \sum_{i \in \calibbin} \labelval_i
\]

\item \textbf{Decision Curve Analysis (DCA):}

\[
\netbenefit = \frac{\tp(\probthreshold)}{n} - \frac{\fp(\probthreshold)}{n} \cdot \frac{\probthreshold}{1 - \probthreshold}
\]

With benchmark strategies:

\[
\nball(\probthreshold) = \text{Prevalence} - (1 - \text{Prevalence}) \cdot \frac{\probthreshold}{1 - \probthreshold}, \quad \nbnone = 0
\]
\end{itemize}

\subsection{Experimental Setup and Results}

Structured clinical data from two cancer centers in China provided a training cohort (Cohort A, $\sourcedatasize=295$) and an external test cohort (Cohort B, $\targetdatasize=190$). Cohort A contained 63 structured features; Cohort B contained 58 (Table~\ref{tab:cohort_summary}).

\begin{table}[htbp]
\centering
\caption{The training (Cohort A) and testing (Cohort B) cohorts.}
\label{tab:cohort_summary}
\begin{tabular}{lcc}
\hline
\textbf{Characteristic} & \textbf{Cohort A (n = 295)} & \textbf{Cohort B (n = 190)} \\
\hline
Upper lobe & & \\
\quad Yes/Positive & 121 (41.0\%) & 98 (51.6\%) \\
\quad No/Negative & 174 (59.0\%) & 92 (48.4\%) \\
Age (years) & 56.95 $\pm$ 11.03 & 58.26 $\pm$ 9.57 \\
Lobe location (upper) & & \\
\quad Category 1 & 161 (54.6\%) & 98 (51.6\%) \\
\quad Category 2 & 29 (9.8\%) & 18 (9.5\%) \\
\quad Category 3 & 105 (35.6\%) & 74 (38.9\%) \\
DLCO1 & 5.90 $\pm$ 2.89 & 6.31 $\pm$ 1.55 \\
VC & 3.33 $\pm$ 0.80 & 2.92 $\pm$ 0.73 \\
CEA & 4.23 $\pm$ 6.90 & 4.15 $\pm$ 10.61 \\
Outcome (Malignant) & & \\
\quad Yes/Positive & 189 (64.1\%) & 125 (65.8\%) \\
\quad No/Negative & 106 (35.9\%) & 65 (34.2\%) \\
\hline
\end{tabular}
\end{table}

\subsection{Main Performance Results}

\subsubsection{Source and Target Domain Performance}
While Figure~\ref{fig:performance-heatmaps} visualizes the relative performance trends across methods, Table~\ref{tab:main_results_table} provides the precise numerical metrics for detailed comparison. In source-domain evaluation, PANDA achieved an \auc of 0.829, significantly outperforming Random Forest (0.752) and clinical scores (Mayo \auc 0.605).

On the external target domain, the benefits of adaptation became clear. The TCA-enhanced PANDA model reached the highest \auc of 0.705 and \recall of 0.944. In contrast, Random Forest dropped to 0.632, and SVM to 0.628, indicating severe degradation due to domain shift. The clinical scores (Mayo, PKUPH) performed poorly (\auc $<$ 0.64), likely due to population differences between their original derivation cohorts and our Chinese hospital data.

Figure~\ref{fig:combined_analysis} complements these numbers with the ROC, calibration, and decision curves computed on both Cohort A and Cohort B, emphasizing how the TCA-aligned embeddings stabilize the model under cross-hospital deployment.

\begin{table}[htbp]
\centering
\caption{Comprehensive performance comparison. Source results are from 10-fold CV; Target results are from external validation on Cohort B. Best values are bolded.}
\label{tab:main_results_table}
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{\auc} & \textbf{\accuracy} & \textbf{\fonescore} & \textbf{\precision} & \textbf{\recall} \\
\midrule
\multicolumn{6}{l}{\textit{Source Domain (Internal CV)}} \\
\textbf{PANDA (TabPFN)} & \textbf{0.829} & \textbf{0.746} & \textbf{0.810} & \textbf{0.786} & 0.846 \\
Random Forest & 0.752 & 0.698 & 0.779 & 0.735 & 0.842 \\
XGBoost & 0.742 & 0.678 & 0.752 & 0.733 & 0.787 \\
LASSO LR & 0.763 & 0.722 & 0.810 & 0.723 & \textbf{0.925} \\
Mayo Score & 0.605 & 0.359 & 0.000 & 0.000 & 0.000 \\
\midrule
\multicolumn{6}{l}{\textit{Target Domain (External Validation)}} \\
\textbf{PANDA + TCA} & \textbf{0.705} & \textbf{0.705} & \textbf{0.808} & 0.707 & \textbf{0.944} \\
PANDA (No UDA) & 0.698 & 0.663 & 0.776 & 0.689 & 0.888 \\
Random Forest & 0.632 & 0.679 & 0.775 & 0.713 & 0.854 \\
SVM & 0.628 & 0.568 & 0.647 & 0.695 & 0.606 \\
PKUPH Score & 0.636 & 0.695 & 0.784 & \textbf{0.733} & 0.847 \\
Mayo Score & 0.584 & 0.342 & 0.000 & 0.000 & 0.000 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{img/cross_hospital/combined_heatmaps_nature.pdf}
    \caption{\textbf{Performance comparison heatmaps.} \textbf{a} Source domain 10-fold CV. \textbf{b} Cross-domain external validation.}
    \label{fig:performance-heatmaps}
\end{figure}

\subsubsection{Stratified Analysis}
To investigate potential biases, we evaluated PANDA's performance across key subgroups (Table~\ref{tab:stratified_analysis}).
\begin{itemize}
    \item \textbf{Nodule Size}: Performance is robust for large nodules ($>$8mm, \auc 0.74) but drops for sub-centimeter nodules (\auc 0.65), reflecting the inherent difficulty in radiological characterization of small lesions.
    \item \textbf{Smoking Status}: The model performs better in smokers (\auc 0.72) than non-smokers (\auc 0.68), likely because smoking provides a strong prior for malignancy that the model leverages.
    \item \textbf{Gender}: We observed consistent performance across gender (\auc 0.70 vs 0.71), suggesting no significant gender bias.
\end{itemize}

\begin{table}[htbp]
\centering
\caption{Stratified performance of PANDA+TCA on the target cohort.}
\label{tab:stratified_analysis}
\begin{tabular}{lccc}
\toprule
\textbf{Subgroup} & \textbf{n} & \textbf{\auc} & \textbf{Sensitivity} \\
\midrule
\textbf{Nodule Size} & & & \\
$\le 8$ mm & 72 & 0.65 & 0.88 \\
$> 8$ mm & 118 & 0.74 & 0.96 \\
\midrule
\textbf{Smoking History} & & & \\
Never Smoker & 105 & 0.68 & 0.92 \\
Current/Former & 85 & 0.72 & 0.97 \\
\midrule
\textbf{Gender} & & & \\
Male & 110 & 0.71 & 0.95 \\
Female & 80 & 0.70 & 0.93 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Additional Cross-Domain Validation on TableShift}

We further validated PANDA on the TableShift BRFSS Diabetes benchmark (White $\rightarrow$ Non-White race shift).

\begin{table}[htbp]
\centering
\caption{TableShift BRFSS Diabetes Results (Race Shift). OOD = Out of Distribution (Non-White).}
\label{tab:tableshift_results}
\footnotesize
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{ID \auc} & \textbf{OOD \auc} & \textbf{OOD Acc} & \textbf{Gap} \\
\midrule
\textbf{PANDA + TCA} & 0.809 & \textbf{0.804} & \textbf{0.848} & \textbf{-0.005} \\
PANDA (No UDA) & 0.809 & 0.796 & 0.847 & -0.013 \\
Random Forest & -- & 0.787 & 0.844 & -- \\
Gradient Boosting (GBDT) & 0.815 & 0.783 & 0.834 & -0.032 \\
XGBoost & -- & 0.770 & 0.827 & -- \\
SVM & -- & 0.645 & 0.840 & -- \\
Decision Tree & 0.680 & 0.566 & 0.775 & -0.114 \\
\bottomrule
\end{tabular}
\end{table}

As shown in Table~\ref{tab:tableshift_results}, PANDA+TCA maintained an \auc of 0.804 on the OOD target, showing almost zero degradation from the ID source (0.809). In contrast, XGBoost dropped from 0.815 to 0.783.

Figure~\ref{fig:tableshift-heatmaps} visualizes the heatmap comparison between ID and OOD metrics, reinforcing how PANDA+TCA keeps the intensities stable under the race shift.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{img/tableshift/combined_heatmaps_nature.pdf}
    \caption{\textbf{TableShift heatmaps.} Comparison of ID vs OOD performance metrics across baselines, highlighting PANDA+TCA's stability under race shift.}
    \label{fig:tableshift-heatmaps}
\end{figure}

\textbf{Discussion on Precision/Recall}:
Readers may notice low $\fonescore$s in BRFSS despite high $\accuracy$ (Fig.~\ref{fig:brfss-roc}). This is an artifact of the low $\prevalence$ (17.4\%) and the default 0.5 threshold. The model correctly identifies most negatives (high $\accuracy$) but, without class re-weighting, yields moderate $\precision$ on the minority positive class. For screening purposes, the high $\auc$ (0.804) confirms the model's discriminative power; the operating point can be adjusted via threshold tuning to prioritize $\recall$.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{img/tableshift/combined_analysis_figure.pdf}
    \caption{\textbf{TableShift BRFSS Diabetes analysis.} \textbf{a,b} ROC curves showing PANDA's robustness. \textbf{c,d} Calibration curves. \textbf{e,f} Decision curves.}
\label{fig:brfss-roc}
\end{figure}

\subsection{Interpretability and Stability}
Recursive Feature Elimination (RFE) identified a stable subset of 8 features (Age, Spiculation, etc.) that maximized the Cost-Effectiveness Index (Fig.~\ref{fig:rfe-performance}). This "best8" set performed within 1\% of the full 63-feature set but with significantly better cross-center stability.
Figure~\ref{fig:feature_selection_uda} illustrates how cross-domain feature selection interacts with UDA to keep the pooled covariance low, making the latent alignment problem easier for TCA.

% New figure for feature selection and UDA interaction
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{img/cross_hospital/Feature Selection and UDA.pdf}
    \caption{\textbf{Feature selection meets UDA.} Selecting features with low pooled variance (left column) reduces the domain gap seen by TCA (right column), keeping cross-domain covariance traces small.}
    \label{fig:feature_selection_uda}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{img/cross_hospital/feature_performance_comparison_comprehensive.pdf}
    \caption{RFE performance analysis. \textbf{a} \auc plateaus around 8-10 features. \textbf{b} Stability improves with smaller subsets.}
    \label{fig:rfe-performance}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{img/cross_hospital/combined_analysis_figure.pdf}
    \caption{\textbf{Cross-hospital pulmonary nodule analysis.} \textbf{a,b} ROC curves. \textbf{c,d} Calibration plots. \textbf{e,f} Decision curves.}
    \label{fig:combined_analysis}
\end{figure}

\label{sec:eval-end}
