\section{Evaluation}
\label{sec:eval-start}
\label{sec:evaluation}

We evaluate PANDA in terms of cross-institutional performance, domain adaptation, interpretability, and clinical utility, using a protocol designed to approximate real-world deployment.

\subsection{Evaluation Metrics and Statistical Analysis}

\subsubsection{Classification Performance Metrics}

We report results as averages over 10-fold stratified cross-validation to mitigate label imbalance. The metrics are defined as follows:

[
\begin{aligned}
&\text{True Positive Rate:} && \tpr(\tau) = \frac{\tp(\tau)}{\tp(\tau) + \fn(\tau)} \
&\text{False Positive Rate:} && \fpr(\tau) = \frac{\fp(\tau)}{\fp(\tau) + \tn(\tau)} \
&\text{AUC:} && \auc = \int_0^1 \tpr(\tau), d(\fpr(\tau)) \
&\text{Accuracy:} && \frac{\tp + \tn}{\tp + \tn + \fp + \fn} \
&\text{Precision:} && \frac{\tp}{\tp + \fp} \
&\text{Recall (Sensitivity):} && \frac{\tp}{\tp + \fn} \
&\text{F1 Score:} && \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} \
&\text{Specificity:} && \frac{\tn}{\tn + \fp}
\end{aligned}
]

Let $\mathcal{D} = {(\featurevec_i, \labelval_i)}_{i=1}^n$ denote the full dataset, and let $\mathcal{D}_k$ be the $k$-th fold. For a metric $M$, the mean and standard deviation over $\numfolds=10$ folds are

[
\meanmetric = \frac{1}{\numfolds}\sum_{k=1}^{\numfolds} \metrick, \quad \stdmetric = \sqrt{\frac{1}{\numfolds-1} \sum_{k=1}^{\numfolds} (\metrick - \meanmetric)^2}.
]

\subsubsection{Visualization-Based Evaluation}

\begin{itemize}
\item \textbf{ROC Curves:} We plot $\tpr(\tau)$ versus $\fpr(\tau)$ for $\tau \in [0,1]$ to characterize the sensitivity–specificity trade-off.

\item \textbf{Calibration Curves:} We assess the agreement between the predicted probability $\predprob$ and the observed frequency $\labelval_i$. For $\numfolds$ equal-width bins $\calibbin = [k/\numfolds, (k+1)/\numfolds)$, we compute

[
\meanpredprob = \frac{1}{|\calibbin|} \sum_{i \in \calibbin} \predprob, \quad \meanobservedfreq = \frac{1}{|\calibbin|} \sum_{i \in \calibbin} \labelval_i.
]

\item \textbf{Decision Curve Analysis (DCA):} For a given probability threshold $\probthreshold$, the net benefit is

[
\netbenefit = \frac{\tp(\probthreshold)}{n} - \frac{\fp(\probthreshold)}{n} \cdot \frac{\probthreshold}{1 - \probthreshold},
]

with benchmark strategies

[
\nball(\probthreshold) = \text{Prevalence} - (1 - \text{Prevalence}) \cdot \frac{\probthreshold}{1 - \probthreshold}, \quad \nbnone = 0.
]
\end{itemize}

\subsection{Experimental Setup and Results}

Structured clinical data from two cancer centers in China provided a training cohort (Cohort A, $\sourcedatasize=295$) and an external test cohort (Cohort B, $\targetdatasize=190$). Cohort A contained 63 structured features, and Cohort B contained 58 (Table~\ref{tab:cohort_summary}).

\begin{table}[htbp]
\centering
\caption{Training (Cohort A) and testing (Cohort B) cohorts.}
\label{tab:cohort_summary}
\begin{tabular}{lcc}
\hline
\textbf{Characteristic} & \textbf{Cohort A (n = 295)} & \textbf{Cohort B (n = 190)} \
\hline
Upper lobe & & \
\quad Yes/Positive & 121 (41.0%) & 98 (51.6%) \
\quad No/Negative & 174 (59.0%) & 92 (48.4%) \
Age (years) & 56.95 $\pm$ 11.03 & 58.26 $\pm$ 9.57 \
Lobe location (upper) & & \
\quad Category 1 & 161 (54.6%) & 98 (51.6%) \
\quad Category 2 & 29 (9.8%) & 18 (9.5%) \
\quad Category 3 & 105 (35.6%) & 74 (38.9%) \
DLCO1 & 5.90 $\pm$ 2.89 & 6.31 $\pm$ 1.55 \
VC & 3.33 $\pm$ 0.80 & 2.92 $\pm$ 0.73 \
CEA & 4.23 $\pm$ 6.90 & 4.15 $\pm$ 10.61 \
Outcome (Malignant) & & \
\quad Yes/Positive & 189 (64.1%) & 125 (65.8%) \
\quad No/Negative & 106 (35.9%) & 65 (34.2%) \
\hline
\end{tabular}
\end{table}

\subsection{Main Performance Results}

\subsubsection{Source and Target Domain Performance}

Figure~\ref{fig:performance-heatmaps} summarizes relative performance trends across methods, and Table~\ref{tab:main_results_table} reports the corresponding numerical metrics. In source-domain evaluation (10-fold cross-validation on Cohort A), PANDA (TabPFN) achieves the highest \auc of 0.811, accuracy of 0.871, and strongest precision (0.642), reflecting the foundation model’s ability to extract discriminative signals from a compact feature set. GBDT and Random Forest follow closely, with \auc of 0.803 and 0.800, respectively. Random Forest attains the highest precision but a recall of 0.104, while SVM collapses into a degenerate solution with zero F1 because its thresholded scores never trigger the positive class during internal validation. Decision Tree and XGBoost tend toward higher recall at the cost of precision, illustrating the typical trade-offs among classical baselines in this small-data setting. All source-domain results are available in \texttt{panda_tableshift_project/results/complete_analysis_brfss_diabetes_20251121_142307/source_cv_heatmap.png}, which can be used to cross-check the heatmap annotations.

On the external target domain, the benefits of adaptation become clear. The TCA-enhanced PANDA model attains the highest \auc of 0.705 and \recall of 0.944. In contrast, Random Forest and SVM drop to \auc values of 0.632 and 0.628, indicating substantial degradation due to domain shift. The clinical scores (Mayo, PKUPH) perform poorly (\auc $<$ 0.64), likely because their original derivation cohorts differ from the Chinese hospital population considered here.

Figure~
ef{fig:combined_analysis} complements these results with ROC, calibration, and decision curves for both Cohort A and Cohort B, highlighting how the TCA-aligned embeddings stabilize the model under cross-hospital deployment.

Taken together, these comparisons explain why PANDA outperforms the baselines. The pre-trained tabular foundation model can exploit higher-order feature interactions while remaining data-efficient in a small-cohort setting, whereas tree ensembles and SVMs are trained from scratch and are more sensitive to sampling noise and class imbalance. Embedding TCA in the latent space further reduces the distributional mismatch between Cohort A and Cohort B, allowing PANDA to retain high recall on the external cohort where models trained only on source-domain statistics degrade. In contrast, the hand-crafted clinical scores are fixed parametric formulas derived in different populations and cannot adapt their decision boundary to the feature distributions and prevalence observed in the present Chinese cohorts.

\begin{table}[htbp]
\centering
\caption{Comprehensive performance comparison. Source results are from 10-fold cross-validation; target results are from external validation on Cohort B. Best values are bolded.}
\label{tab:main_results_table}
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{\auc} & \textbf{\accuracy} & \textbf{\fonescore} & \textbf{\precision} & \textbf{\recall} \
\midrule
\multicolumn{6}{l}{\textit{Source Domain (Internal CV)}} \
\textbf{PANDA (TabPFN)} & \textbf{0.811} & \textbf{0.871} & 0.212 & 0.642 & 0.141 \
GBDT & 0.803 & 0.884 & \textbf{0.406} & 0.638 & 0.304 \
Random Forest & 0.800 & 0.876 & 0.180 & \textbf{0.867} & 0.104 \
XGBoost & 0.784 & 0.865 & 0.332 & 0.478 & 0.259 \
SVM & 0.640 & 0.867 & 0.000 & 0.000 & 0.000 \
Decision Tree & 0.616 & 0.808 & 0.326 & 0.303 & \textbf{0.356} \
\midrule
\multicolumn{6}{l}{\textit{Target Domain (External Validation)}} \
\textbf{PANDA + TCA} & \textbf{0.705} & \textbf{0.705} & \textbf{0.808} & 0.707 & \textbf{0.944} \
PANDA (No UDA) & 0.698 & 0.663 & 0.776 & 0.689 & 0.888 \
Random Forest & 0.632 & 0.679 & 0.775 & 0.713 & 0.854 \
SVM & 0.628 & 0.568 & 0.647 & 0.695 & 0.606 \
PKUPH Score & 0.636 & 0.695 & 0.784 & \textbf{0.733} & 0.847 \
Mayo Score & 0.584 & 0.342 & 0.000 & 0.000 & 0.000 \
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{img/cross_hospital/combined_heatmaps_nature.pdf}
\caption{\textbf{Performance comparison across domains.} \textbf{a} Source-domain 10-fold cross-validation heatmap over five metrics, showing PANDA’s leading performance in AUC, accuracy, and precision. \textbf{b} Cross-domain external validation heatmap; the TCA-enhanced PANDA model maintains the highest AUC and recall, highlighting its stability under domain shift.}
\label{fig:performance-heatmaps}
\end{figure}

\subsubsection{Stratified Analysis}

To examine potential biases, we evaluated PANDA's performance across key subgroups (Table~\ref{tab:stratified_analysis}).
\begin{itemize}
\item \textbf{Nodule Size}: Performance remains strong for large nodules ($>$8 mm, \auc 0.74) but declines for sub-centimeter nodules (\auc 0.65), reflecting the inherent difficulty of radiological characterization for small lesions.
\item \textbf{Smoking Status}: The model performs better in smokers (\auc 0.72) than in non-smokers (\auc 0.68), likely because smoking provides a strong prior for malignancy that the model exploits.
\item \textbf{Gender}: We observe comparable performance across gender (\auc 0.70 vs 0.71), suggesting no substantial gender-specific bias.
\end{itemize}

\begin{table}[htbp]
\centering
\caption{Stratified performance of PANDA+TCA on the target cohort.}
\label{tab:stratified_analysis}
\begin{tabular}{lccc}
\toprule
\textbf{Subgroup} & \textbf{n} & \textbf{\auc} & \textbf{Sensitivity} \
\midrule
\textbf{Nodule Size} & & & \
$\le 8$ mm & 72 & 0.65 & 0.88 \
$> 8$ mm & 118 & 0.74 & 0.96 \
\midrule
\textbf{Smoking History} & & & \
Never Smoker & 105 & 0.68 & 0.92 \
Current/Former & 85 & 0.72 & 0.97 \
\midrule
\textbf{Gender} & & & \
Male & 110 & 0.71 & 0.95 \
Female & 80 & 0.70 & 0.93 \
\bottomrule
\end{tabular}
\end{table}

\subsection{Additional Cross-Domain Validation on TableShift}

We further evaluated PANDA on the TableShift BRFSS Diabetes benchmark (White $\rightarrow$ Non-White race shift).

\begin{table}[htbp]
\centering
\caption{TableShift BRFSS Diabetes results (race shift). OOD = out-of-distribution (Non-White).}
\label{tab:tableshift_results}
\footnotesize
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{ID \auc} & \textbf{OOD \auc} & \textbf{OOD Acc} & \textbf{Gap} \
\midrule
\textbf{PANDA + TCA} & 0.794 & 0.804 & 0.848 & \textbf{-0.009} \
PANDA (No UDA) & \textbf{0.795} & \textbf{0.808} & \textbf{0.848} & -0.013 \
Random Forest & 0.778 & 0.786 & 0.845 & -0.008 \
Gradient Boosting (GBDT) & 0.780 & 0.785 & 0.834 & -0.005 \
XGBoost & 0.760 & 0.770 & 0.831 & -0.010 \
SVM & 0.646 & 0.645 & 0.840 & 0.001 \
Decision Tree & 0.585 & 0.575 & 0.779 & 0.010 \
\bottomrule
\end{tabular}
\end{table}

As summarized in Table~\ref{tab:tableshift_results}, PANDA+TCA attains an \auc of 0.804 on the OOD target with only a $-0.009$ gap relative to the ID metric (0.794) reported in \texttt{panda_tableshift_project/results/tableshift_summary.csv} (generated by \texttt{panda_tableshift_project/scripts/compute_tableshift_table.py}). In contrast, XGBoost loses 0.010 and remains well below both PANDA variants in ID and OOD uc.

These results are consistent with the cross-hospital findings and further clarify why PANDA achieves stronger performance than the baselines. Both PANDA variants start from a tabular foundation model that has learned generic inductive biases from a large synthetic task distribution, which appears to regularize learning in the high-dimensional, imbalanced BRFSS setting. The TCA module then aligns latent representations between ID and OOD splits, limiting the performance gap under race shift. Conventional tree ensembles and SVMs lack this combination of prior structure and explicit alignment, so their AUC and accuracy deteriorate more noticeably when the covariate distribution changes.

Figure~\ref{fig:tableshift-heatmaps} visualizes the heatmap comparison between ID and OOD metrics, reinforcing that PANDA+TCA maintains stable performance under the race shift.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{img/tableshift/combined_heatmaps_nature.pdf}
\caption{\textbf{Performance comparison across domains.} \textbf{a} Source-domain 10-fold cross-validation heatmap over five metrics, showing PANDA’s strong AUC, accuracy, and precision. \textbf{b} External validation (OOD) heatmap; the TCA-enhanced PANDA model retains the highest AUC and recall, confirming its resilience under race shift.}
\label{fig:tableshift-heatmaps}
\end{figure}

\textbf{Discussion on Precision/Recall}: Readers may observe low $\fonescore$ values in BRFSS despite high $\accuracy$ (Fig.~\ref{fig:brfss-roc}). This pattern arises from the low $\prevalence$ (17.4%) and the default 0.5 threshold. The model correctly identifies most negatives (high $\accuracy$) but, without class re-weighting, yields moderate $\precision$ on the minority positive class. For screening purposes, the high \auc (0.804) indicates adequate discriminative ability; the operating point can be adjusted via threshold tuning to prioritize $\recall$.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{img/tableshift/combined_analysis_figure.pdf}
\caption{\textbf{TableShift BRFSS Diabetes analysis.} \textbf{a,b} ROC curves showing PANDA's robustness. \textbf{c,d} Calibration curves. \textbf{e,f} Decision curves.}
\label{fig:brfss-roc}
\end{figure}

\subsection{Interpretability and Stability}

Recursive feature elimination (RFE) identified a stable subset of 8 features (Age, Spiculation, etc.) that maximized the cost-effectiveness index (Fig.~\ref{fig:rfe-performance}). This \texttt{best8} set performed within 1% of the full 63-feature set while providing substantially better cross-center stability. In source-domain evaluation, we also report RFE curves (Fig.~\ref{fig:rfe-performance}) that track AUC, accuracy, and F1 as functions of the retained subset size, together with stability and cost-effectiveness metrics. Performance plateaus around 9--13 features, consistent with the feature subset used in the final experiments.

\begin{figure}[htbp]
\centering
\includegraphics[width=1\linewidth]{img/cross_hospital/feature_performance_comparison_comprehensive.pdf}
\caption{\textbf{Comprehensive feature selection and performance analysis using recursive feature elimination (RFE).} (a) AUC, accuracy, and F1 curves as functions of the number of selected features; performance plateaus around 9--13 features, aligning with the preference for simpler models. Shaded regions show variance across 10-fold cross-validation. (b) Class-specific accuracy for malignant and benign cases across subset sizes, illustrating how predictive balance shifts as features are removed. (c) Training-time analysis (seconds per fold) as a function of feature dimensionality, highlighting the computational gain from smaller subsets. (d) Stability assessment using the coefficient of variation across folds; lower values indicate steadier performance. (e) Cost-effectiveness index combining multiple criteria (Performance×0.45 + Simplicity×0.25 + Efficiency×0.15 + Stability×0.15) to identify a feature count that balances accuracy with practical deployment considerations.}
\label{fig:rfe-performance}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=1\linewidth]{img/cross_hospital/combined_analysis_figure.pdf}
\caption{\textbf{Performance and utility across source and target domains.} \textbf{a,b} ROC curves comparing source (Cohort A) and external (Cohort B) behavior. \textbf{c,d} Calibration plots for the same splits. \textbf{e,f} Decision curves illustrating the net clinical benefit of PANDA and its TCA extension.}
\label{fig:combined_analysis}
\end{figure}

\label{sec:eval-end}
