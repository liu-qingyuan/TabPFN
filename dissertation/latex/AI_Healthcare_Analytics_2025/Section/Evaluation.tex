\section{Evaluation}
\label{sec:evaluation}

We assess PANDA across cross-institutional performance, domain adaptation, interpretability, and clinical utility, using a protocol meant to resemble what deployment would actually look like.

\subsection{Evaluation Metrics and Statistical Analysis}

\subsubsection{Classification Performance Metrics}

Results are averaged over 10-fold stratified cross-validation to temper label imbalance. The metrics are:

\[
\begin{aligned}
&\text{True Positive Rate:} && TPR(\tau) = \frac{TP(\tau)}{TP(\tau) + FN(\tau)} \\
&\text{False Positive Rate:} && FPR(\tau) = \frac{FP(\tau)}{FP(\tau) + TN(\tau)} \\
&\text{AUC:} && AUC = \int_0^1 TPR(\tau)\, d(FPR(\tau)) \\
&\text{Accuracy:} && \frac{TP + TN}{TP + TN + FP + FN} \\
&\text{Precision:} && \frac{TP}{TP + FP} \\
&\text{Recall (Sensitivity):} && \frac{TP}{TP + FN} \\
&\text{F1 Score:} && \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2TP}{2TP + FP + FN} \\
&\text{Specificity:} && \frac{TN}{TN + FP}
\end{aligned}
\]

Let $\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^n$ denote the full dataset, and $\mathcal{D}_k$ be the $k$-th fold. For metric $M$, the mean and standard deviation over $K=10$ folds are:

\[
\bar{M} = \frac{1}{K}\sum_{k=1}^K M_k, \quad \sigma_M = \sqrt{\frac{1}{K-1} \sum_{k=1}^K (M_k - \bar{M})^2}
\]


\subsubsection{Visualization-Based Evaluation}

\begin{itemize}
\item \textbf{ROC Curves:} Plot $TPR(\tau)$ versus $FPR(\tau)$ for $\tau \in [0,1]$ to see the sensitivity-specificity trade-off. 

\item \textbf{Calibration Curves:} Check agreement between predicted probability $\hat{p}_i$ and observed frequency $y_i$. For $K$ equal-width bins $B_k = [k/K, (k+1)/K)$:

\[
\bar{p}_k = \frac{1}{|B_k|} \sum_{i \in B_k} \hat{p}_i, \quad \bar{y}_k = \frac{1}{|B_k|} \sum_{i \in B_k} y_i
\]

\item \textbf{Decision Curve Analysis (DCA):}

\[
NB(p_t) = \frac{TP(p_t)}{n} - \frac{FP(p_t)}{n} \cdot \frac{p_t}{1 - p_t}
\]

With benchmark strategies:

\[
NB_{all}(p_t) = \text{Prevalence} - (1 - \text{Prevalence}) \cdot \frac{p_t}{1 - p_t}, \quad NB_{none} = 0
\]

where $\text{Prevalence} = \frac{1}{n} \sum_{i=1}^n y_i$
\end{itemize}

\subsection{Experimental Setup and Results}

Structured clinical data from two cancer centers in China provided a training cohort (Cohort A, $n_s=295$) and an external test cohort (Cohort B, $n_t=190$). Cohort A contained 63 structured features; Cohort B contained 58 (Table~\ref{tab:cohort_summary}).

\begin{table}[htbp]
\centering
\caption{The training (Cohort A) and testing (Cohort B) cohorts.}
\label{tab:cohort_summary}
\begin{tabular}{lcc}
\hline
\textbf{Characteristic} & \textbf{Cohort A (n = 295)} & \textbf{Cohort B (n = 190)} \\
\hline
Upper lobe & & \\
\quad Yes/Positive & 121 (41.0\%) & 98 (51.6\%) \\
\quad No/Negative & 174 (59.0\%) & 92 (48.4\%) \\
Age (years) & 56.95 $\pm$ 11.03 & 58.26 $\pm$ 9.57 \\
Lobe location (upper) & & \\
\quad Category 1 & 161 (54.6\%) & 98 (51.6\%) \\
\quad Category 2 & 29 (9.8\%) & 18 (9.5\%) \\
\quad Category 3 & 105 (35.6\%) & 74 (38.9\%) \\
DLCO1 & 5.90 $\pm$ 2.89 & 6.31 $\pm$ 1.55 \\
VC & 3.33 $\pm$ 0.80 & 2.92 $\pm$ 0.73 \\
CEA & 4.23 $\pm$ 6.90 & 4.15 $\pm$ 10.61 \\
CRE & 73.41 $\pm$ 17.16 & 62.94 $\pm$ 13.64 \\
NSE & 13.07 $\pm$ 3.90 & 13.82 $\pm$ 4.36 \\
Outcome (Malignant) & & \\
\quad Yes/Positive & 189 (64.1\%) & 125 (65.8\%) \\
\quad No/Negative & 106 (35.9\%) & 65 (34.2\%) \\
\hline
\end{tabular}
\end{table}

In source-domain evaluation (10-fold cross-validation on Cohort A), PANDA led on all metrics (Fig.~\ref{fig:performance-heatmaps}): AUC 0.829, accuracy 0.746, F1-score 0.810, precision 0.786, recall 0.846. The high recall is what screening workflows tend to care about. Classical machine learning methods were moderate (Random Forest AUC 0.752; XGBoost 0.742), and clinical scores fared poorly.

For external validation (train on Cohort A, test on Cohort B), the TCA-enhanced PANDA model again came out ahead (AUC 0.705, F1-score 0.808, recall 0.944), with the non-adaptive version slightly behind at AUC 0.698. Among baselines, LASSO LR reached AUC 0.668 with recall 0.943; Random Forest dropped to 0.632; SVM, GBDT, and XGBoost fell below 0.59, underscoring shift sensitivity.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{img/cross_hospital/combined_heatmaps_nature.pdf}
    \caption{\textbf{Performance comparison across source and target domains.}
    \textbf{a} Source domain 10-fold cross-validation performance heatmap across five classification metrics. The PANDA framework achieves the best overall performance across all metrics. \textbf{b} Cross-domain performance heatmap on the external validation set. The TCA-enhanced PANDA model shows the highest AUC and recall, indicating improved generalization under domain shift.}
    \label{fig:performance-heatmaps}
\end{figure}

Feature-space checks (Fig.~\ref{fig:tca-visualization}) suggest TCA is doing its job: PCA and t-SNE views tighten the alignment between source and target after transformation, even if some scatter remains.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{img/cross_hospital/TCA_dimensionality_reduction.pdf}
    \caption{TCA-based domain adaptation visualization. \textbf{a,b} PCA visualization before and after TCA transformation, showing improved alignment of target samples with source samples. \textbf{c,d} t-SNE visualization before and after TCA transformation, demonstrating enhanced cluster center alignment and distribution consistency.}
    \label{fig:tca-visualization}
\end{figure}

\subsection{Additional Cross-Domain Validation on a Public Benchmark}

We added a public benchmark to stress-test PANDA under a race shift: the TableShift BRFSS
Diabetes task, which trains on primarily White respondents and evaluates on non-White
respondents. Table~\ref{tab:brfss_cohort_summary} summarizes the full cohorts before subsampling,
highlighting the massive data scale, the imbalance in diabetes prevalence (12.5\% vs.\ 17.4\%), and
the precise definition of the domain variable (PRACE1). For reproducible experiments we draw
$n_{\text{train}}=1024$ and $n_{\text{test}}=2048$ records while keeping the same label distribution, mirroring
the small-sample constraints studied in the hospital setting.

\begin{table}[htbp]
\centering
\caption{Source (ID) vs Target (OOD) cohorts for BRFSS Diabetes (race shift).}
\label{tab:brfss_cohort_summary}
\begin{tabular}{lcc}
\hline
\textbf{Characteristic} & \textbf{Source / ID (PRACE1=1)} & \textbf{Target / OOD (PRACE1 in $\{2..6\}$)} \\\hline
Sample size (full split) & 969{,}229 (train) + 121{,}154 (val) + 121{,}154 (test) & 23{,}264 (val) + 209{,}375 (test) \\
Diabetes positive rate & 12.5\% (train) & 17.4\% (ood\_test) \\
Years (top four) & 2015: 245{,}675; 2017: 244{,}996; 2019: 221{,}847; 2021: 223{,}088 & 2015: 49{,}216; 2017: 52{,}150; 2019: 48{,}012; 2021: 50{,}595 \\
Domain shift variable & PRACE1 = 1 (non-Hispanic White) & PRACE1 in $\{2,3,4,5,6\}$ (other races) \\
Label definition & \multicolumn{2}{c}{DIABETES coded 1 (Yes) vs 0 (No/Prediabetes/Borderline); NA rows dropped} \\
Feature summary & \multicolumn{2}{c}{142 numerical features; cross-year aligned; underscores removed; IYEAR retained} \\
Preprocessing notes & \multicolumn{2}{c}{DRNK\_PER\_WEEK=99900 dropped; SEX$\rightarrow\{0,1\}$; health-day 88$\rightarrow$0; TOLDHI/SMOKDAY2 fill NOTASKED\_MISSING} \\
Sampling for modeling & 1{,}024 sampled for training (seed 42) & 2{,}048 sampled for evaluation (seed 42) \\\hline
\end{tabular}
\end{table}

Because the evaluation uses the native 0.5 threshold on a 17\% positive cohort without class weighting,
the classifiers lean toward the majority (negative) class. As a result, the TableShift panels in
Fig.~\ref{fig:brfss-roc} and Fig.~\ref{fig:brfss-heatmap} show high accuracy yet visibly low precision/recall/F1:
the models rarely fire on positives, so recall collapses while precision spikes only on the few true
positives. We keep these raw values to reflect the default-deployment setting; more aggressive
threshold tuning or balanced loss functions could raise the recall if needed.

Across seven baselines and variants, PANDA with TCA achieved the highest OOD AUC (0.804) and
accuracy (0.848), edging out the non-adaptive TabPFN variant (AUC 0.796, accuracy 0.847) and
tree ensembles (best GBDT AUC 0.783). Using the source cross-validation tuning run
(AUC 0.809, accuracy 0.848) as a reference, the race-shift performance drop is mild
($\Delta$AUC $\approx$ 0.005; $\Delta$Accuracy $<0.001$), while adaptation provides a small but consistent
gain over PANDA\_NoUDA ($+0.008$ AUC). Classic SVM and decision trees degraded more sharply under
the race shift, reinforcing the need for explicit alignment even on large public datasets.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{img/tableshift/combined_analysis_figure.pdf}
    \caption{\textbf{TableShift BRFSS Diabetes (race shift: White $\rightarrow$ non-White).}
    \textbf{a,b} ROC curves for baselines and PANDA variants on the public benchmark.
    \textbf{c,d} Calibration curves highlighting probability alignment post-adaptation.
    \textbf{e,f} Decision curves illustrating net benefit across threshold ranges.}
    \label{fig:brfss-roc}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{img/tableshift/combined_heatmaps_nature.pdf}
    \caption{\textbf{Performance heatmaps for the TableShift BRFSS Diabetes task.}
    \textbf{a} Source cross-validation metrics.
    \textbf{b} Race-shift (White $\rightarrow$ non-White) OOD metrics.}
    \label{fig:brfss-heatmap}
\end{figure}


\subsection{Model Explainability, Reliability, and Clinical Utility}

RFE with the pre-trained model kept interpretation manageable, and performance across subset sizes leveled off around 9--13 features (Fig.~\ref{fig:rfe-performance}). In terms of reliability, the ROC curves give PANDA a clear edge--AUC 0.829 on the source cohort and 0.705 for the TCA-augmented model on the external one. Calibration plots also place PANDA closer to the diagonal, with TCA nudging the target-side curve a bit nearer to what we would hope for. Decision curves, which weigh net clinical benefit across thresholds, tilt in PANDA's favor as well, and the TCA variant adds a small but noticeable gain on the external cohort.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{img/cross_hospital/feature_performance_comparison_comprehensive.pdf}
    \caption{Comprehensive feature selection and performance analysis using recursive feature elimination (RFE). 
    \textbf{a} AUC, accuracy, and F1 curves as functions of the number of selected features. Performance plateaus around 9--13 features, aligning with the preference for simpler models. Shaded regions show variance across 10-fold cross-validation. 
    \textbf{b} Class-specific accuracy for malignant and benign cases across feature subset sizes, illustrating how predictive balance shifts as features are removed. 
    \textbf{c} Training-time analysis (seconds per fold) as a function of feature dimensionality, highlighting the computational gain from smaller subsets. 
    \textbf{d} Stability assessment using the coefficient of variation across folds; lower values indicate steadier performance. 
    \textbf{e} Cost-effectiveness index combining multiple criteria (Performance×0.45 + Simplicity×0.25 + Efficiency×0.15 + Stability×0.15) to identify a feature count that balances accuracy with practical deployment considerations.}
    \label{fig:rfe-performance}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{img/cross_hospital/combined_analysis_figure.pdf}
    \caption{\textbf{Cross-hospital pulmonary nodule task (Cohort A $\rightarrow$ Cohort B).}
    \textbf{a,b} ROC curves on source and target cohorts.
    \textbf{c,d} Calibration plots showing probability reliability after TCA.
    \textbf{e,f} Decision curves quantifying net benefit for internal versus external
    deployment.}
    \label{fig:combined_analysis}
\end{figure}
