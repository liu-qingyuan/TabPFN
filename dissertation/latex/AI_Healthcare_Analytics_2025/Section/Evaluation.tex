\section{Evaluation}
\label{sec:eval-start}
\label{sec:evaluation}

We evaluate PANDA from an artificial intelligence and deployment perspective,
focusing on its ability to (i) learn from small, heterogeneous tabular cohorts,
(ii) remain robust under cross-hospital and cross-population distribution shifts, and
(iii) provide interpretable, clinically useful predictions.
To this end, we adopt a unified evaluation protocol across two complementary
experiments: \textbf{Experiment 1 (E1)}, which targets cross-hospital pulmonary
nodule malignancy prediction, and \textbf{Experiment 2 (E2)}, which targets
race-driven cross-population shift in the TableShift BRFSS Diabetes benchmark.

\subsection{Evaluation Protocols for Cross-Domain Diagnostic Models}

We adopt a common set of discrimination, calibration, and clinical-utility metrics
to enable consistent comparison across hospitals and benchmarks.
Unless otherwise specified, all reported metrics are computed over 10-fold
stratified cross-validation on the source domain and once on the external or
out-of-distribution (OOD) domain.

\subsubsection{Classification Performance Metrics}

We report all results as averages over 10-fold stratified cross-validation to mitigate label imbalance. The metrics are defined as follows:

\begin{equation}
    \label{eq:tpr}
    \text{True Positive Rate:} \quad \tpr(\tau) = \frac{\tp(\tau)}{\tp(\tau) + \fn(\tau)}
\end{equation}

\begin{equation}
    \label{eq:fpr}
    \text{False Positive Rate:} \quad \fpr(\tau) = \frac{\fp(\tau)}{\fp(\tau) + \tn(\tau)}
\end{equation}

\begin{equation}
    \label{eq:auc}
    \text{AUC:} \quad \auc = \int_0^1 \tpr(\tau)\, d(\fpr(\tau))
\end{equation}

\begin{equation}
    \label{eq:accuracy}
    \text{Accuracy:} \quad \frac{\tp + \tn}{\tp + \tn + \fp + \fn}
\end{equation}

\begin{equation}
    \label{eq:precision}
    \text{Precision:} \quad \frac{\tp}{\tp + \fp}
\end{equation}

\begin{equation}
    \label{eq:recall}
    \text{Recall (Sensitivity):} \quad \frac{\tp}{\tp + \fn}
\end{equation}

\begin{equation}
    \label{eq:f1}
    \text{F1 Score:} \quad \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}

\begin{equation}
    \label{eq:specificity}
    \text{Specificity:} \quad \frac{\tn}{\tn + \fp}
\end{equation}

Let $\mathcal{D} = \{(\featurevec_i, \labelval_i)\}_{i=1}^n$ denote the full dataset, and let $\mathcal{D}_k$ be the $k$-th fold. For a metric $M$, the mean and standard deviation over $\numfolds=10$ folds are

\begin{equation}
    \label{eq:cross-validation-stats}
    \meanmetric = \frac{1}{\numfolds}\sum_{k=1}^{\numfolds} \metrick, \quad \stdmetric = \sqrt{\frac{1}{\numfolds-1} \sum_{k=1}^{\numfolds} (\metrick - \meanmetric)^2}.
\end{equation}

\subsubsection{Visualization-Based Evaluation}

\begin{itemize}
\item \textbf{ROC Curves:} We plot $\tpr(\tau)$ versus $\fpr(\tau)$ for $\tau \in [0,1]$ to characterize the trade-off between sensitivity and specificity across operating thresholds.

\item \textbf{Calibration Curves:} We assess the agreement between the predicted probability $\predprob$ and the observed frequency $\labelval_i$. For $\numfolds$ equal-width bins $\calibbin = [k/\numfolds, (k+1)/\numfolds)$, we compute
\begin{equation}
    \label{eq:calibration-bins}
    \meanpredprob = \frac{1}{|\calibbin|} \sum_{i \in \calibbin} \predprob, \quad \meanobservedfreq = \frac{1}{|\calibbin|} \sum_{i \in \calibbin} \labelval_i.
\end{equation}
These quantities summarize both over- and underestimation of risk across the probability range.

\item \textbf{Decision Curve Analysis (DCA):} For a given probability threshold $\probthreshold$, the net benefit is
\begin{equation}
    \label{eq:net-benefit}
    \netbenefit = \frac{\tp(\probthreshold)}{n} - \frac{\fp(\probthreshold)}{n} \cdot \frac{\probthreshold}{1 - \probthreshold},
\end{equation}
with benchmark strategies
\begin{equation}
    \label{eq:benchmark-strategies}
    \nball(\probthreshold) = \text{Prevalence} - (1 - \text{Prevalence}) \cdot \frac{\probthreshold}{1 - \probthreshold}, \quad \nbnone = 0.
\end{equation}
DCA therefore quantifies the clinical utility of a model across plausible decision thresholds relative to treating all or no patients.
\end{itemize}

\subsection{Experiment 1 (E1): Cross-Hospital Pulmonary Nodule Malignancy Prediction}

% --- 这一小段可作为"数据/设置/挑战"描述 ---
Experiment 1 evaluates PANDA in a realistic cross-hospital deployment scenario
for pulmonary nodule malignancy prediction.
Structured clinical data from two cancer centers in China provide a labeled
source cohort (Cohort A, $\sourcedatasize=295$) and an external target cohort
(Cohort B, $\targetdatasize=190$).
Cohort A contains 63 structured features and Cohort B contains 58, reflecting
differences in local documentation and biomarker panels
(Table~\ref{tab:cohort_summary}).
Both cohorts exhibit moderate class imbalance, with malignant nodules comprising
64.1\% of patients in Cohort A and 65.8\% in Cohort B, and key covariates such as
upper-lobe location, age, pulmonary function indices (DLCO, VC), and serum CEA
show distributional differences between the two hospitals.
These discrepancies capture the covariate shift that motivates domain adaptation
in this setting.

\paragraph{Prevalence and label shift.}
Real-world LDCT screening cohorts typically exhibit malignancy prevalences of approximately 5\%. In contrast, both of our internal pulmonary nodule cohorts are enriched case-control datasets with a higher malignancy proportion (around 65\%), constructed to ensure sufficient malignant samples for model development and cross-hospital comparison. Consequently, these cohorts do not constitute a canonical low-prevalence label-shift scenario. Instead, our pulmonary experiment focuses on covariate shift (e.g., shifts in age, smoking history, and acquisition protocols) and concept shift arising from differences in local labeling and management practices across hospitals.

\begin{table}[htbp]
\centering
\caption{Training (Cohort A) and testing (Cohort B) cohorts.}
\label{tab:cohort_summary}
\begin{tabular}{lcc}
\toprule
\textbf{Characteristic} & \textbf{Cohort A (n = 295)} & \textbf{Cohort B (n = 190)} \\
\midrule
Upper lobe & & \\
\quad Yes/Positive & 121 (41.0\%) & 98 (51.6\%) \\
\quad No/Negative & 174 (59.0\%) & 92 (48.4\%) \\
Age (years) & 56.95 $\pm$ 11.03 & 58.26 $\pm$ 9.57 \\
Lobe location (upper) & & \\
\quad Category 1 & 161 (54.6\%) & 98 (51.6\%) \\
\quad Category 2 & 29 (9.8\%) & 18 (9.5\%) \\
\quad Category 3 & 105 (35.6\%) & 74 (38.9\%) \\
DLCO1 & 5.90 $\pm$ 2.89 & 6.31 $\pm$ 1.55 \\
VC & 3.33 $\pm$ 0.80 & 2.92 $\pm$ 0.73 \\
CEA & 4.23 $\pm$ 6.90 & 4.15 $\pm$ 10.61 \\
Outcome (Malignant) & & \\
\quad Yes/Positive & 189 (64.1\%) & 125 (65.8\%) \\
\quad No/Negative & 106 (35.9\%) & 65 (34.2\%) \\
\bottomrule
\end{tabular}
\end{table}

% --- 把原来的第二个 subection 降一级 ---
\subsubsection{Internal and Cross-Hospital Generalization}

\subsubsection{Source and Target Domain Performance}

Figure~\ref{fig:performance-heatmaps} summarizes relative performance trends
across methods, and Table~\ref{tab:main_results_table} reports the corresponding
numerical metrics.
On the source domain (10-fold cross-validation on Cohort A), PANDA (TabPFN) achieves the
highest \auc of 0.8287, demonstrating the superior performance of the tabular
foundation model on small, heterogeneous medical datasets.
LASSO LR and Random Forest follow closely, with \auc values of 0.7631 and 0.7515,
respectively.
XGBoost and GBDT show competitive performance (\auc 0.7416 and 0.7212), while
traditional clinical scores (Mayo, PKUPH) and classical machine learning baselines
exhibit lower discrimination.
All source-domain experimental results are reproducible using the provided
codebase and configuration scripts.

On the external target domain (Cohort B), the benefits of unsupervised domain
adaptation become evident.
The TCA-enhanced PANDA model attains the highest \auc\ of 0.7046 and an
exceptionally high \recall\ of 0.9440, indicating strong sensitivity for clinical
screening applications.
PANDA\_NoUDA follows closely with \auc of 0.6980, demonstrating the inherent
transferability of the tabular foundation model.
In contrast, traditional machine learning methods experience substantial
performance drops under domain shift, with Random Forest declining to \auc 0.6324
and tree ensembles dropping below 0.600.
The clinical scores continue to show poor transportability across institutions.

\begin{table}[htbp]
\centering
\caption{Comprehensive performance comparison. Source results are from 10-fold cross-validation; target results are from external validation on Cohort B. Best values are bolded.}
\label{tab:main_results_table}
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{\auc} & \textbf{\accuracy} & \textbf{\fonescore} & \textbf{\precision} & \textbf{\recall} \\
\midrule
\multicolumn{6}{l}{\textit{Source Domain (Internal CV)}} \\
\textbf{PANDA (TabPFN)} & \textbf{0.8287} & 0.7460 & \textbf{0.8102} & 0.7864 & 0.8462 \\
LASSO LR & 0.7631 & 0.7224 & 0.8101 & 0.7227 & \textbf{0.9254} \\
Random Forest & 0.7515 & 0.6983 & 0.7792 & 0.7351 & 0.8415 \\
XGBoost & 0.7416 & 0.6778 & 0.7520 & 0.7325 & 0.7871 \\
GBDT & 0.7212 & 0.6911 & 0.7690 & 0.7394 & 0.8140 \\
SVM & 0.7175 & 0.6645 & 0.7197 & \textbf{0.7794} & 0.6769 \\
PKUPH & 0.6640 & \textbf{0.6782} & 0.7672 & 0.7148 & 0.8354 \\
Mayo & 0.6049 & 0.3592 & 0.0000 & 0.0000 & 0.0000 \\
Decision Tree & 0.5764 & 0.6099 & 0.6929 & 0.6997 & 0.6974 \\
\midrule
\multicolumn{6}{l}{\textit{Target Domain (External Validation)}} \\
\textbf{TCA} & \textbf{0.7046} & \textbf{0.7053} & \textbf{0.8082} & 0.7066 & \textbf{0.9440} \\
PANDA\_NoUDA & 0.6980 & 0.6632 & 0.7762 & 0.6894 & 0.8880 \\
Random Forest & 0.6324 & 0.6789 & 0.7753 & 0.7128 & 0.8538 \\
PKUPH & 0.6356 & 0.6947 & 0.7838 & \textbf{0.7329} & 0.8474 \\
LASSO LR & 0.6678 & 0.6737 & 0.7911 & 0.6825 & 0.9429 \\
SVM & 0.6285 & 0.5684 & 0.6468 & 0.6950 & 0.6064 \\
GBDT & 0.5906 & 0.5842 & 0.6834 & 0.6689 & 0.7109 \\
XGBoost & 0.5672 & 0.5947 & 0.6937 & 0.6701 & 0.7244 \\
Decision Tree & 0.5090 & 0.5684 & 0.6759 & 0.6650 & 0.6942 \\
Mayo & 0.5837 & 0.3421 & 0.0000 & 0.0000 & 0.0000 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{img/cross_hospital/combined_heatmaps_nature.pdf}
\caption{\textbf{Performance comparison across domains.} \textbf{a} Source-domain 10-fold cross-validation heatmap over five metrics, showing PANDA’s leading performance in AUC, accuracy, and precision. \textbf{b} Cross-domain external validation heatmap; the TCA-enhanced PANDA model maintains the highest AUC and recall, highlighting its stability under domain shift.}
\label{fig:performance-heatmaps}
\end{figure}

\subsubsection{Stratified Analysis}

To examine potential biases and subgroup robustness, we evaluated PANDA's performance across key subgroups (Table~\ref{tab:stratified_analysis}).

\begin{itemize}
\item \textbf{Nodule Size}: Performance remains strong for large nodules ($>8$ mm, \auc 0.74) but declines for sub-centimeter nodules (\auc 0.65), reflecting the inherent difficulty of radiological characterization for small lesions.
\item \textbf{Smoking Status}: The model performs better in smokers (\auc 0.72) than in non-smokers (\auc 0.68), likely because smoking provides a strong prior for malignancy that the model can exploit.
\item \textbf{Gender}: We observe comparable performance across gender (\auc 0.70 vs 0.71), suggesting no substantial gender-specific bias at the current sample size.
\end{itemize}

\begin{table}[htbp]
\centering
\caption{Stratified performance of PANDA+TCA on the target cohort.}
\label{tab:stratified_analysis}
\begin{tabular}{lccc}
\toprule
\textbf{Subgroup} & \textbf{n} & \textbf{\auc} & \textbf{Sensitivity} \\
\midrule
\textbf{Nodule Size} & & & \\
$\le 8$ mm & 72 & 0.65 & 0.88 \\
$> 8$ mm & 118 & 0.74 & 0.96 \\
\midrule
\textbf{Smoking History} & & & \\
Never Smoker & 105 & 0.68 & 0.92 \\
Current/Former & 85 & 0.72 & 0.97 \\
\midrule
\textbf{Gender} & & & \\
Male & 110 & 0.71 & 0.95 \\
Female & 80 & 0.70 & 0.93 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Experiment 2 (E2): Cross-Population Validation on TableShift}

We further evaluated PANDA on the TableShift BRFSS Diabetes benchmark, which introduces a race-driven shift by training on one demographic group and evaluating on another (White $\rightarrow$ Non-White). This benchmark provides a complementary stress test to the cross-hospital shift in the pulmonary nodule experiment and mimics realistic deployment across heterogeneous populations in population-health applications.

In this setting, our goal is to assess whether a tabular foundation model with lightweight domain alignment can remain stable when the covariate distribution changes along demographic axes. Qualitatively, PANDA (with and without TCA) achieves discrimination and calibration that are competitive with well-tuned tree ensembles, while avoiding the severe performance collapse that would be concerning in a cross-population deployment. This behavior supports the view that the inductive biases learned by the foundation model transfer beyond the pulmonary nodule task and remain useful under a distinct form of distribution shift.

\begin{table}[htbp]
\small
\centering
\caption{Source (ID) vs target (OOD) cohorts for the BRFSS Diabetes race shift.}
\label{tab:tableshift_cohort_summary}
\begin{tabular}{p{0.34\linewidth}p{0.28\linewidth}p{0.28\linewidth}}
\toprule
\textbf{Characteristic} & \textbf{Source / ID (PRACE1 = 1)} & \textbf{Target / OOD (PRACE1 $\in \{2,\dots,6\}$)} \\
\midrule
Sample size (full) & Train 969{,}229; Val 121{,}154; Test 121{,}154 & Val 23{,}264; Test 209{,}375 \\
Diabetes positive rate & 12.5\% (train) & 17.4\% (ood\_test) \\
Years & 2015: 245{,}675; 2016: 5{,}789; 2017: 244{,}996; 2018: 6{,}403; 2019: 221{,}847; 2020: 9{,}630; 2021: 223{,}088; 2022: 11{,}801 & 2015: 49{,}216; 2016: 1{,}507; 2017: 52{,}150; 2018: 1{,}424; 2019: 48{,}012; 2020: 3{,}147; 2021: 50{,}595; 2022: 3{,}324 \\
Domain shift variable & PRACE1 = 1 (non-Hispanic White) & PRACE1 $\in \{2,3,4,5,6\}$ (other races) \\
Modeling sample (seed 42) & 1{,}024 sampled for training & 2{,}048 sampled for evaluation \\
Sampled diabetes rate & 13.2\% & 17.3\% \\
Sampled pos / neg counts & 135 / 889 & 355 / 1{,}693 \\
Sampled years (seed 42) & 2015: 245; 2016: 8; 2017: 278; 2018: 2; 2019: 232; 2020: 7; 2021: 241; 2022: 11 & 2015: 497; 2016: 17; 2017: 488; 2018: 17; 2019: 445; 2020: 30; 2021: 525; 2022: 29 \\
\bottomrule
\end{tabular}
\end{table}

The BRFSS \texttt{DIABETES} label is coded 1 (Yes) versus 0 (No/Prediabetes/Borderline), with NA rows removed. After preprocessing, all 142 features are numerical, cross-year aligned, and retain the survey year (\texttt{IYEAR}). Preprocessing steps drop \texttt{DRNK\_PER\_WEEK=99900}, map \texttt{SEX$\rightarrow\{0,1\}$}, set health-day 88 to 0, and fill remaining non-asked entries with \texttt{NOTASKED\_MISSING}.

Figure~\ref{fig:tableshift-heatmaps} summarizes the behavior of different models across the training and race-shifted evaluation splits, and Figure~\ref{fig:brfss-roc} reports ROC, calibration, and decision curves. Together, these visualizations indicate that PANDA’s score distributions and decision boundaries can be adapted to new demographic groups without sacrificing overall screening utility.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{img/tableshift/combined_heatmaps_nature.pdf}
\caption{\textbf{Performance comparison on the TableShift BRFSS Diabetes benchmark.} Heatmaps summarize multiple metrics for PANDA and baseline models across the training split and the race-shifted evaluation split. PANDA with TCA remains competitive with strong tree ensembles and does not exhibit pronounced degradation under race shift.}
\label{fig:tableshift-heatmaps}
\end{figure}

\textbf{Discussion on Precision/Recall}: Readers may observe low $\fonescore$ values in BRFSS despite high $\accuracy$ (Fig.~\ref{fig:brfss-roc}). This pattern arises from the low positive-class prevalence $\pi = P(Y=1)$ (17.4\%) and the default 0.5 threshold: the model correctly identifies most negatives (high $\accuracy$) but, without class re-weighting, yields moderate $\precision$ on the minority positive class. For screening purposes, the ROC curves indicate adequate discriminative ability; the operating point can be adjusted via threshold tuning to prioritize $\recall$ when desired.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{img/tableshift/combined_analysis_figure.pdf}
\caption{\textbf{TableShift BRFSS Diabetes analysis.} \textbf{a,b} ROC curves showing PANDA's robustness under race-driven shift. \textbf{c,d} Calibration curves assessing probability estimates. \textbf{e,f} Decision curves illustrating net benefit across clinical thresholds.}
\label{fig:brfss-roc}
\end{figure}

\subsection{Interpretability and Stability}

Figure~\ref{fig:combined_analysis} compares ROC curves, calibration, and decision curves for PANDA and baseline models on the source and target hospitals. It shows that TCA-based alignment preserves screening-level sensitivity while improving net clinical benefit relative to traditional scores and tree ensembles. The same figure also demonstrates better calibration and higher net clinical benefit in decision-curve analysis, particularly in the clinically relevant risk range.

From an interpretability perspective, recursive feature elimination (RFE) identified a stable subset of eight features (Age, Spiculation, etc.) that maximized the cost-effectiveness index (Fig.~\ref{fig:rfe-performance}). This \texttt{best8} set performed within 1\% of the full 63-feature set while providing substantially greater cross-center stability. In source-domain evaluation, the RFE curves (Fig.~\ref{fig:rfe-performance}) track AUC, accuracy, and F1 as functions of the retained subset size, together with stability and cost-effectiveness metrics. Performance plateaus around 9--13 features, consistent with the subset used in the final experiments.

\begin{figure}[htbp]
\centering
\includegraphics[width=1\linewidth]{img/cross_hospital/feature_performance_comparison_comprehensive.pdf}
\caption{\textbf{Comprehensive feature selection and performance analysis using recursive feature elimination (RFE).} (a) AUC, accuracy, and F1 curves as functions of the number of selected features; performance plateaus around 9--13 features, aligning with the preference for simpler models. Shaded regions show variance across 10-fold cross-validation. (b) Class-specific accuracy for malignant and benign cases across subset sizes, illustrating how predictive balance shifts as features are removed. (c) Training-time analysis (seconds per fold) as a function of feature dimensionality, highlighting the computational gain from smaller subsets. (d) Stability assessment using the coefficient of variation across folds; lower values indicate steadier performance. (e) Cost-effectiveness index combining multiple criteria (Performance$\times$0.45 + Simplicity$\times$0.25 + Efficiency$\times$0.15 + Stability$\times$0.15) to identify a feature count that balances accuracy with practical deployment considerations.}
\label{fig:rfe-performance}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=1\linewidth]{img/cross_hospital/combined_analysis_figure.pdf}
\caption{\textbf{Performance and utility across source and target domains.} \textbf{a,b} ROC curves comparing source (Cohort A) and external (Cohort B) behavior. \textbf{c,d} Calibration plots for the same splits. \textbf{e,f} Decision curves illustrating the net clinical benefit of PANDA and its TCA extension.}
\label{fig:combined_analysis}
\end{figure}

\label{sec:eval-end}
