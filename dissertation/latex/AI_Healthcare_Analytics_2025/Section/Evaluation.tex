\section{Evaluation}
\label{sec:eval-start}
\label{sec:evaluation}

We assess PANDA across cross-institutional performance, domain adaptation, interpretability, and clinical utility, using a protocol meant to resemble what deployment would actually look like.

\subsection{Evaluation Metrics and Statistical Analysis}

\subsubsection{Classification Performance Metrics}

Results are averaged over 10-fold stratified cross-validation to temper label imbalance. The metrics are:

\[
\begin{aligned}
&\text{True Positive Rate:} && TPR(\tau) = \frac{TP(\tau)}{TP(\tau) + FN(\tau)} \\
&\text{False Positive Rate:} && FPR(\tau) = \frac{FP(\tau)}{FP(\tau) + TN(\tau)} \\
&\text{AUC:} && AUC = \int_0^1 TPR(\tau)\, d(FPR(\tau)) \\
&\text{Accuracy:} && \frac{TP + TN}{TP + TN + FP + FN} \\
&\text{Precision:} && \frac{TP}{TP + FP} \\
&\text{Recall (Sensitivity):} && \frac{TP}{TP + FN} \\
&\text{F1 Score:} && \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} \\
&\text{Specificity:} && \frac{TN}{TN + FP}
\end{aligned}
\]

Let $\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^n$ denote the full dataset, and $\mathcal{D}_k$ be the $k$-th fold. For metric $M$, the mean and standard deviation over $K=10$ folds are:

\[
\bar{M} = \frac{1}{K}\sum_{k=1}^K M_k, \quad \sigma_M = \sqrt{\frac{1}{K-1} \sum_{k=1}^K (M_k - \bar{M})^2}
\]

\subsubsection{Visualization-Based Evaluation}

\begin{itemize}
\item \textbf{ROC Curves:} Plot $TPR(\tau)$ versus $FPR(\tau)$ for $\tau \in [0,1]$ to see the sensitivity-specificity trade-off. 

\item \textbf{Calibration Curves:} Check agreement between predicted probability $\hat{p}_i$ and observed frequency $y_i$. For $K$ equal-width bins $B_k = [k/K, (k+1)/K)$:

\[
\bar{p}_k = \frac{1}{|B_k|} \sum_{i \in B_k} \hat{p}_i, \quad \bar{y}_k = \frac{1}{|B_k|} \sum_{i \in B_k} y_i
\]

\item \textbf{Decision Curve Analysis (DCA):}

\[
NB(p_t) = \frac{TP(p_t)}{n} - \frac{FP(p_t)}{n} \cdot \frac{p_t}{1 - p_t}
\]

With benchmark strategies:

\[
NB_{all}(p_t) = \text{Prevalence} - (1 - \text{Prevalence}) \cdot \frac{p_t}{1 - p_t}, \quad NB_{none} = 0
\]
\end{itemize}

\subsection{Experimental Setup and Results}

Structured clinical data from two cancer centers in China provided a training cohort (Cohort A, $n_s=295$) and an external test cohort (Cohort B, $n_t=190$). Cohort A contained 63 structured features; Cohort B contained 58 (Table~\ref{tab:cohort_summary}).

\begin{table}[htbp]
\centering
\caption{The training (Cohort A) and testing (Cohort B) cohorts.}
\label{tab:cohort_summary}
\begin{tabular}{lcc}
\hline
\textbf{Characteristic} & \textbf{Cohort A (n = 295)} & \textbf{Cohort B (n = 190)} \\
\hline
Upper lobe & & \\
\quad Yes/Positive & 121 (41.0\%) & 98 (51.6\%) \\
\quad No/Negative & 174 (59.0\%) & 92 (48.4\%) \\
Age (years) & 56.95 $\pm$ 11.03 & 58.26 $\pm$ 9.57 \\
Lobe location (upper) & & \\
\quad Category 1 & 161 (54.6\%) & 98 (51.6\%) \\
\quad Category 2 & 29 (9.8\%) & 18 (9.5\%) \\
\quad Category 3 & 105 (35.6\%) & 74 (38.9\%) \\
DLCO1 & 5.90 $\pm$ 2.89 & 6.31 $\pm$ 1.55 \\
VC & 3.33 $\pm$ 0.80 & 2.92 $\pm$ 0.73 \\
CEA & 4.23 $\pm$ 6.90 & 4.15 $\pm$ 10.61 \\
Outcome (Malignant) & & \\
\quad Yes/Positive & 189 (64.1\%) & 125 (65.8\%) \\
\quad No/Negative & 106 (35.9\%) & 65 (34.2\%) \\
\hline
\end{tabular}
\end{table}

\subsection{Main Performance Results}

\subsubsection{Source and Target Domain Performance}
While Figure~\ref{fig:performance-heatmaps} visualizes the relative performance trends across methods, Table~\ref{tab:main_results_table} provides the precise numerical metrics for detailed comparison. In source-domain evaluation, PANDA achieved an AUC of 0.829, significantly outperforming Random Forest (0.752) and clinical scores (Mayo AUC 0.605).

On the external target domain, the benefits of adaptation became clear. The TCA-enhanced PANDA model reached the highest AUC of 0.705 and Recall of 0.944. In contrast, Random Forest dropped to 0.632, and SVM to 0.628, indicating severe degradation due to domain shift. The clinical scores (Mayo, PKUPH) performed poorly (AUC $<$ 0.64), likely due to population differences between their original derivation cohorts and our Chinese hospital data.

\begin{table}[htbp]
\centering
\caption{Comprehensive performance comparison. Source results are from 10-fold CV; Target results are from external validation on Cohort B. Best values are bolded.}
\label{tab:main_results_table}
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{AUC} & \textbf{Accuracy} & \textbf{F1 Score} & \textbf{Precision} & \textbf{Recall} \\
\midrule
\multicolumn{6}{l}{\textit{Source Domain (Internal CV)}} \\
\textbf{PANDA (TabPFN)} & \textbf{0.829} & \textbf{0.746} & \textbf{0.810} & \textbf{0.786} & 0.846 \\
Random Forest & 0.752 & 0.698 & 0.779 & 0.735 & 0.842 \\
XGBoost & 0.742 & 0.678 & 0.752 & 0.733 & 0.787 \\
LASSO LR & 0.763 & 0.722 & 0.810 & 0.723 & \textbf{0.925} \\
Mayo Score & 0.605 & 0.359 & 0.000 & 0.000 & 0.000 \\
\midrule
\multicolumn{6}{l}{\textit{Target Domain (External Validation)}} \\
\textbf{PANDA + TCA} & \textbf{0.705} & \textbf{0.705} & \textbf{0.808} & 0.707 & \textbf{0.944} \\
PANDA (No UDA) & 0.698 & 0.663 & 0.776 & 0.689 & 0.888 \\
Random Forest & 0.632 & 0.679 & 0.775 & 0.713 & 0.854 \\
SVM & 0.628 & 0.568 & 0.647 & 0.695 & 0.606 \\
PKUPH Score & 0.636 & 0.695 & 0.784 & \textbf{0.733} & 0.847 \\
Mayo Score & 0.584 & 0.342 & 0.000 & 0.000 & 0.000 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{img/cross_hospital/combined_heatmaps_nature.pdf}
    \caption{\textbf{Performance comparison heatmaps.} \textbf{a} Source domain 10-fold CV. \textbf{b} Cross-domain external validation.}
    \label{fig:performance-heatmaps}
\end{figure}

\subsubsection{Stratified Analysis}
To investigate potential biases, we evaluated PANDA's performance across key subgroups (Table~\ref{tab:stratified_analysis}).
\begin{itemize}
    \item \textbf{Nodule Size}: Performance is robust for large nodules ($>$8mm, AUC 0.74) but drops for sub-centimeter nodules (AUC 0.65), reflecting the inherent difficulty in radiological characterization of small lesions.
    \item \textbf{Smoking Status}: The model performs better in smokers (AUC 0.72) than non-smokers (AUC 0.68), likely because smoking provides a strong prior for malignancy that the model leverages.
    \item \textbf{Gender}: We observed consistent performance across gender (AUC 0.70 vs 0.71), suggesting no significant gender bias.
\end{itemize}

\begin{table}[htbp]
\centering
\caption{Stratified performance of PANDA+TCA on the target cohort.}
\label{tab:stratified_analysis}
\begin{tabular}{lccc}
\toprule
\textbf{Subgroup} & \textbf{n} & \textbf{AUC} & \textbf{Sensitivity} \\
\midrule
\textbf{Nodule Size} & & & \\
$\\le 8$ mm & 72 & 0.65 & 0.88 \\
$> 8$ mm & 118 & 0.74 & 0.96 \\
\midrule
\textbf{Smoking History} & & & \\
Never Smoker & 105 & 0.68 & 0.92 \\
Current/Former & 85 & 0.72 & 0.97 \\
\midrule
\textbf{Gender} & & & \\
Male & 110 & 0.71 & 0.95 \\
Female & 80 & 0.70 & 0.93 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Additional Cross-Domain Validation on TableShift}

We further validated PANDA on the TableShift BRFSS Diabetes benchmark (White $\rightarrow$ Non-White race shift).

\begin{table}[htbp]
\centering
\caption{TableShift BRFSS Diabetes Results (Race Shift). OOD = Out of Distribution (Non-White).}
\label{tab:tableshift_results}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{ID AUC} & \textbf{OOD AUC} & \textbf{OOD Acc} & \textbf{Gap} \\
\midrule
\textbf{PANDA + TCA} & 0.809 & \textbf{0.804} & \textbf{0.848} & \textbf{-0.005} \\
PANDA (No UDA) & 0.809 & 0.796 & 0.847 & -0.013 \\
XGBoost & 0.815 & 0.783 & 0.840 & -0.032 \\
Decision Tree & 0.680 & 0.566 & 0.720 & -0.114 \\
\bottomrule
\end{tabular}
\end{table}

As shown in Table~\ref{tab:tableshift_results}, PANDA+TCA maintained an AUC of 0.804 on the OOD target, showing almost zero degradation from the ID source (0.809). In contrast, XGBoost dropped from 0.815 to 0.783.

\textbf{Discussion on Precision/Recall}:
Readers may notice low F1 scores in BRFSS despite high accuracy (Fig.~\ref{fig:brfss-roc}). This is an artifact of the low prevalence (17.4\%) and the default 0.5 threshold. The model correctly identifies most negatives (high accuracy) but, without class re-weighting, yields moderate precision on the minority positive class. For screening purposes, the high AUC (0.804) confirms the model's discriminative power; the operating point can be adjusted via threshold tuning to prioritize recall.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\textwidth]{img/tableshift/combined_analysis_figure.pdf}
    \caption{\textbf{TableShift BRFSS Diabetes analysis.} \textbf{a,b} ROC curves showing PANDA's robustness. \textbf{c,d} Calibration curves. \textbf{e,f} Decision curves.}
    \label{fig:brfss-roc}
\end{figure}

\subsection{Interpretability and Stability}
Recursive Feature Elimination (RFE) identified a stable subset of 8 features (Age, Spiculation, etc.) that maximized the Cost-Effectiveness Index (Fig.~\ref{fig:rfe-performance}). This "best8" set performed within 1\% of the full 63-feature set but with significantly better cross-center stability.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{img/cross_hospital/feature_performance_comparison_comprehensive.pdf}
    \caption{RFE performance analysis. \textbf{a} AUC plateaus around 8-10 features. \textbf{b} Stability improves with smaller subsets.}
    \label{fig:rfe-performance}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{img/cross_hospital/combined_analysis_figure.pdf}
    \caption{\textbf{Cross-hospital pulmonary nodule analysis.} \textbf{a,b} ROC curves. \textbf{c,d} Calibration plots. \textbf{e,f} Decision curves.}
    \label{fig:combined_analysis}
\end{figure}

\label{sec:eval-end}