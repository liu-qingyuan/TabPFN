\section{Analysis}
\label{sec:analysis}

We trace how PANDA deals with the main sources of failure in cross-site medical AI. Each component is tied to a specific hurdle rather than bolted on for convenience, and the mechanics show up in both the math and the observed gains.

\subsection{In-Context Learning for Small-Sample Robustness}

Deep models tend to overfit on small cohorts (e.g., $n_s = 295$) and swing wildly with minor perturbations. PANDA avoids heavy re-training by casting classification as in-context learning. The TabPFN backbone uses a \textit{Per-Feature Transformer Architecture}, treating each input $\mathbf{x} = [x_1, x_2, \ldots, x_d] \in \mathbb{R}^d$ as a token sequence:
\[
\mathbf{e}_i = \text{Embed}(x_i) + \mathbf{p}_i, \quad i = 1, \ldots, d
\]
where $\text{Embed}(\cdot): \mathbb{R} \rightarrow \mathbb{R}^{d_{\text{model}}}$ maps features to a $d_{\text{model}}$-dimensional space. This embedded sequence $\mathbf{E} = [\mathbf{e}_1, \ldots, \mathbf{e}_d]$ is processed through a 12-layer Transformer encoder:
\[
\mathbf{H}^{(\ell)} = \text{LayerNorm}(\text{MultiHead}(\mathbf{H}^{(\ell-1)}) + \mathbf{H}^{(\ell-1)})
\]
\[
\mathbf{H}^{(\ell+1)} = \text{LayerNorm}(\text{FFN}(\mathbf{H}^{(\ell)}) + \mathbf{H}^{(\ell)})
\]
where $\mathbf{H}^{(0)} = \mathbf{E}$. To circumvent data scarcity, the model is pre-trained using a stochastic task generator that synthesizes classification problems from diverse function priors. For each batch, we sample a prior family and hyperparameters:
\[
r \sim \mathrm{Categorical}(\boldsymbol{\pi}),\qquad
\boldsymbol{\theta} \sim p(\boldsymbol{\theta}\mid r),
\]
where $r\in\{\text{gp},\text{mlp},\text{ridge},\text{mix\_gp\}}$. Inputs are sampled independently from a factorized base distribution and optionally transformed:
\[
\mathbf{x}_t \sim p_{\text{base}}(\mathbf{x}), \qquad \tilde{\mathbf{x}}_t = \psi_{\boldsymbol{\theta}}(\mathbf{x}_t)
\]
During inference, the model performs in-context learning by processing the entire sequence of context examples $\mathcal{D}_{\text{ctx}} = \{(\mathbf{x}_i, y_i)\}_{i=1}^{n_{\text{ctx}}}$ and query inputs $\mathbf{x}_{\text{query}}$:
\[
\mathbf{z} = [\mathbf{x}_1, y_1, \ldots, \mathbf{x}_{n_{\text{ctx}}}, y_{n_{\text{ctx}}}, \mathbf{x}_{\text{query}}]
\]
The prediction minimizes the in-context loss over query positions (averaged over a batch of size $B$):
\[
\mathcal{L}_{\text{ICL}} = \frac{1}{B} \sum_{i=1}^{B} \sum_{t=n_{\text{ctx}}+1}^{T} \ell(f_\Theta(\mathbf{z}_{1:t-1}), y_t)
\]
The pre-trained priors act as regularizers, helping the model interpolate in sparse regions where conventional models often fail to learn stable boundaries.

\subsection{Mitigating Distributional Heterogeneity}

Performance usually drops when moving across hospitals because of small shifts in encoding and feature distributions. A dual preprocessing strategy tackles positional bias and distribution mismatch. To reduce ordering bias in the Transformer input, each ensemble member applies a cyclical permutation to the features:
\[
\mathbf{x}^{(m)}_{\text{rotated}} = \text{rotate}(\mathbf{x}, m) = [x_{(m) \bmod d}, x_{(m+1) \bmod d}, \ldots, x_{(m+d-1) \bmod d}]
\]
with rotation offsets generated deterministically for each ensemble member $m \in [0, N-1]$. In parallel, we employ \textit{Adaptive Feature Transformation} to bridge distributional gaps. The \textbf{Enhanced Feature Transformation} performs a quantile transform followed by dimensionality expansion:
\[
\mathbf{x}_{\text{quantile}} = \text{QuantileTransformer}(\mathbf{x}, n_{\text{quantiles}} = \max(\lfloor n_{\text{samples}}/10 \rfloor, 2))
\]
\[
\mathbf{X}_{\text{expanded}} = \text{SVD}(\mathbf{X}_{\text{quantile}}, n_{\text{components}} = \min(4, d))
\]
yielding a final representation $\mathbf{x}_{\text{final}} = [\mathbf{x}_{\text{original}}; \mathbf{x}_{\text{quantile}}; \mathbf{x}_{\text{SVD}}]$. A complementary \textbf{Preserved Feature Transformation} keeps the raw feature distribution:
\[
\mathbf{x}_{\text{preserved}} = \mathbf{x}_{\text{original}}
\]
Categorical variables are processed using \textit{Intelligent Categorical Encoding}:
\[
\text{encode}(x_{ij}) = \begin{cases}
\phi_j(x_{ij}) & \text{if feature $j$ has frequently occurring categories} \\
x_{ij} & \text{otherwise}
\end{cases}
\]
where $\phi_j = \pi(\{0, 1, \ldots, |U_j|-1\})$ employs randomized integer assignment. Alternatively, the \textbf{Numeric Treatment Strategy} treats categorical features as continuous:
\[
\text{encode}(x_{ij}) = \text{float}(x_{ij})
\]
Providing multiple “views” of the data lets the model marginalize hospital-specific artifacts and focus on the clinical signal.

\subsection{Addressing Feature Inconsistency}

Noisy or missing variables across cohorts make careful selection essential, and RFE offers a fairly transparent way to handle it. The workflow is straightforward:

\begin{enumerate}
    \item Train the Pre-trained Tabular Foundation Model $f_\Theta^{(t)}$ on the current feature subset $\mathcal{F}^{(t)}$.
    \item Estimate importance scores $\mathbf{I}^{(t)} = [I^{(t)}_1, I^{(t)}_2, \dots, I^{(t)}_{|\mathcal{F}^{(t)}|}]$ using permutation-based evaluation.
    \item Remove the feature with the smallest score:\\
    $\mathcal{F}^{(t+1)} \leftarrow \mathcal{F}^{(t)} \setminus \{\arg\min_j I^{(t)}_j\}$.
    \item Repeat until the subset reaches the target size $|\mathcal{F}^{(t+1)}| = k$.
\end{enumerate}

Feature importance here is defined by how much performance drops when a variable is randomly shuffled:
\[
I_j = \frac{1}{R} \sum_{r=1}^{R} \left[ \mathrm{AUC}(f_\Theta, \mathcal{D}) - \mathrm{AUC}(f_\Theta, \mathcal{D}_{\text{perm}(j)}^{(r)}) \right].
\]

To determine the optimal feature subset, we optimize a comprehensive cost-effectiveness index:
\[
\text{CostEffectiveness}(k) = w_1 \cdot S_{\text{perf}}(k) + w_2 \cdot S_{\text{eff}}(k) + w_3 \cdot S_{\text{stab}}(k) + w_4 \cdot S_{\text{simp}}(k)
\]
where the component scores are normalized as follows:
\begin{itemize}
    \item \textbf{Performance Score}:
    \[
    S_{\text{perf}}(k) = 0.5 \cdot \text{AUC}(k) + 0.3 \cdot \text{Accuracy}(k) + 0.2 \cdot \text{F1}(k)
    \]
    \item \textbf{Efficiency Score}:
    \[
    S_{\text{eff}}(k) = 1 - \frac{T(k) - T_{\min}}{T_{\max} - T_{\min}}
    \]
    \item \textbf{Stability Score}:
    \[
    S_{\text{stab}}(k) = 1 - \frac{CV(k) - CV_{\min}}{CV_{\max} - CV_{\min}}
    \]
    \item \textbf{Simplicity Score}:
    \[
    S_{\text{simp}}(k) = \exp(-\alpha \cdot k)
    \]
\end{itemize}
The optimal subset is chosen as $k^* = \arg\max_{k} \text{CostEffectiveness}(k)$, yielding a feature set that keeps strong discriminative value while still matching what hospitals can reliably collect.

\subsection{Latent Space Alignment for Covariate Shift}

A noticeable gap between internal and external validation often hints at covariate shift ($P_s(\mathbf{x}) \neq P_t(\mathbf{x})$). \textit{Transfer Component Analysis (TCA)} addresses this by mapping both domains into a shared latent subspace where their distributions look closer. Let $X_s \in \mathbb{R}^{n_s \times d}$ and $X_t \in \mathbb{R}^{n_t \times d}$ be source and target feature matrices. A combined kernel matrix $K \in \mathbb{R}^{(n_s+n_t) \times (n_s+n_t)}$ with a linear kernel $K(x_i, x_j) = x_i^\top x_j$ is partitioned as:
\[
K =
\begin{bmatrix}
K_{ss} & K_{st} \\
K_{ts} & K_{tt}
\end{bmatrix}
\]
A projection matrix $W \in \mathbb{R}^{(n_s+n_t) \times k}$ is learned by solving:
\[
\min_W \; \mathrm{tr}(W^\top K L K^\top W) + \mu \cdot \mathrm{tr}(W^\top K H K^\top W),
\]
where the alignment matrix $L$ encourages domain alignment:
\[
L =
\begin{bmatrix}
\frac{1}{n_s^2} \mathbf{1}_{n_s \times n_s} & -\frac{1}{n_s n_t} \mathbf{1}_{n_s \times n_t} \\
-\frac{1}{n_s n_t} \mathbf{1}_{n_t \times n_s} & \frac{1}{n_t^2} \mathbf{1}_{n_t \times n_t}
\end{bmatrix}
\]
and the centering matrix $H = I - \frac{1}{n_s+n_t} \mathbf{1} \mathbf{1}^\top$ ensures zero-centered features. The eigen-decomposition $(I + \mu K L K) S = K H K S$ yields $W$, and source and target samples project via $Z_s = K_s W$ and $Z_t = K_t W$. Distances are computed in the TCA space using pooled statistics $\hat{\mu}, \hat{\sigma}$ and standardized features $\mathbf{X}_s^{\text{norm}}, \mathbf{X}_t^{\text{norm}}$:
\[
\mathbf{X}_s^{\text{norm}} = \frac{\mathbf{X}_s - \hat{\mu}}{\hat{\sigma}}, \quad \mathbf{X}_t^{\text{norm}} =
 \frac{\mathbf{X}_t - \hat{\mu}}{\hat{\sigma}}
\]
These metrics include \textbf{Wasserstein Distance}:
\[
W_{\text{norm}}(\mathbf{X}_s, \mathbf{X}_t) = \frac{1}{d} \sum_{i=1}^d W_1(X_{s,i}^{\text{norm}}, X_{t,i}^{\text{norm}})
\]
\textbf{Symmetric KL Divergence}:
\[
KL_{\text{norm}}(\mathbf{X}_s, \mathbf{X}_t) = \frac{1}{d} \sum_{i=1}^d \frac{KL(P_{s,i}^{\text{norm}} || P_{t,i}^{\text{norm}}) + KL(P_{t,i}^{\text{norm}} || P_{s,i}^{\text{norm}})}{2}
\]
and \textbf{MMD with RBF Kernel}:
\[
\text{MMD}^2(\mathbf{X}_s, \mathbf{X}_t) =
\frac{1}{n_s(n_s-1)} \sum_{i \neq j} k(x_i^s, x_j^s)
+ \frac{1}{n_t(n_t-1)} \sum_{i \neq j} k(x_i^t, x_j^t)
- \frac{2}{n_s n_t} \sum_{i,j} k(x_i^s, x_j^t)
\]
where $k(\mathbf{x}, \mathbf{y}) = \exp(-\gamma ||\mathbf{x} - \mathbf{y}||^2)$. 

\subsection{Stabilizing Predictions with Ensemble Aggregation}

Single models often give poorly calibrated scores that drift toward the majority class. PANDA tempers this tendency with an ensemble setup, which aggregates multiple slightly varied representations to steady both calibration and overall stability. \textbf{Class imbalance handling} uses inverse-frequency reweighting:
\[
\hat{p}_i^{\text{balanced}} = \frac{\hat{p}_i / \pi_i}{\sum_{j=1}^C \hat{p}_j / \pi_j}
\]
where $\hat{p} = (p_1, \dots, p_C)$ are predicted probabilities and $\pi$ the empirical class distribution. \textbf{Ensemble aggregation} takes a simple but surprisingly steadying approach: it averages the temperature-scaled outputs from $N=32$ members,
\[
p(y=c \mid \mathbf{x}) = \frac{1}{N} \sum_{i=1}^{N} \frac{\exp(z_i^c/T)}{\sum_{c'=1}^C \exp(z_i^{c'}/T)},
\]
where $z_i^c$ are the logits and $T=0.9$ sets the softmax temperature. This kind of averaging tends to smooth out the quirks of any single model. It usually improves calibration and cuts down variance, giving risk scores that feel a bit more stable—something clinicians often care about more than a marginal bump in accuracy.

\subsection{Why PANDA Outperforms Baselines}

Before applying TCA, the PCA and t-SNE plots (Fig.~\ref{fig:tca-visualization}a,c) show that the two hospitals' data don't quite land in the same neighborhood—there's some separation, though perhaps not as dramatic as one might expect from a textbook domain-shift example. Still, the shape of the clusters hints at meaningful differences in how the two cohorts distribute themselves in feature space. After alignment (Fig.~\ref{fig:tca-visualization}b,d), those clouds pull a bit closer together. They don't collapse into a single blob, but the overlap becomes tighter in a way that feels more reassuring than the raw-input view.

When we looked at the numbers behind the scenes—the MMD, Wasserstein-1 distance, and symmetric KL divergence computed on the latent representations—they all moved in the direction we hoped for: smaller gaps, less tug-of-war between hospitals. These weren't included as explicit figures, but the calculations (following the definitions in Sec.~\ref{sec:analysis}) back up the visual impression. It's not perfect alignment, but it seems to argue that the method is at least nudging the domains toward the same latent "language."

Another piece that quietly helps is the cross-domain RFE step. By trimming the features down to the eight variables both hospitals actually measure—and that stay predictive across both—it strips away a lot of those site-specific quirks that often masquerade as signal. This makes the alignment problem less messy. There's even a theoretical hint supporting this: the covariance bound discussed in the Theoretical Foundation – Feature selection and domain adaptation interact section suggests that selecting lower-variance shared features may shrink the alignment complexity. In practice, that seems to match what we observed: once the feature set stops dragging along hospital-specific noise, TCA has an easier time finding a common subspace that both cohorts can live with.

