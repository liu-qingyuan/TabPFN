\section{Analysis: Mechanisms of Generalization}
\label{sec:analysis-start}
\label{sec:analysis}

This section dissects \textit{why} PANDA succeeds where traditional baselines fail. By isolating the contributions of the pre-trained backbone, the feature selection strategy, and the domain alignment, we show that performance gains stem from specific architectural choices rather than random chance.

\subsection{Mechanism 1: The Pre-training Prior as a Regularizer}

The most striking result in our experiments (refer to Table~\ref{tab:source_cv_results}) is the performance gap on the source domain itself. TabPFN achieves an AUC of 0.829, significantly outperforming Random Forest (0.752) and XGBoost (0.742) on the same 295-patient cohort.

\textbf{Why does this happen?}
Deep learning theory suggests that small datasets (\$n < 1000\$) lack sufficient signal to constrain the vast hypothesis space of over-parameterized models (like gradient boosted trees). TabPFN circumvents this by not learning from scratch. Instead, it performs \textit{in-context Bayesian inference}.

Mathematically, a standard model estimates $P(y|x, \mathcal{D}_{\text{train}})$ by optimizing parameters $\theta$ to minimize empirical risk. TabPFN, however, approximates the posterior predictive distribution:
\[
P(y|x, \mathcal{D}_{\text{train}}) \approx \int P(y|x, \theta) P(\theta|\mathcal{D}_{\text{train}}) d\theta
\]
using a Transformer pre-trained to simulate this integral over millions of synthetic priors. This "synthetic prior" acts as a massive regularizer. When the model sees a small medical dataset, it doesn't try to fit a new complex decision boundary; it effectively matches the data pattern to a library of known robust functions (e.g., smooth linear trends, sparse interactions). This explains why TabPFN retains high performance (AUC 0.698) on the external target domain even without adaptation, whereas Random Forest collapses to 0.632—a generalization gap of over 6\%.

\subsection{Mechanism 2: Feature Stability via Cross-Domain RFE}

Medical datasets are notorious for "site-specific artifacts"—features that are highly predictive in one hospital but meaningless in another (e.g., a specific scanner setting or a radiologist's subjective "spiculation" score).

Our Cross-Domain RFE protocol forces the model to discard these brittle features. By intersecting the feature importance rankings from the source domain with the availability constraints of the target domain, we converge on a "minimal sufficient set" (the \texttt{best8} set).

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{img/hospital_cross_domain/feature_stability.png}
\caption{Feature stability analysis. Features retained by Cross-Domain RFE (green) show consistent importance rankings across domains, while rejected features (red) exhibit high variance or domain-specific bias.}
\label{fig:feature_stability}
\end{figure}

The "Cost-Effectiveness Index" (CEI) defined in Eq.~\ref{eq:cei} further penalizes large feature sets. We observed that adding features beyond the top 8 yielded diminishing returns in AUC but linearly increased the risk of missing data in the target domain. Thus, RFE acts as a "validity filter," ensuring that the downstream adaptation engine (TCA) only processes signals that are likely to transfer.

\subsection{Mechanism 3: Latent Space Alignment (TCA)}

While TabPFN provides a strong initialization, a distribution shift remains. The source and target cohorts differ in subtle demographic ways (e.g., smoking prevalence, age distribution). Transfer Component Analysis (TCA) bridges this last mile.

Our results show that TCA improves AUC from 0.698 (PANDA\_NoUDA) to 0.705 (PANDA). While this gain appears modest (+0.007), it is statistically significant in the context of clinical risk scoring, where thresholds are sensitive.

\textbf{Why Linear Alignment Works in Latent Space:}
TCA seeks a projection $\mathbf{W}$ to minimize the Maximum Mean Discrepancy (MMD) between domains. We apply this in the \textit{embedding space} of the Transformer, not the raw input space.
\[
\mathbf{z} = \text{Transformer}(\mathbf{x})
\]
\[
\text{MMD}^2(P_S, P_T) = \|\mu_S(\mathbf{z}) - \mu_T(\mathbf{z})\|_{\mathcal{H}}^2
\]
Since the Transformer has already linearized the complex manifolds of the raw data (disentangling class clusters), a simple linear alignment (TCA) on $\mathbf{z}$ is sufficient to correct global shifts (like mean shifts due to calibration differences) without overfitting. Non-linear alignment methods (like Kernel PCA) often failed in our experiments, likely inducing negative transfer by aligning noise.

\subsection{Ablation Analysis: Dissecting PANDA's Components}

To understand the sources of PANDA's performance, we analyze the contribution of its two main pillars: the pre-trained foundation model backbone and the domain adaptation mechanism.

\subsubsection{Contribution of Domain Adaptation (TCA)}
The primary ablation compares the full PANDA framework (with TCA) against the PANDA baseline without domain adaptation (No-TCA). As shown in the experimental results, applying TCA improves the AUC on the target domain from 0.6980 to 0.7046. While the absolute gain in AUC is modest, the primary benefit of TCA lies in correcting the distributional mismatch, ensuring that the model's confidence scores are better aligned with the target population's risk profile. This alignment is critical for determining safe operating thresholds in a clinical setting.

\subsubsection{Contribution of the Pre-trained Backbone}
We can view the traditional machine learning baselines (Random Forest, XGBoost) as an ablation of the "Foundation Model" component. The PANDA (No-TCA) variant uses the TabPFN backbone and achieves an AUC of 0.6980 on the external cohort, whereas Random Forest and XGBoost achieve only 0.6324 and 0.5672, respectively. This significant gap ($\Delta \text{AUC} > 0.06$) demonstrates that the robustness of PANDA is primarily driven by the priors learned during the pre-training phase of the TabPFN backbone, which prevents the overfitting observed in tree-based models on small datasets.

\subsection{Error Analysis and Limitations}

Despite the improvements, PANDA makes errors. We analyzed 50 misclassified cases from the target cohort:

\begin{enumerate}
    \item \textbf{False Negatives (Risk Underestimation)}: Most often occurred in small nodules ($<8$mm) in non-smokers. The model, driven by priors that associate malignancy with size and smoking, tends to be conservative here.
    \item \textbf{False Positives (Over-treatment risk)}: Often involved inflammatory granulomas (tuberculosis scars). These mimic the radiological appearance of malignancy (spiculation) but are benign. PANDA struggles to distinguish these without specific biomarkers (like PET-CT metabolic activity), which were not in the "best8" set.
    \item \textbf{Subgroup Bias}: Performance remains lower in younger patients ($<45$), likely because the training cohort (avg age ~60) is dominated by older demographics.
\end{enumerate}

These findings highlight the "Closed-World Assumption" limitation: PANDA cannot learn features that are not present in the input schema. If crucial discriminators (like PET-CT) are missing from the shared set, no amount of adaptation can fully recover the performance.

\subsection{Real-Time Feasibility}
The complete PANDA pipeline, including feature preprocessing, TabPFN inference, and TCA projection, executes in less than one minute per patient case on standard hardware. This latency is negligible compared to the time required for radiological image acquisition and human review, confirming that the proposed framework is computationally viable for real-time clinical decision support systems.

\label{sec:analysis-end}