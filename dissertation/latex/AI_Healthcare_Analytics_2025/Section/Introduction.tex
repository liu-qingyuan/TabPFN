\section{Introduction}
\label{sec:intro-start}

Accurate diagnostic risk prediction is a canonical setting in which advances in machine learning (ML) and data analytics can have immediate clinical impact. In pulmonary nodule screening, classical clinical risk scores such as the Mayo Clinic, Veterans Affairs, Brock (PanCan), PKUPH, and Li models achieve strong internal discrimination (AUC $\approx 0.80$--$0.94$) by fitting logistic regressions to carefully curated, single-center cohorts~\cite{swensen1997chest,mcwilliams2013probability,li2011development,he2021novel,zhang_comprehensive_2022,liu_establishment_2024}. However, meta-analyses and external validations show that their performance can decline to AUCs of $0.60$--$0.75$ when transported to community-screening sites, TB-endemic regions, or demographically distinct populations~\cite{garau_external_2020,zhang_comprehensive_2022,liu_establishment_2024}. These degradations, driven by shifts in disease prevalence, acquisition protocols, and background pathology (e.g., granulomas versus tuberculosis), illustrate how non-adaptive risk calculators can become unreliable in cross-hospital practice.

From an AI perspective, these failures reflect a mismatch between the complexity of real-world deployment and the simplifying assumptions of classical supervised learning. Clinical tabular datasets are typically small, imbalanced, and heterogeneous: even high-value registries often contain only a few hundred labeled patients, with malignant nodules representing a minority class. Features are high-dimensional and only partially overlapping across sites, as institutions log different biomarker panels, coding schemes, and acquisition protocols. This combination of small sample size, distribution shift, and feature-space mismatch violates the closed-world assumptions underlying many standard models and exposes the limits of purely local training.

The algorithmic trajectory for structured data in healthcare mirrors this tension. Gradient-boosted decision trees (GBDTs), led by XGBoost, LightGBM, and related ensembles, remain the workhorses of tabular ML because they tolerate heterogeneous scales, missingness, and noisy categorical codes~\cite{chen2016xgboost,gorishniy2021revisiting}. Neural ``deep tabular'' architectures—including TabNet, TabTransformer, SAINT, FT-Transformer, NODE, and other attention- or gating-based variants—extend differentiability to structured data and enable multimodal fusion, but they require substantial data, are sensitive to hyperparameters, and often lag well-tuned trees on clinical benchmarks when effective sample sizes are small~\cite{arik2021tabnet,huang2020tabtransformer,somepalli2021saint,borisov2022deep,gorishniy2021revisiting}. Radiomics pipelines engineer thousands of texture descriptors from CT volumes, and 3D convolutional neural networks (CNNs) achieve strong internal performance on datasets such as NLST and LIDC, yet their scanner sensitivity and propensity for shortcut learning often negate cross-site gains: external validations reveal double-digit AUC drops when voxel spacing, reconstruction kernels, or case mix shift, and models can latch onto hospital-specific artifacts rather than biological signals~\cite{ardila_end--end_2019,zech_variable_2018,garau_external_2020,hellin2024unraveling}.

More recently, tabular foundation models and large tabular language models have emerged as promising directions. TabPFN meta-learns a transformer that approximates Bayesian posterior predictions across millions of synthetic tabular tasks, delivering hyperparameter-free, small-sample inference via in-context learning~\cite{hollmann2025accurate,schneider2024foundation}. Successors such as TabPFN-2.5 and drift-resilient TabPFN extend context length, relax attention bottlenecks, and incorporate simulated drifts into the prior~\cite{noauthor_prior_nodate,noauthor_realistic_nodate,noauthor_automldrift-resilient_tabpfn_2025}. Other work explores more realistic priors and cross-domain training curricula~\cite{noauthor_closer_nodate,noauthor_realistic_nodate}, and ``TabLLM''-style approaches serialize rows into prompts to reuse general-purpose reasoning from large language models~\cite{eremeev_turning_2025}. Parallel efforts investigate federated optimization and continual or on-device learning so that models can absorb new hospital evidence without breaching privacy constraints~\cite{guan2021domain,musa2025addressing}. Collectively, these developments define a new generation of AI systems for tabular healthcare data.

However, cross-hospital transfer remains fragile because three dominant pathologies of medical tabular data co-occur. First, sample scarcity: most pulmonary nodule cohorts contain only a few hundred labeled patients, which limits the stability of purely supervised training and amplifies overfitting~\cite{borisov2022deep}. Second, distribution shift: label prevalence, scanner kernels, demographics, and clinical workflows alter the marginal $P(X)$ and even the conditional $P(Y \mid X)$ between hospitals~\cite{koch2024distribution,guo_evaluation_2022}. Third, feature heterogeneity: sites log disjoint biomarker panels, adopt different measurement units, and follow distinct coding policies, which invalidates naive feature alignment and introduces missingness shifts~\cite{orouji_domain_nodate}. Domain adaptation research in imaging and wearables shows that adversarial training, cycle-consistent style transfer, optimal transport, and statistical moment matching can recover some performance under shift~\cite{guan2021domain,ahn_unsupervised_2023}, but these methods are rarely specialized for structured clinical data. Benchmarks such as TableShift and Wild-Time demonstrate that off-the-shelf robustness mechanisms still incur large out-of-distribution (OOD) gaps even when in-distribution accuracy is high~\cite{gardner_benchmarking_2024,yao2022wild}. Large-scale regulators and hospital governance boards increasingly regard shift detection, recalibration, and drift monitoring as core AI-safety requirements rather than optional post hoc checks~\cite{koch2024distribution}.

Tabular foundation models partially alleviate data scarcity, yet they inherit a closed-world assumption: the context set used during in-context learning is assumed to reflect the same joint distribution and feature schema as the query samples~\cite{schneider2024foundation}. When shifts in biomarkers, acquisition settings, or schemata emerge, attention weights may anchor on non-comparable neighbors, yielding overconfident yet incorrect predictions~\cite{noauthor_realistic_nodate,noauthor_automldrift-resilient_tabpfn_2025}. Emerging variants such as TabPFN-2.5 and drift-resilient TabPFN extend context length and inject synthetic drifts into the prior~\cite{noauthor_prior_nodate,noauthor_automldrift-resilient_tabpfn_2025}, but they remain sensitive to mismatched feature spaces and unlabeled target domains in the absence of explicit alignment. Tabular LLM approaches add reasoning capacity but incur substantial latency, quantization error for numerical values, and lack built-in clinical calibration, especially when lab panels or race-specific prevalences deviate from training distributions~\cite{eremeev_turning_2025}. Consequently, bridging the gap between high internal accuracy and safe cross-site deployment requires combining foundation models with principled unsupervised domain adaptation and feature selection that respect clinical realities.

Pulmonary nodule malignancy prediction is an archetypal stress test for these issues. Traditional clinical scores and their LASSO or GBDT successors were derived from narrowly defined cohorts with fixed demographic profiles and scanner protocols, so their coefficients silently encode source-specific prevalence, upper-lobe priors, and calcification heuristics~\cite{swensen1997chest,mcwilliams2013probability,li2011development,he2021novel,zhang_comprehensive_2022}. Meta-analyses across Asian screening programs and European cancer centers show that the same score threshold yields widely varying sensitivities (50--90\%) once smoking histories, granulomatous disease burdens, or acquisition kernels change~\cite{garau_external_2020,liu_establishment_2024}. Radiomics signatures tuned on sharp-kernel CTs lose discriminatory power on smooth-kernel images unless aggressively harmonized, and even then residual scanner bias can dominate texture features~\cite{hellin2024unraveling}. 3D CNNs for end-to-end malignancy prediction exhibit similar behavior, with performance degrading under scanner upgrades or demographic shifts~\cite{ardila_end--end_2019,zech_variable_2018}. These failures underscore that, without explicit feature pruning and alignment, both classical and modern models can become confidently wrong.

Similar tensions arise in population-health settings such as the BRFSS race-shift diabetes task. Demographic composition, socioeconomic exposures, and survey-year wording alter the marginal distribution of risk factors, while diabetes prevalence rises from roughly 12.5\% in White respondents to 17.4\% in non-White cohorts, causing fixed operating points to misfire~\cite{gardner_benchmarking_2024}. Benchmarks such as TableShift and Wild-Time make explicit that covariate shift ($P_s(X) \neq P_t(X)$), label shift ($P_s(Y) \neq P_t(Y)$), and concept shift ($P_s(Y \mid X) \neq P_t(Y \mid X)$) often co-occur, and that classical empirical risk minimization (ERM) on the source domain does not control the divergence term that drives target error~\cite{gardner_benchmarking_2024,koch2024distribution,yao2022wild}. In practice, these shifts invalidate the implicit closed-world assumptions behind most off-the-shelf models.

Feature engineering and feature selection choices are therefore as important as model class. Clinical tables mix continuous laboratory values, ordinal scores, sparse categorical codes, and structured missingness; naive one-hot encoding can expand dimensionality and encode site-specific artifacts. Stability-driven feature pruning, hierarchical encoding of categorical variables, and unit-aware normalization reduce spurious site signatures and focus attention on shared, clinically interpretable signals~\cite{sun2019informative}. Recursive feature elimination (RFE) across domains further enforces schema overlap, trading a slight drop in ceiling accuracy for substantial gains in portability when hospitals differ, and it is particularly helpful in small-sample, high-dimensional, and imbalanced regimes such as pulmonary nodules and radiomics panels~\cite{sun2019informative,borisov2022deep}.

Taken together, the research gap is stark. Tree ensembles and deep tabular networks struggle with small, heterogeneous cohorts and typically require retraining when schemas change~\cite{chen2016xgboost,gorishniy2021revisiting,borisov2022deep}. Foundation models improve small-sample performance but assume matched domains and aligned schemas~\cite{hollmann2025accurate,schneider2024foundation,noauthor_realistic_nodate}. Generic domain adaptation methods rarely account for missing features, label drift, or unlabeled targets in clinical tables~\cite{guan2021domain,ahn_unsupervised_2023,gardner_benchmarking_2024,koch2024distribution}. Federated and continual learning strategies help with privacy and incremental updates but do not by themselves guarantee cross-hospital calibration~\cite{guan2021domain,musa2025addressing}. A credible solution must (i) retain sample efficiency via strong pre-trained priors, (ii) discard site-specific signals that cannot transfer, and (iii) align source and target representations without target labels, while exposing calibration behavior under prevalence drift.

This study therefore adopts a pragmatic stance and introduces \emph{PANDA} (Pretrained Adaptation Network with Domain Alignment), a framework designed to transform diagnostic prediction through advanced ML and data analytics in realistic cross-hospital settings. PANDA chains three complementary components. First, a pre-trained tabular foundation model (TabPFN) supplies strong inductive priors for small cohorts by meta-learning across millions of synthetic tasks and enabling hyperparameter-free inference~\cite{hollmann2025accurate,schneider2024foundation}. Second, cross-domain RFE prunes to biomarkers that are consistently available and stable across sites, mitigating schema mismatch and hospital-specific artifacts~\cite{sun2019informative}. Third, a statistical alignment module based on Transfer Component Analysis (TCA) projects source and target cohorts into a shared reproducing-kernel subspace using unlabeled target data, minimizing distributional divergence while preserving clinical variance~\cite{pan2010domain}. PANDA targets the explicit goal of cross-hospital pulmonary nodule prediction with screening-level sensitivity and is further validated on the TableShift BRFSS Diabetes race-shift benchmark~\cite{gardner_benchmarking_2024}, ensuring that the proposed approach addresses both clinical and population-level distribution shifts without adding bespoke modeling components for each dataset.

In summary, cross-hospital pulmonary nodule prediction and BRFSS race-shift diabetes prediction expose the same deployment realities: privacy constraints, schema mismatch, prevalence drift, and the need for sensitivity at clinically actionable thresholds~\cite{koch2024distribution,gardner_benchmarking_2024}. Existing AI toolkits—tree ensembles, deep tabular networks, tabular foundation models, tabular LLMs, and generic domain adaptation—each leave gaps relative to these constraints~\cite{chen2016xgboost,arik2021tabnet,huang2020tabtransformer,somepalli2021saint,borisov2022deep,schneider2024foundation,guan2021domain,ahn_unsupervised_2023}. By integrating pre-trained tabular priors, schema-aware feature selection, and unsupervised domain alignment into a single pipeline, PANDA aims to restore calibration and discrimination under realistic deployment shifts. The remainder of this manuscript formalizes the cross-domain problem, surveys related work in tabular learning and medical domain adaptation, and presents PANDA as a practical instantiation of this design philosophy.

\label{sec:intro-end}
