#!/usr/bin/env python3
"""
äº¤å‰éªŒè¯æµ‹è¯•æ–‡ä»¶

æµ‹è¯•äº¤å‰éªŒè¯æ¨¡å—çš„å„ç§åŠŸèƒ½ï¼š
1. å•ä¸ªäº¤å‰éªŒè¯å®éªŒï¼ˆTabPFNï¼‰
2. æ‰€æœ‰æ ‡å‡†åŒ–ä»¥åŠä¸å¹³è¡¡å¤„ç†æ–¹æ³•çš„10æŠ˜äº¤å‰éªŒè¯ï¼ˆTabPFNï¼‰
3. ä¸åŒæ–¹æ³•TabPFNä»¥åŠå…¶ä»–æ¨¡å‹çš„å¯¹æ¯”

æ³¨æ„ï¼š
- TabPFNæ¨¡å‹ï¼šéœ€è¦æ ‡å‡†åŒ–å’Œä¸å¹³è¡¡å¤„ç†ï¼Œä½¿ç”¨best10/best8/best9/best10ç‰¹å¾é›†
- paper methodï¼šä¸éœ€è¦æ ‡å‡†åŒ–å’Œä¸å¹³è¡¡å¤„ç†ï¼Œä½¿ç”¨allç‰¹å¾é›†ï¼ˆ58ä¸ªç‰¹å¾ï¼‰
- base models (PKUPH/Mayo)ï¼šä¸éœ€è¦æ ‡å‡†åŒ–å’Œä¸å¹³è¡¡å¤„ç†ï¼Œä½¿ç”¨allç‰¹å¾é›†ï¼ˆ58ä¸ªç‰¹å¾ï¼‰
"""

import sys
import logging
import numpy as np
import pandas as pd
import json
from datetime import datetime
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from data.loader import MedicalDataLoader
from evaluation.cross_validation import CrossValidationEvaluator, run_cv_experiment, run_model_comparison_cv
from config.settings import get_features_by_type, get_categorical_features


class TestCrossValidation:
    """äº¤å‰éªŒè¯æµ‹è¯•ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•ç±»"""
        # åˆ›å»ºæµ‹è¯•ç»“æœç›®å½•
        self.test_results_dir = Path(__file__).parent / "test_results"
        self.test_results_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºæ—¶é—´æˆ³
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        logging.info(f"æµ‹è¯•ç»“æœå°†ä¿å­˜åˆ°: {self.test_results_dir}")
    
    def save_results(self, test_name, results, summary_data=None):
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
        try:
            # åˆ›å»ºæµ‹è¯•ä¸“ç”¨ç›®å½•
            test_dir = self.test_results_dir / f"{test_name}_{self.timestamp}"
            test_dir.mkdir(exist_ok=True)
            
            # ä¿å­˜è¯¦ç»†ç»“æœï¼ˆJSONæ ¼å¼ï¼‰
            results_file = test_dir / "detailed_results.json"
            with open(results_file, 'w', encoding='utf-8') as f:
                # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
                serializable_results = self._make_serializable(results)
                json.dump(serializable_results, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜æ‘˜è¦æ•°æ®ï¼ˆCSVæ ¼å¼ï¼‰
            if summary_data is not None:
                summary_file = test_dir / "summary.csv"
                if isinstance(summary_data, dict):
                    # è½¬æ¢å­—å…¸ä¸ºDataFrame
                    df = pd.DataFrame([summary_data])
                elif isinstance(summary_data, list):
                    df = pd.DataFrame(summary_data)
                else:
                    df = summary_data
                
                df.to_csv(summary_file, index=False, encoding='utf-8')
            
            logging.info(f"ç»“æœå·²ä¿å­˜åˆ°: {test_dir}")
            return test_dir
            
        except Exception as e:
            logging.error(f"ä¿å­˜ç»“æœæ—¶å‡ºé”™: {e}")
            return None
    
    def _make_serializable(self, obj):
        """å°†å¯¹è±¡è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼"""
        if isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.float64, np.float32)):
            return float(obj)
        elif pd.isna(obj):
            return None
        else:
            return obj
    
    def load_sample_data(self):
        """åŠ è½½æµ‹è¯•ç”¨çš„æ ·æœ¬æ•°æ®"""
        logging.info("å‡†å¤‡æµ‹è¯•æ•°æ®...")
        
        # ä½¿ç”¨æ•°æ®åŠ è½½å™¨åŠ è½½çœŸå®æ•°æ®
        loader = MedicalDataLoader()
        
        try:
            # å°è¯•åŠ è½½æ•°æ®é›†Aï¼ˆAI4healthï¼‰- ä½¿ç”¨all63ç‰¹å¾é›†ä»¥æ”¯æŒæ‰€æœ‰æ¨¡å‹
            dataset_info = loader.load_dataset('A', feature_type='all63')
            X = pd.DataFrame(dataset_info['X'], columns=dataset_info['feature_names'])
            y = pd.Series(dataset_info['y'])
            
            logging.info(f"æˆåŠŸåŠ è½½çœŸå®æ•°æ®: {X.shape[0]}æ ·æœ¬, {X.shape[1]}ç‰¹å¾")
            logging.info(f"ç±»åˆ«åˆ†å¸ƒ: {dict(y.value_counts().sort_index())}")
            logging.info(f"ç‰¹å¾åˆ—è¡¨: {list(X.columns)}")
            
            return X, y
            
        except Exception as e:
            logging.warning(f"æ— æ³•åŠ è½½çœŸå®æ•°æ® ({e})ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            
            # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
            np.random.seed(42)
            n_samples = 300
            
            # è·å–all63ç‰¹å¾ä»¥æ”¯æŒæ‰€æœ‰æ¨¡å‹
            features = get_features_by_type('all63')
            categorical_features = get_categorical_features('all63')
            
            # åˆ›å»ºç‰¹å¾æ•°æ®
            X_data = {}
            for feature in features:
                if feature in categorical_features:
                    # ç±»åˆ«ç‰¹å¾ï¼š0æˆ–1
                    X_data[feature] = np.random.choice([0, 1], size=n_samples, p=[0.6, 0.4])
                else:
                    # æ•°å€¼ç‰¹å¾ï¼šæ­£æ€åˆ†å¸ƒ
                    X_data[feature] = np.random.normal(0, 1, n_samples)
            
            X = pd.DataFrame(X_data)
            
            # åˆ›å»ºä¸å¹³è¡¡çš„æ ‡ç­¾ï¼ˆçº¦1:2çš„æ¯”ä¾‹ï¼‰
            y = np.random.choice([0, 1], size=n_samples, p=[0.65, 0.35])
            y = pd.Series(y)
            
            logging.info(f"åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®: {X.shape[0]}æ ·æœ¬, {X.shape[1]}ç‰¹å¾")
            logging.info(f"ç±»åˆ«åˆ†å¸ƒ: {dict(y.value_counts().sort_index())}")
            
            return X, y
    
    def test_single_cv_experiment_tabpfn(self):
        """æµ‹è¯•1: å•ä¸ªäº¤å‰éªŒè¯å®éªŒï¼ˆTabPFNï¼‰"""
        logging.info("="*80)
        logging.info("æµ‹è¯•1: å•ä¸ªäº¤å‰éªŒè¯å®éªŒï¼ˆTabPFNï¼‰")
        logging.info("="*80)
        
        X, y = self.load_sample_data()
        
        # åˆ›å»ºäº¤å‰éªŒè¯è¯„ä¼°å™¨
        evaluator = CrossValidationEvaluator(
            model_type='tabpfn',
            feature_set='best10',
            scaler_type='standard',
            imbalance_method='smote',
            cv_folds=10,  # ä½¿ç”¨10æŠ˜äº¤å‰éªŒè¯
            random_state=42,
            verbose=False  # å…³é—­è¯¦ç»†è¾“å‡ºä»¥é¿å…é‡å¤æ—¥å¿—
        )
        
        logging.info("é…ç½®ä¿¡æ¯:")
        logging.info(f"  æ¨¡å‹ç±»å‹: {evaluator.model_type}")
        logging.info(f"  ç‰¹å¾é›†: {evaluator.feature_set} ({len(evaluator.features)}ä¸ªç‰¹å¾)")
        logging.info(f"  ç‰¹å¾åˆ—è¡¨: {evaluator.features}")
        logging.info(f"  æ ‡å‡†åŒ–: {evaluator.scaler_type}")
        logging.info(f"  ä¸å¹³è¡¡å¤„ç†: {evaluator.imbalance_method}")
        logging.info(f"  ç±»åˆ«ç‰¹å¾: {evaluator.categorical_features}")
        
        # è¿è¡Œäº¤å‰éªŒè¯
        result = evaluator.run_cross_validation(X, y)
        
        # éªŒè¯ç»“æœ
        assert 'fold_results' in result
        assert 'summary' in result
        assert 'predictions' in result
        
        # éªŒè¯æŠ˜æ•°
        assert len(result['fold_results']) == 10
        
        # éªŒè¯æŒ‡æ ‡
        summary = result['summary']
        assert 'auc_mean' in summary
        assert 'accuracy_mean' in summary
        assert 'f1_mean' in summary
        
        # éªŒè¯æŒ‡æ ‡èŒƒå›´
        assert 0 <= summary['auc_mean'] <= 1
        assert 0 <= summary['accuracy_mean'] <= 1
        assert 0 <= summary['f1_mean'] <= 1
        
        logging.info("å•ä¸ªäº¤å‰éªŒè¯å®éªŒæµ‹è¯•é€šè¿‡")
        logging.info(f"  å¹³å‡AUC: {summary['auc_mean']:.4f} Â± {summary['auc_std']:.4f}")
        logging.info(f"  å¹³å‡å‡†ç¡®ç‡: {summary['accuracy_mean']:.4f} Â± {summary['accuracy_std']:.4f}")
        logging.info(f"  å¹³å‡F1: {summary['f1_mean']:.4f} Â± {summary['f1_std']:.4f}")
    
    def test_all_preprocessing_methods_tabpfn(self):
        """æµ‹è¯•2: æ‰€æœ‰æ ‡å‡†åŒ–ä»¥åŠä¸å¹³è¡¡å¤„ç†æ–¹æ³•çš„10æŠ˜äº¤å‰éªŒè¯ï¼ˆTabPFNï¼‰- å¤šç‰¹å¾é›†å¯¹æ¯”"""
        logging.info("="*80)
        logging.info("æµ‹è¯•2: æ‰€æœ‰æ ‡å‡†åŒ–ä»¥åŠä¸å¹³è¡¡å¤„ç†æ–¹æ³•çš„10æŠ˜äº¤å‰éªŒè¯ï¼ˆTabPFNï¼‰- å¤šç‰¹å¾é›†å¯¹æ¯”")
        logging.info("="*80)
        
        X, y = self.load_sample_data()
        
        # å®šä¹‰æµ‹è¯•é…ç½®
        feature_sets = ['best7', 'best8', 'best9', 'best10']
        scaler_types = ['standard', 'robust', 'none']
        imbalance_methods = [
            'none', 'smote', 'smotenc', 'borderline_smote', 
            'kmeans_smote', 'adasyn', 'smote_tomek', 'random_under'
        ]
        
        logging.info(f"æµ‹è¯•é…ç½®:")
        logging.info(f"  ç‰¹å¾é›†: {feature_sets}")
        logging.info(f"  æ ‡å‡†åŒ–æ–¹æ³•: {scaler_types}")
        logging.info(f"  ä¸å¹³è¡¡å¤„ç†æ–¹æ³•: {imbalance_methods}")
        logging.info(f"  æ€»ç»„åˆæ•°: {len(feature_sets) * len(scaler_types) * len(imbalance_methods)}")
        
        # è¿è¡Œå¤šç§é…ç½®çš„äº¤å‰éªŒè¯å®éªŒ
        results = run_cv_experiment(
            X=X,
            y=y,
            model_types=['tabpfn'],
            feature_sets=feature_sets,
            scaler_types=scaler_types,
            imbalance_methods=imbalance_methods,
            cv_folds=10,  # ä½¿ç”¨10æŠ˜äº¤å‰éªŒè¯
            random_state=42,
            verbose=False  # å‡å°‘è¾“å‡ºä»¥é¿å…æµ‹è¯•æ—¥å¿—è¿‡é•¿
        )
        
        # éªŒè¯ç»“æœ
        expected_experiments = len(feature_sets) * len(scaler_types) * len(imbalance_methods)
        
        # æŒ‰ç‰¹å¾é›†åˆ†ç»„ç»Ÿè®¡ç»“æœ
        feature_set_results = {}
        for feature_set in feature_sets:
            feature_set_results[feature_set] = {
                'experiments': [],
                'best_auc': 0,
                'best_config': '',
                'successful_count': 0
            }
        
        successful_experiments = 0
        overall_best_auc = 0
        overall_best_config = ""
        
        logging.info(f"\nå®éªŒç»“æœæ‘˜è¦:")
        logging.info(f"{'é…ç½®':<50} {'AUC':<8} {'å‡†ç¡®ç‡':<8} {'F1':<8}")
        logging.info("-" * 80)
        
        for experiment_name, result in results.items():
            if result['summary'] and 'auc_mean' in result['summary']:
                summary = result['summary']
                auc = summary['auc_mean']
                acc = summary['accuracy_mean']
                f1 = summary['f1_mean']
                
                logging.info(f"{experiment_name:<50} {auc:<8.4f} {acc:<8.4f} {f1:<8.4f}")
                
                successful_experiments += 1
                
                # æå–ç‰¹å¾é›†åç§°
                for feature_set in feature_sets:
                    if f'_{feature_set}_' in experiment_name:
                        feature_set_results[feature_set]['experiments'].append({
                            'name': experiment_name,
                            'auc': auc,
                            'acc': acc,
                            'f1': f1
                        })
                        feature_set_results[feature_set]['successful_count'] += 1
                        
                        if auc > feature_set_results[feature_set]['best_auc']:
                            feature_set_results[feature_set]['best_auc'] = auc
                            feature_set_results[feature_set]['best_config'] = experiment_name
                        break
                
                if auc > overall_best_auc:
                    overall_best_auc = auc
                    overall_best_config = experiment_name
        
        # è¾“å‡ºå„ç‰¹å¾é›†çš„æœ€ä½³ç»“æœ
        logging.info(f"\nå„ç‰¹å¾é›†æœ€ä½³ç»“æœ:")
        logging.info(f"{'ç‰¹å¾é›†':<10} {'æˆåŠŸå®éªŒæ•°':<12} {'æœ€ä½³AUC':<10} {'æœ€ä½³é…ç½®':<50}")
        logging.info("-" * 90)
        
        for feature_set in feature_sets:
            result = feature_set_results[feature_set]
            expected_per_feature = len(scaler_types) * len(imbalance_methods)
            logging.info(f"{feature_set:<10} {result['successful_count']:<12}/{expected_per_feature:<3} "
                        f"{result['best_auc']:<10.4f} {result['best_config']:<50}")
        
        logging.info(f"\nâœ“ å¤šç‰¹å¾é›†å¤šé…ç½®äº¤å‰éªŒè¯å®éªŒæµ‹è¯•é€šè¿‡")
        logging.info(f"  æ€»æˆåŠŸå®éªŒæ•°: {successful_experiments}/{expected_experiments}")
        logging.info(f"  å…¨å±€æœ€ä½³é…ç½®: {overall_best_config}")
        logging.info(f"  å…¨å±€æœ€ä½³AUC: {overall_best_auc:.4f}")
        
        # å‡†å¤‡ä¿å­˜çš„æ‘˜è¦æ•°æ®
        summary_rows = []
        for experiment_name, result in results.items():
            if result['summary'] and 'auc_mean' in result['summary']:
                summary = result['summary']
                summary_rows.append({
                    'experiment_name': experiment_name,
                    'auc_mean': summary['auc_mean'],
                    'auc_std': summary['auc_std'],
                    'accuracy_mean': summary['accuracy_mean'],
                    'accuracy_std': summary['accuracy_std'],
                    'f1_mean': summary['f1_mean'],
                    'f1_std': summary['f1_std'],
                    'precision_mean': summary.get('precision_mean', 0),
                    'recall_mean': summary.get('recall_mean', 0)
                })
        
        # ä¿å­˜ç»“æœ
        self.save_results("test2_multi_feature_preprocessing", results, summary_rows)
        
        # è‡³å°‘è¦æœ‰ä¸€åŠçš„å®éªŒæˆåŠŸ
        assert successful_experiments >= expected_experiments // 2
        
        # éªŒè¯æ¯ä¸ªç‰¹å¾é›†éƒ½æœ‰æˆåŠŸçš„å®éªŒ
        for feature_set in feature_sets:
            assert feature_set_results[feature_set]['successful_count'] > 0, f"ç‰¹å¾é›†{feature_set}æ²¡æœ‰æˆåŠŸçš„å®éªŒ"
    
    def test_comprehensive_tabpfn_and_models_comparison(self):
        """æµ‹è¯•3: æ‰€æœ‰æ ‡å‡†åŒ–ä»¥åŠä¸å¹³è¡¡å¤„ç†æ–¹æ³•çš„10æŠ˜äº¤å‰éªŒè¯ï¼ˆTabPFNï¼‰ä»¥åŠå…¶ä»–æ¨¡å‹å¯¹æ¯”"""
        logging.info("="*80)
        logging.info("æµ‹è¯•3: æ‰€æœ‰æ ‡å‡†åŒ–ä»¥åŠä¸å¹³è¡¡å¤„ç†æ–¹æ³•çš„10æŠ˜äº¤å‰éªŒè¯ï¼ˆTabPFNï¼‰ä»¥åŠå…¶ä»–æ¨¡å‹å¯¹æ¯”")
        logging.info("="*80)
        
        X, y = self.load_sample_data()
        
        # ç¬¬ä¸€éƒ¨åˆ†ï¼šTabPFNçš„æ‰€æœ‰é¢„å¤„ç†æ–¹æ³•ç»„åˆæµ‹è¯•
        logging.info("ç¬¬ä¸€éƒ¨åˆ†: TabPFNæ‰€æœ‰é¢„å¤„ç†æ–¹æ³•ç»„åˆæµ‹è¯•")
        logging.info("-" * 60)
        
        # å®šä¹‰æµ‹è¯•é…ç½®
        scaler_types = ['standard', 'robust', 'none']
        imbalance_methods = [
            'none', 'smote', 'smotenc', 'borderline_smote', 
            'kmeans_smote', 'adasyn', 'smote_tomek', 'random_under'
        ]
        
        logging.info(f"TabPFNæµ‹è¯•é…ç½®:")
        logging.info(f"  æ ‡å‡†åŒ–æ–¹æ³•: {scaler_types}")
        logging.info(f"  ä¸å¹³è¡¡å¤„ç†æ–¹æ³•: {imbalance_methods}")
        logging.info(f"  æ€»ç»„åˆæ•°: {len(scaler_types) * len(imbalance_methods)}")
        
        # è¿è¡ŒTabPFNå¤šç§é…ç½®çš„äº¤å‰éªŒè¯å®éªŒ
        tabpfn_results = run_cv_experiment(
            X=X,
            y=y,
            model_types=['tabpfn'],
            feature_sets=['best10'],
            scaler_types=scaler_types,
            imbalance_methods=imbalance_methods,
            cv_folds=10,  # ä½¿ç”¨10æŠ˜äº¤å‰éªŒè¯
            random_state=42,
            verbose=False
        )
        
        # ç»Ÿè®¡TabPFNç»“æœ
        successful_tabpfn_experiments = 0
        best_tabpfn_auc = 0
        best_tabpfn_config = ""
        
        logging.info("TabPFNå®éªŒç»“æœæ‘˜è¦:")
        logging.info(f"{'é…ç½®':<40} {'AUC':<8} {'å‡†ç¡®ç‡':<8} {'F1':<8}")
        logging.info("-" * 70)
        
        for experiment_name, result in tabpfn_results.items():
            if result['summary'] and 'auc_mean' in result['summary']:
                summary = result['summary']
                auc = summary['auc_mean']
                acc = summary['accuracy_mean']
                f1 = summary['f1_mean']
                
                logging.info(f"{experiment_name:<40} {auc:<8.4f} {acc:<8.4f} {f1:<8.4f}")
                
                successful_tabpfn_experiments += 1
                
                if auc > best_tabpfn_auc:
                    best_tabpfn_auc = auc
                    best_tabpfn_config = experiment_name
        
        logging.info(f"TabPFNæˆåŠŸå®éªŒæ•°: {successful_tabpfn_experiments}/{len(scaler_types) * len(imbalance_methods)}")
        logging.info(f"TabPFNæœ€ä½³é…ç½®: {best_tabpfn_config} (AUC: {best_tabpfn_auc:.4f})")
        
        # ç¬¬äºŒéƒ¨åˆ†ï¼šå…¶ä»–æ¨¡å‹å¯¹æ¯”æµ‹è¯•
        logging.info("\nç¬¬äºŒéƒ¨åˆ†: å…¶ä»–æ¨¡å‹å¯¹æ¯”æµ‹è¯•")
        logging.info("-" * 60)
        
        logging.info("æ¨¡å‹å¯¹æ¯”é…ç½®:")
        logging.info("  TabPFN: ä½¿ç”¨best10ç‰¹å¾é›† + æœ€ä½³é¢„å¤„ç†é…ç½®")
        logging.info("  PKUPH: ä½¿ç”¨allç‰¹å¾é›† + æ— é¢„å¤„ç†")
        logging.info("  Mayo: ä½¿ç”¨allç‰¹å¾é›† + æ— é¢„å¤„ç†")
        logging.info("  Paper LR: ä½¿ç”¨allç‰¹å¾é›† + æ— é¢„å¤„ç†")
        
        # è¿è¡Œå…¶ä»–æ¨¡å‹å¯¹æ¯”å®éªŒ
        model_results = run_model_comparison_cv(
            X=X,
            y=y,
            feature_set='best10',  # TabPFNä½¿ç”¨çš„ç‰¹å¾é›†
            scaler_type='standard',  # TabPFNä½¿ç”¨çš„æ ‡å‡†åŒ–
            cv_folds=10,  # ä½¿ç”¨10æŠ˜äº¤å‰éªŒè¯
            random_state=42,
            verbose=False
        )
        
        # ç»Ÿè®¡æ¨¡å‹å¯¹æ¯”ç»“æœ
        expected_models = ['tabpfn_best10', 'pkuph_all63', 'mayo_all63', 'paper_lr_all63']
        
        logging.info("æ¨¡å‹å¯¹æ¯”ç»“æœ:")
        logging.info(f"{'æ¨¡å‹':<15} {'AUC':<10} {'å‡†ç¡®ç‡':<10} {'F1':<10} {'ç²¾ç¡®ç‡':<10} {'å¬å›ç‡':<10}")
        logging.info("-" * 80)
        
        successful_models = 0
        model_performances = {}
        
        for model_name in expected_models:
            if model_name in model_results:
                result = model_results[model_name]
                if result['summary'] and 'auc_mean' in result['summary']:
                    summary = result['summary']
                    auc = summary.get('auc_mean', 0)
                    acc = summary.get('accuracy_mean', 0)
                    f1 = summary.get('f1_mean', 0)
                    prec = summary.get('precision_mean', 0)
                    rec = summary.get('recall_mean', 0)
                    
                    # ä»æ¨¡å‹åç§°ä¸­æå–ç®€çŸ­åç§°
                    short_name = model_name.split('_')[0]
                    logging.info(f"{short_name:<15} {auc:<10.4f} {acc:<10.4f} {f1:<10.4f} {prec:<10.4f} {rec:<10.4f}")
                    
                    model_performances[short_name] = auc
                    successful_models += 1
                else:
                    short_name = model_name.split('_')[0]
                    logging.info(f"{short_name:<15} {'å¤±è´¥':<10} {'å¤±è´¥':<10} {'å¤±è´¥':<10} {'å¤±è´¥':<10} {'å¤±è´¥':<10}")
            else:
                short_name = model_name.split('_')[0]
                logging.info(f"{short_name:<15} {'æœªè¿è¡Œ':<10} {'æœªè¿è¡Œ':<10} {'æœªè¿è¡Œ':<10} {'æœªè¿è¡Œ':<10} {'æœªè¿è¡Œ':<10}")
        
        # ç»¼åˆç»“æœåˆ†æ
        logging.info("\nç»¼åˆç»“æœåˆ†æ:")
        logging.info("-" * 60)
        
        if model_performances:
            best_model = max(model_performances.keys(), key=lambda k: model_performances[k])
            best_model_auc = model_performances[best_model]
            logging.info(f"æœ€ä½³æ¨¡å‹: {best_model} (AUC: {best_model_auc:.4f})")
            
            # æ¯”è¾ƒTabPFNæœ€ä½³é…ç½®ä¸å…¶ä»–æ¨¡å‹
            if best_tabpfn_auc > best_model_auc:
                logging.info(f"TabPFNæœ€ä½³é…ç½® ({best_tabpfn_config}) ä¼˜äºå…¶ä»–æ¨¡å‹")
                logging.info(f"TabPFNæœ€ä½³AUC: {best_tabpfn_auc:.4f} vs å…¶ä»–æ¨¡å‹æœ€ä½³AUC: {best_model_auc:.4f}")
            else:
                logging.info(f"å…¶ä»–æ¨¡å‹ ({best_model}) ä¼˜äºTabPFNæœ€ä½³é…ç½®")
                logging.info(f"å…¶ä»–æ¨¡å‹æœ€ä½³AUC: {best_model_auc:.4f} vs TabPFNæœ€ä½³AUC: {best_tabpfn_auc:.4f}")
        
        logging.info(f"TabPFNæˆåŠŸå®éªŒæ•°: {successful_tabpfn_experiments}")
        logging.info(f"å…¶ä»–æ¨¡å‹æˆåŠŸæ•°: {successful_models}/{len(expected_models)}")
        
        # éªŒè¯æµ‹è¯•æˆåŠŸ
        assert successful_tabpfn_experiments > 0, "TabPFNå®éªŒå…¨éƒ¨å¤±è´¥"
        assert successful_models > 0, "å…¶ä»–æ¨¡å‹å®éªŒå…¨éƒ¨å¤±è´¥"
        
        logging.info("ç»¼åˆæµ‹è¯•é€šè¿‡")
    
    def test_feature_sets_comparison(self):
        """æµ‹è¯•4: ä¸åŒç‰¹å¾é›†çš„å¯¹æ¯”ï¼ˆTabPFNï¼‰"""
        print("\n" + "="*80)
        print("æµ‹è¯•4: ä¸åŒç‰¹å¾é›†çš„å¯¹æ¯”ï¼ˆTabPFNï¼‰")
        print("="*80)
        
        X, y = self.load_sample_data()
        
        feature_sets = ['best7', 'best8', 'best9', 'best10']
        
        print(f"ç‰¹å¾é›†å¯¹æ¯”é…ç½®:")
        for fs in feature_sets:
            features = get_features_by_type(fs)
            cat_features = get_categorical_features(fs)
            print(f"  {fs}: {len(features)}ä¸ªç‰¹å¾ ({len(cat_features)}ä¸ªç±»åˆ«ç‰¹å¾)")
        
        results = {}
        
        for feature_set in feature_sets:
            print(f"\næµ‹è¯•ç‰¹å¾é›†: {feature_set}")
            
            evaluator = CrossValidationEvaluator(
                model_type='tabpfn',
                feature_set=feature_set,
                scaler_type='standard',
                imbalance_method='smote',
                cv_folds=10,  # ä½¿ç”¨10æŠ˜äº¤å‰éªŒè¯
                random_state=42,
                verbose=False
            )
            
            try:
                result = evaluator.run_cross_validation(X, y)
                results[feature_set] = result
                
                if result['summary'] and 'auc_mean' in result['summary']:
                    auc = result['summary']['auc_mean']
                    print(f"  âœ“ {feature_set}: AUC = {auc:.4f}")
                else:
                    print(f"  âœ— {feature_set}: å®éªŒå¤±è´¥")
                    
            except Exception as e:
                print(f"  âœ— {feature_set}: å¼‚å¸¸ - {e}")
                results[feature_set] = None
        
        # ç»“æœæ‘˜è¦
        print(f"\nç‰¹å¾é›†å¯¹æ¯”ç»“æœ:")
        print(f"{'ç‰¹å¾é›†':<10} {'ç‰¹å¾æ•°':<8} {'AUC':<10} {'å‡†ç¡®ç‡':<10} {'F1':<10}")
        print("-" * 50)
        
        successful_feature_sets = 0
        
        for feature_set in feature_sets:
            features = get_features_by_type(feature_set)
            n_features = len(features)
            
            if results[feature_set] and results[feature_set]['summary']:
                summary = results[feature_set]['summary']
                auc = summary.get('auc_mean', 0)
                acc = summary.get('accuracy_mean', 0)
                f1 = summary.get('f1_mean', 0)
                
                print(f"{feature_set:<10} {n_features:<8} {auc:<10.4f} {acc:<10.4f} {f1:<10.4f}")
                successful_feature_sets += 1
            else:
                print(f"{feature_set:<10} {n_features:<8} {'å¤±è´¥':<10} {'å¤±è´¥':<10} {'å¤±è´¥':<10}")
        
        print(f"\nâœ“ ç‰¹å¾é›†å¯¹æ¯”å®éªŒæµ‹è¯•é€šè¿‡")
        print(f"  æˆåŠŸç‰¹å¾é›†æ•°: {successful_feature_sets}/{len(feature_sets)}")
        
        # è‡³å°‘è¦æœ‰ä¸€åŠçš„ç‰¹å¾é›†æˆåŠŸ
        assert successful_feature_sets >= len(feature_sets) // 2
    
    def test_model_specific_features(self):
        """æµ‹è¯•5: éªŒè¯ä¸åŒæ¨¡å‹ä½¿ç”¨æ­£ç¡®çš„ç‰¹å¾"""
        print("\n" + "="*80)
        print("æµ‹è¯•5: éªŒè¯ä¸åŒæ¨¡å‹ä½¿ç”¨æ­£ç¡®çš„ç‰¹å¾")
        print("="*80)
        
        X, y = self.load_sample_data()
        
        # æµ‹è¯•é…ç½®
        model_configs = [
            {
                'model_type': 'tabpfn',
                'feature_set': 'best10',
                'expected_features': 10,
                'needs_preprocessing': True
            },
            {
                'model_type': 'pkuph',
                'feature_set': 'all63',  # åŸºçº¿æ¨¡å‹ä½¿ç”¨all63ç‰¹å¾é›†ï¼Œä½†å®é™…åªç”¨è‡ªå·±çš„6ä¸ªç‰¹å¾
                'expected_features': 6,  # PKUPHæ¨¡å‹å®é™…ä½¿ç”¨6ä¸ªç‰¹å¾
                'needs_preprocessing': False
            },
            {
                'model_type': 'mayo',
                'feature_set': 'all63',  # åŸºçº¿æ¨¡å‹ä½¿ç”¨all63ç‰¹å¾é›†ï¼Œä½†å®é™…åªç”¨è‡ªå·±çš„6ä¸ªç‰¹å¾
                'expected_features': 6,  # Mayoæ¨¡å‹å®é™…ä½¿ç”¨6ä¸ªç‰¹å¾
                'needs_preprocessing': False
            },
            {
                'model_type': 'paper_lr',
                'feature_set': 'all63',  # è®ºæ–‡æ–¹æ³•ä½¿ç”¨all63ç‰¹å¾é›†ï¼Œä½†å®é™…åªç”¨è‡ªå·±çš„11ä¸ªç‰¹å¾
                'expected_features': 11,  # Paper LRæ¨¡å‹å®é™…ä½¿ç”¨11ä¸ªç‰¹å¾
                'needs_preprocessing': False
            }
        ]
        
        print(f"æ¨¡å‹ç‰¹å¾é…ç½®éªŒè¯:")
        print(f"{'æ¨¡å‹':<12} {'ç‰¹å¾é›†':<8} {'é¢„æœŸç‰¹å¾æ•°':<10} {'éœ€è¦é¢„å¤„ç†':<10} {'çŠ¶æ€':<10}")
        print("-" * 65)
        
        for config in model_configs:
            model_type = config['model_type']
            feature_set = config['feature_set']
            expected_features = config['expected_features']
            needs_preprocessing = config['needs_preprocessing']
            
            try:
                evaluator = CrossValidationEvaluator(
                    model_type=model_type,
                    feature_set=feature_set,
                    scaler_type='standard' if needs_preprocessing else 'none',
                    imbalance_method='smote' if needs_preprocessing else 'none',
                    cv_folds=10,  # ä½¿ç”¨10æŠ˜äº¤å‰éªŒè¯
                    random_state=42,
                    verbose=False
                )
                
                # éªŒè¯ç‰¹å¾æ•°é‡
                actual_features = len(evaluator.features)
                
                # éªŒè¯é¢„å¤„ç†è®¾ç½®
                should_preprocess = evaluator._should_apply_preprocessing()
                
                status = "âœ“" if (actual_features == expected_features and 
                               should_preprocess == needs_preprocessing) else "âœ—"
                
                print(f"{model_type:<12} {feature_set:<8} {actual_features:<10} {should_preprocess:<10} {status:<10}")
                
                # è¿è¡Œä¸€ä¸ªå¿«é€Ÿæµ‹è¯•ä»¥ç¡®ä¿æ¨¡å‹å¯ä»¥å·¥ä½œ
                if status == "âœ“":
                    result = evaluator.run_cross_validation(X, y)
                    if result['summary'] and 'auc_mean' in result['summary']:
                        print(f"  â””â”€ å¿«é€Ÿæµ‹è¯•é€šè¿‡ï¼ŒAUC: {result['summary']['auc_mean']:.4f}")
                    else:
                        print(f"  â””â”€ å¿«é€Ÿæµ‹è¯•å¤±è´¥")
                        
            except Exception as e:
                print(f"{model_type:<12} {feature_set:<8} {'å¼‚å¸¸':<10} {'å¼‚å¸¸':<10} {'âœ—':<10}")
                print(f"  â””â”€ é”™è¯¯: {e}")
        
        print(f"\nâœ“ æ¨¡å‹ç‰¹å¾é…ç½®éªŒè¯å®Œæˆ")


def run_comprehensive_cv_test():
    """è¿è¡Œå…¨é¢çš„äº¤å‰éªŒè¯æµ‹è¯•"""
    logging.info("="*80)
    logging.info("äº¤å‰éªŒè¯æ¨¡å—å…¨é¢æµ‹è¯•")
    logging.info("="*80)
    
    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    test_instance = TestCrossValidation()
    
    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        # test_instance.test_single_cv_experiment_tabpfn()
        test_instance.test_all_preprocessing_methods_tabpfn()
        test_instance.test_comprehensive_tabpfn_and_models_comparison()
        test_instance.test_feature_sets_comparison()
        test_instance.test_model_specific_features()
        
        logging.info("="*80)
        logging.info("ğŸ‰ æ‰€æœ‰äº¤å‰éªŒè¯æµ‹è¯•é€šè¿‡ï¼")
        logging.info(f"æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {test_instance.test_results_dir}")
        logging.info("="*80)
        
    except Exception as e:
        logging.error(f"æµ‹è¯•å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œæµ‹è¯•
    run_comprehensive_cv_test() 