"""
UDA Medical Imbalance Project - è®ºæ–‡æ–¹æ³•æµ‹è¯•

æµ‹è¯•åŸºäºŽpredict_healthcare_LR.pyçš„è®ºæ–‡æ–¹æ³•åŠŸèƒ½å’Œæ€§èƒ½ã€‚
"""

import pytest
import numpy as np
import pandas as pd
import os
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from modeling.paper_methods import (
    PaperLRModel, get_paper_method, evaluate_paper_methods, 
    compare_with_original_lr_script
)


class TestPaperLRModel:
    """æµ‹è¯•è®ºæ–‡LRæ¨¡åž‹"""
    
    def setup_method(self):
        """è®¾ç½®æµ‹è¯•æ•°æ®"""
        self.model = PaperLRModel()
        self.n_samples = 100
        np.random.seed(42)
        
        # åˆ›å»ºåŒ…å«è®ºæ–‡æ–¹æ³•æ‰€éœ€ç‰¹å¾çš„æµ‹è¯•æ•°æ®
        self.features = [
            'Feature2', 'Feature5', 'Feature48', 'Feature49', 'Feature50', 
            'Feature52', 'Feature56', 'Feature57', 'Feature61', 'Feature42', 'Feature43'
        ]
        self.X = pd.DataFrame({
            feature: np.random.normal(0, 1, self.n_samples) for feature in self.features
        })
        self.y = np.random.randint(0, 2, self.n_samples)
    
    def test_model_initialization(self):
        """æµ‹è¯•æ¨¡åž‹åˆå§‹åŒ–"""
        assert self.model.intercept_ == -1.137
        assert len(self.model.features) == 11
        assert len(self.model.coefficients) == 11
        assert not self.model.is_fitted_
        
        # æ£€æŸ¥ç‰¹å¾å’Œç³»æ•°çš„å¯¹åº”å…³ç³»
        expected_features = [
            'Feature2', 'Feature5', 'Feature48', 'Feature49', 'Feature50', 
            'Feature52', 'Feature56', 'Feature57', 'Feature61', 'Feature42', 'Feature43'
        ]
        assert self.model.features == expected_features
        
        # æ£€æŸ¥ç³»æ•°å€¼
        assert self.model.coefficients['Feature2'] == 0.036
        assert self.model.coefficients['Feature5'] == 0.380
        assert self.model.coefficients['Feature50'] == -0.290
    
    def test_model_fit(self):
        """æµ‹è¯•æ¨¡åž‹æ‹Ÿåˆ"""
        fitted_model = self.model.fit(self.X, self.y)
        assert fitted_model is self.model
        assert self.model.is_fitted_
    
    def test_calculate_risk_score(self):
        """æµ‹è¯•é£Žé™©è¯„åˆ†è®¡ç®—"""
        self.model.fit(self.X, self.y)
        risk_scores = self.model.calculate_risk_score(self.X)
        
        assert risk_scores.shape == (self.n_samples,)
        assert isinstance(risk_scores, np.ndarray)
        
        # æ‰‹åŠ¨è®¡ç®—ç¬¬ä¸€ä¸ªæ ·æœ¬çš„é£Žé™©è¯„åˆ†è¿›è¡ŒéªŒè¯
        first_sample = self.X.iloc[0]
        expected_score = self.model.intercept_
        for feature, coef in self.model.coefficients.items():
            expected_score += coef * first_sample[feature]
        
        np.testing.assert_almost_equal(risk_scores[0], expected_score, decimal=6)
    
    def test_predict_proba_before_fit(self):
        """æµ‹è¯•æœªæ‹Ÿåˆæ—¶é¢„æµ‹æ¦‚çŽ‡åº”è¯¥æŠ¥é”™"""
        with pytest.raises(ValueError, match="æ¨¡åž‹å°šæœªæ‹Ÿåˆ"):
            self.model.predict_proba(self.X)
    
    def test_predict_proba_after_fit(self):
        """æµ‹è¯•æ‹ŸåˆåŽé¢„æµ‹æ¦‚çŽ‡"""
        self.model.fit(self.X, self.y)
        proba = self.model.predict_proba(self.X)
        
        assert proba.shape == (self.n_samples, 2)
        assert np.all(proba >= 0) and np.all(proba <= 1)
        assert np.allclose(proba.sum(axis=1), 1.0)
        
        # éªŒè¯æ¦‚çŽ‡è®¡ç®—å…¬å¼
        risk_scores = self.model.calculate_risk_score(self.X)
        expected_p_malignant = np.exp(risk_scores) / (1 + np.exp(risk_scores))
        np.testing.assert_array_almost_equal(proba[:, 1], expected_p_malignant)
    
    def test_predict_after_fit(self):
        """æµ‹è¯•æ‹ŸåˆåŽé¢„æµ‹ç±»åˆ«"""
        self.model.fit(self.X, self.y)
        predictions = self.model.predict(self.X)
        
        assert predictions.shape == (self.n_samples,)
        assert np.all(np.isin(predictions, [0, 1]))
        
        # éªŒè¯é¢„æµ‹ä¸Žæ¦‚çŽ‡çš„ä¸€è‡´æ€§
        proba = self.model.predict_proba(self.X)
        expected_predictions = np.argmax(proba, axis=1)
        np.testing.assert_array_equal(predictions, expected_predictions)
    
    def test_get_feature_names(self):
        """æµ‹è¯•èŽ·å–ç‰¹å¾åç§°"""
        feature_names = self.model.get_feature_names()
        assert feature_names == self.features
        assert feature_names is not self.model.features  # åº”è¯¥è¿”å›žå‰¯æœ¬
    
    def test_get_risk_scores(self):
        """æµ‹è¯•èŽ·å–é£Žé™©è¯„åˆ†"""
        self.model.fit(self.X, self.y)
        risk_scores = self.model.get_risk_scores(self.X)
        
        assert risk_scores.shape == (self.n_samples,)
        
        # åº”è¯¥ä¸Žcalculate_risk_scoreè¿”å›žç›¸åŒç»“æžœ
        expected_scores = self.model.calculate_risk_score(self.X)
        np.testing.assert_array_equal(risk_scores, expected_scores)
    
    def test_get_risk_scores_before_fit(self):
        """æµ‹è¯•æœªæ‹Ÿåˆæ—¶èŽ·å–é£Žé™©è¯„åˆ†åº”è¯¥æŠ¥é”™"""
        with pytest.raises(ValueError, match="æ¨¡åž‹å°šæœªæ‹Ÿåˆ"):
            self.model.get_risk_scores(self.X)
    
    def test_numpy_input(self):
        """æµ‹è¯•numpyæ•°ç»„è¾“å…¥"""
        self.model.fit(self.X, self.y)
        X_numpy = self.X.values
        
        proba = self.model.predict_proba(X_numpy)
        predictions = self.model.predict(X_numpy)
        risk_scores = self.model.get_risk_scores(X_numpy)
        
        assert proba.shape == (self.n_samples, 2)
        assert predictions.shape == (self.n_samples,)
        assert risk_scores.shape == (self.n_samples,)
    
    def test_missing_features_warning(self, capsys):
        """æµ‹è¯•ç¼ºå¤±ç‰¹å¾æ—¶çš„è­¦å‘Š"""
        # åˆ›å»ºç¼ºå°‘æŸäº›ç‰¹å¾çš„æ•°æ®
        X_incomplete = self.X[['Feature2', 'Feature5', 'Feature48']].copy()
        self.model.fit(X_incomplete, self.y[:len(X_incomplete)])
        
        self.model.predict_proba(X_incomplete)
        captured = capsys.readouterr()
        assert "è­¦å‘Š" in captured.out


class TestPaperMethodFactory:
    """æµ‹è¯•è®ºæ–‡æ–¹æ³•å·¥åŽ‚å‡½æ•°"""
    
    def test_get_paper_method_paper_lr(self):
        """æµ‹è¯•èŽ·å–è®ºæ–‡LRæ–¹æ³•"""
        model = get_paper_method('paper_lr')
        assert isinstance(model, PaperLRModel)
        
        # æµ‹è¯•åˆ«å
        model = get_paper_method('lr_paper')
        assert isinstance(model, PaperLRModel)
        
        model = get_paper_method('paper_method')
        assert isinstance(model, PaperLRModel)
    
    def test_get_paper_method_case_insensitive(self):
        """æµ‹è¯•å¤§å°å†™ä¸æ•æ„Ÿ"""
        model = get_paper_method('PAPER_LR')
        assert isinstance(model, PaperLRModel)
    
    def test_get_paper_method_invalid(self):
        """æµ‹è¯•èŽ·å–ä¸æ”¯æŒçš„æ–¹æ³•"""
        with pytest.raises(ValueError, match="ä¸æ”¯æŒçš„è®ºæ–‡æ–¹æ³•"):
            get_paper_method('invalid_method')


class TestPaperMethodEvaluation:
    """æµ‹è¯•è®ºæ–‡æ–¹æ³•è¯„ä¼°åŠŸèƒ½"""
    
    def setup_method(self):
        """è®¾ç½®æµ‹è¯•æ•°æ®"""
        self.n_samples = 200
        np.random.seed(42)
        
        # åˆ›å»ºåŒ…å«è®ºæ–‡æ–¹æ³•æ‰€éœ€ç‰¹å¾çš„æµ‹è¯•æ•°æ®
        self.features = [
            'Feature2', 'Feature5', 'Feature48', 'Feature49', 'Feature50', 
            'Feature52', 'Feature56', 'Feature57', 'Feature61', 'Feature42', 'Feature43'
        ]
        self.X = pd.DataFrame({
            feature: np.random.normal(0, 1, self.n_samples) for feature in self.features
        })
        self.y = np.random.randint(0, 2, self.n_samples)
        
        # åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•é›†
        split_idx = self.n_samples // 2
        self.X_train = self.X[:split_idx]
        self.y_train = self.y[:split_idx]
        self.X_test = self.X[split_idx:]
        self.y_test = self.y[split_idx:]
    
    def test_evaluate_paper_methods_default(self):
        """æµ‹è¯•é»˜è®¤è¯„ä¼°ï¼ˆpaper_lrï¼‰"""
        results = evaluate_paper_methods(
            self.X_train, self.y_train, self.X_test, self.y_test
        )
        
        assert 'paper_lr' in results
        
        result = results['paper_lr']
        assert 'error' not in result
        assert 'model' in result
        assert 'predictions' in result
        assert 'probabilities' in result
        assert 'metrics' in result
        assert 'risk_scores' in result
        assert 'risk_score_stats' in result
        
        # æ£€æŸ¥æŒ‡æ ‡
        metrics = result['metrics']
        assert 'accuracy' in metrics
        assert 'auc' in metrics
        assert 'f1' in metrics
        assert 'precision' in metrics
        assert 'recall' in metrics
        
        # æ£€æŸ¥æŒ‡æ ‡èŒƒå›´
        assert 0 <= metrics['accuracy'] <= 1
        assert 0 <= metrics['auc'] <= 1
        assert 0 <= metrics['f1'] <= 1
        assert 0 <= metrics['precision'] <= 1
        assert 0 <= metrics['recall'] <= 1
        
        # æ£€æŸ¥é£Žé™©è¯„åˆ†ç»Ÿè®¡
        risk_stats = result['risk_score_stats']
        assert 'mean' in risk_stats
        assert 'std' in risk_stats
        assert 'min' in risk_stats
        assert 'max' in risk_stats
    
    def test_evaluate_paper_methods_specific(self):
        """æµ‹è¯•è¯„ä¼°ç‰¹å®šæ–¹æ³•"""
        results = evaluate_paper_methods(
            self.X_train, self.y_train, self.X_test, self.y_test,
            methods=['lr_paper']
        )
        
        assert len(results) == 1
        assert 'lr_paper' in results
    
    def test_evaluate_paper_methods_with_missing_features(self):
        """æµ‹è¯•ç¼ºå¤±ç‰¹å¾æ—¶çš„è¯„ä¼°"""
        # åªä¿ç•™éƒ¨åˆ†ç‰¹å¾
        X_incomplete = self.X[['Feature2', 'Feature5', 'Feature48']].copy()
        
        results = evaluate_paper_methods(
            X_incomplete, self.y_train, X_incomplete, self.y_test
        )
        
        # åº”è¯¥ä»ç„¶èƒ½è¿è¡Œï¼Œä½†å¯èƒ½æœ‰è­¦å‘Š
        assert 'paper_lr' in results


class TestCompareWithOriginalScript:
    """æµ‹è¯•ä¸ŽåŽŸå§‹è„šæœ¬çš„å¯¹æ¯”åŠŸèƒ½"""
    
    def setup_method(self):
        """è®¾ç½®æµ‹è¯•æ•°æ®"""
        self.n_samples = 100
        np.random.seed(42)
        
        # åˆ›å»ºåŒ…å«è®ºæ–‡æ–¹æ³•æ‰€éœ€ç‰¹å¾çš„æµ‹è¯•æ•°æ®
        self.features = [
            'Feature2', 'Feature5', 'Feature48', 'Feature49', 'Feature50', 
            'Feature52', 'Feature56', 'Feature57', 'Feature61', 'Feature42', 'Feature43'
        ]
        self.X = pd.DataFrame({
            feature: np.random.normal(0, 1, self.n_samples) for feature in self.features
        })
        self.y = np.random.randint(0, 2, self.n_samples)
    
    def test_compare_with_original_lr_script(self):
        """æµ‹è¯•ä¸ŽåŽŸå§‹LRè„šæœ¬çš„å¯¹æ¯”"""
        results = compare_with_original_lr_script(self.X, self.y)
        
        assert 'model_implementation' in results
        assert results['model_implementation'] == 'PaperLRModel'
        
        assert 'metrics' in results
        metrics = results['metrics']
        assert 'accuracy' in metrics
        assert 'auc' in metrics
        assert 'f1' in metrics
        assert 'acc_0' in metrics
        assert 'acc_1' in metrics
        
        assert 'predictions' in results
        assert 'probabilities' in results
        assert 'risk_scores' in results
        assert 'risk_score_stats' in results
        assert 'feature_names' in results
        
        # æ£€æŸ¥æ•°æ®å½¢çŠ¶
        assert len(results['predictions']) == self.n_samples
        assert len(results['probabilities']) == self.n_samples
        assert len(results['risk_scores']) == self.n_samples
        assert results['feature_names'] == self.features


class TestModelPerformanceConsistency:
    """æµ‹è¯•æ¨¡åž‹æ€§èƒ½ä¸€è‡´æ€§"""
    
    def test_paper_lr_model_deterministic(self):
        """æµ‹è¯•è®ºæ–‡LRæ¨¡åž‹çš„ç¡®å®šæ€§"""
        n_samples = 50
        features = [
            'Feature2', 'Feature5', 'Feature48', 'Feature49', 'Feature50', 
            'Feature52', 'Feature56', 'Feature57', 'Feature61', 'Feature42', 'Feature43'
        ]
        
        # åˆ›å»ºå›ºå®šçš„æµ‹è¯•æ•°æ®
        np.random.seed(123)
        X = pd.DataFrame({
            feature: np.random.normal(0, 1, n_samples) for feature in features
        })
        y = np.random.randint(0, 2, n_samples)
        
        # å¤šæ¬¡è¿è¡Œåº”è¯¥å¾—åˆ°ç›¸åŒç»“æžœ
        model1 = PaperLRModel()
        model1.fit(X, y)
        pred1 = model1.predict_proba(X)
        risk1 = model1.get_risk_scores(X)
        
        model2 = PaperLRModel()
        model2.fit(X, y)
        pred2 = model2.predict_proba(X)
        risk2 = model2.get_risk_scores(X)
        
        np.testing.assert_array_almost_equal(pred1, pred2)
        np.testing.assert_array_almost_equal(risk1, risk2)
    
    def test_risk_score_formula_validation(self):
        """æµ‹è¯•é£Žé™©è¯„åˆ†å…¬å¼çš„æ­£ç¡®æ€§"""
        # åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®
        X = pd.DataFrame({
            'Feature2': [1.0],
            'Feature5': [2.0],
            'Feature48': [0.5],
            'Feature49': [1.5],
            'Feature50': [0.8],
            'Feature52': [1.2],
            'Feature56': [0.3],
            'Feature57': [0.7],
            'Feature61': [1.1],
            'Feature42': [0.9],
            'Feature43': [1.3]
        })
        y = np.array([1])
        
        model = PaperLRModel()
        model.fit(X, y)
        
        # æ‰‹åŠ¨è®¡ç®—æœŸæœ›çš„é£Žé™©è¯„åˆ†
        expected_risk = (-1.137 + 
                        0.036 * 1.0 +
                        0.380 * 2.0 +
                        0.195 * 0.5 +
                        0.016 * 1.5 +
                        (-0.290) * 0.8 +
                        0.026 * 1.2 +
                        (-0.168) * 0.3 +
                        (-0.236) * 0.7 +
                        0.052 * 1.1 +
                        0.018 * 0.9 +
                        0.004 * 1.3)
        
        calculated_risk = model.get_risk_scores(X)[0]
        np.testing.assert_almost_equal(calculated_risk, expected_risk, decimal=6)
        
        # éªŒè¯æ¦‚çŽ‡è®¡ç®—
        expected_prob = np.exp(expected_risk) / (1 + np.exp(expected_risk))
        calculated_prob = model.predict_proba(X)[0, 1]
        np.testing.assert_almost_equal(calculated_prob, expected_prob, decimal=6)


def test_paper_methods_integration():
    """é›†æˆæµ‹è¯•ï¼šæµ‹è¯•è®ºæ–‡æ–¹æ³•çš„å®Œæ•´å·¥ä½œæµç¨‹"""
    print("\nðŸ§ª è¿è¡Œè®ºæ–‡æ–¹æ³•é›†æˆæµ‹è¯•...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    n_samples = 100
    np.random.seed(42)
    
    # åŒ…å«è®ºæ–‡æ–¹æ³•éœ€è¦çš„ç‰¹å¾
    features = [
        'Feature2', 'Feature5', 'Feature48', 'Feature49', 'Feature50', 
        'Feature52', 'Feature56', 'Feature57', 'Feature61', 'Feature42', 'Feature43'
    ]
    X = pd.DataFrame({
        feature: np.random.normal(0, 1, n_samples) for feature in features
    })
    y = np.random.randint(0, 2, n_samples)
    
    print(f"æµ‹è¯•æ•°æ®: {X.shape}, æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y)}")
    
    # åˆ†å‰²æ•°æ®
    split_idx = n_samples // 2
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    # æµ‹è¯•è®ºæ–‡æ–¹æ³•
    print(f"\nðŸ“Š æµ‹è¯•è®ºæ–‡LRæ–¹æ³•:")
    
    # åˆ›å»ºæ¨¡åž‹
    model = get_paper_method('paper_lr')
    print(f"  ç‰¹å¾: {model.get_feature_names()}")
    
    # è®­ç»ƒå’Œé¢„æµ‹
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    probabilities = model.predict_proba(X_test)
    risk_scores = model.get_risk_scores(X_test)
    
    # è®¡ç®—æŒ‡æ ‡
    from sklearn.metrics import accuracy_score, roc_auc_score, f1_score
    
    accuracy = accuracy_score(y_test, predictions)
    auc = roc_auc_score(y_test, probabilities[:, 1])
    f1 = f1_score(y_test, predictions, zero_division=0)
    
    print(f"  å‡†ç¡®çŽ‡: {accuracy:.4f}")
    print(f"  AUC: {auc:.4f}")
    print(f"  F1åˆ†æ•°: {f1:.4f}")
    print(f"  é£Žé™©è¯„åˆ†èŒƒå›´: [{risk_scores.min():.3f}, {risk_scores.max():.3f}]")
    print(f"  é£Žé™©è¯„åˆ†å‡å€¼: {risk_scores.mean():.3f}")
    
    print("\nâœ… è®ºæ–‡æ–¹æ³•é›†æˆæµ‹è¯•å®Œæˆ!")
    
    return {
        'accuracy': accuracy,
        'auc': auc,
        'f1': f1,
        'risk_score_stats': {
            'mean': risk_scores.mean(),
            'std': risk_scores.std(),
            'min': risk_scores.min(),
            'max': risk_scores.max()
        }
    }


if __name__ == "__main__":
    # è¿è¡Œé›†æˆæµ‹è¯•
    test_paper_methods_integration()
    
    # è¿è¡Œpytestæµ‹è¯•
    print("\nðŸ§ª è¿è¡Œpytestæµ‹è¯•...")
    pytest.main([__file__, "-v"]) 