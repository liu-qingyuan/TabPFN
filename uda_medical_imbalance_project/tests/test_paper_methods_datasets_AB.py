"""
UDA Medical Imbalance Project - è®ºæ–‡LRæ–¹æ³•åœ¨æ•°æ®é›†Aå’ŒBä¸Šçš„ç‹¬ç«‹æµ‹è¯•

åˆ†åˆ«åœ¨æ•°æ®é›†A (AI4health) å’Œæ•°æ®é›†B (HenanCancerHospital) ä¸Šç‹¬ç«‹æµ‹è¯•è®ºæ–‡LRæ–¹æ³•ã€‚
"""

import pandas as pd
import numpy as np
import sys
from pathlib import Path
from typing import Dict, Tuple, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from modeling.paper_methods import PaperLRModel
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score
import warnings
warnings.filterwarnings('ignore')


def load_datasets() -> Dict[str, pd.DataFrame]:
    """åŠ è½½æ•°æ®é›†Aå’ŒB"""
    print("ğŸ”„ åŠ è½½æ•°æ®é›†...")
    
    # æ•°æ®é›†è·¯å¾„ï¼ˆåŸºäºé¡¹ç›®æ ¹ç›®å½•ï¼‰
    data_dir = project_root.parent / "data"
    
    dataset_paths = {
        'A': data_dir / "AI4healthcare.xlsx",
        'B': data_dir / "HenanCancerHospital_features63_58.xlsx"
    }
    
    datasets = {}
    
    for name, path in dataset_paths.items():
        if path.exists():
            print(f"  ğŸ“‚ åŠ è½½æ•°æ®é›†{name}: {path.name}")
            df = pd.read_excel(path)
            datasets[name] = df
            print(f"     æ ·æœ¬æ•°: {len(df)}, ç‰¹å¾æ•°: {len(df.columns)-1}")
            print(f"     æ ‡ç­¾åˆ†å¸ƒ: {df['Label'].value_counts().to_dict()}")
        else:
            print(f"  âŒ æ•°æ®é›†{name}æ–‡ä»¶ä¸å­˜åœ¨: {path}")
    
    return datasets


def analyze_feature_coverage(datasets: Dict[str, pd.DataFrame]) -> Tuple[list, Dict[str, Dict]]:
    """åˆ†æè®ºæ–‡æ–¹æ³•ç‰¹å¾åœ¨å„æ•°æ®é›†ä¸­çš„è¦†ç›–æƒ…å†µ"""
    print("\nğŸ” åˆ†æç‰¹å¾å…¼å®¹æ€§...")
    
    # è®ºæ–‡æ–¹æ³•éœ€è¦çš„ç‰¹å¾
    paper_features = [
        'Feature2', 'Feature5', 'Feature48', 'Feature49', 'Feature50', 
        'Feature52', 'Feature56', 'Feature57', 'Feature61', 'Feature42', 'Feature43'
    ]
    
    print(f"  ğŸ“‹ è®ºæ–‡LRæ–¹æ³•éœ€è¦çš„ç‰¹å¾: {paper_features}")
    
    # æ£€æŸ¥æ¯ä¸ªæ•°æ®é›†çš„ç‰¹å¾è¦†ç›–æƒ…å†µ
    feature_coverage = {}
    for name, df in datasets.items():
        available_features = [f for f in paper_features if f in df.columns]
        missing_features = [f for f in paper_features if f not in df.columns]
        
        feature_coverage[name] = {
            'available': available_features,
            'missing': missing_features,
            'coverage_rate': len(available_features) / len(paper_features)
        }
        
        print(f"  ğŸ“Š æ•°æ®é›†{name}:")
        print(f"     å¯ç”¨ç‰¹å¾: {len(available_features)}/{len(paper_features)} ({feature_coverage[name]['coverage_rate']:.1%})")
        if missing_features:
            print(f"     ç¼ºå¤±ç‰¹å¾: {missing_features}")
    
    return paper_features, feature_coverage


def test_paper_lr_on_dataset(dataset_name: str, df: pd.DataFrame, 
                            features: list) -> Dict[str, Any]:
    """åœ¨å•ä¸ªæ•°æ®é›†ä¸Šæµ‹è¯•è®ºæ–‡LRæ–¹æ³•"""
    print(f"\nğŸ§ª åœ¨æ•°æ®é›†{dataset_name}ä¸Šæµ‹è¯•è®ºæ–‡LRæ–¹æ³•...")
    
    # å‡†å¤‡æ•°æ® - åªä½¿ç”¨å¯ç”¨çš„ç‰¹å¾
    available_features = [f for f in features if f in df.columns]
    X = df[available_features].copy()
    y = df['Label'].copy()
    
    print(f"  ğŸ“Š ä½¿ç”¨ç‰¹å¾æ•°: {len(available_features)}/{len(features)}")
    print(f"  ğŸ“Š æ ·æœ¬æ•°: {len(X)}")
    print(f"  ğŸ“Š æ­£è´Ÿæ ·æœ¬æ¯”: {y.value_counts().to_dict()}")
    
    # åˆ›å»ºè®ºæ–‡LRæ¨¡å‹ï¼ˆé¢„å®šä¹‰ç³»æ•°ï¼Œä¸éœ€è¦è®­ç»ƒï¼‰
    model = PaperLRModel()
    model.fit(X, y)  # åªæ˜¯æ ‡è®°ä¸ºå·²æ‹Ÿåˆ
    
    # åœ¨å…¨éƒ¨æ•°æ®ä¸Šé¢„æµ‹
    predictions = model.predict(X)
    probabilities = model.predict_proba(X)
    risk_scores = model.get_risk_scores(X)
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    metrics = {
        'dataset': dataset_name,
        'accuracy': accuracy_score(y, predictions),
        'auc': roc_auc_score(y, probabilities[:, 1]),
        'f1': f1_score(y, predictions, zero_division=0),
        'precision': precision_score(y, predictions, zero_division=0),
        'recall': recall_score(y, predictions, zero_division=0),
        'risk_score_mean': risk_scores.mean(),
        'risk_score_std': risk_scores.std(),
        'risk_score_min': risk_scores.min(),
        'risk_score_max': risk_scores.max(),
        'features_used': len(available_features),
        'features_missing': len(features) - len(available_features),
        'sample_count': len(X),
        'positive_samples': (y == 1).sum(),
        'negative_samples': (y == 0).sum()
    }
    
    # è®¡ç®—åˆ†ç±»åˆ«å‡†ç¡®ç‡
    if (y == 0).sum() > 0:
        acc_0 = accuracy_score(y[y == 0], predictions[y == 0])
    else:
        acc_0 = 0.0
    
    if (y == 1).sum() > 0:
        acc_1 = accuracy_score(y[y == 1], predictions[y == 1])
    else:
        acc_1 = 0.0
    
    metrics['acc_negative'] = acc_0
    metrics['acc_positive'] = acc_1
    
    # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
    print(f"  ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
    print(f"     å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
    print(f"     AUC: {metrics['auc']:.4f}")
    print(f"     F1åˆ†æ•°: {metrics['f1']:.4f}")
    print(f"     ç²¾ç¡®ç‡: {metrics['precision']:.4f}")
    print(f"     å¬å›ç‡: {metrics['recall']:.4f}")
    print(f"     è´Ÿç±»å‡†ç¡®ç‡: {metrics['acc_negative']:.4f}")
    print(f"     æ­£ç±»å‡†ç¡®ç‡: {metrics['acc_positive']:.4f}")
    print(f"  ğŸ“Š é£é™©è¯„åˆ†ç»Ÿè®¡:")
    print(f"     å‡å€¼: {metrics['risk_score_mean']:.3f}")
    print(f"     æ ‡å‡†å·®: {metrics['risk_score_std']:.3f}")
    print(f"     èŒƒå›´: [{metrics['risk_score_min']:.3f}, {metrics['risk_score_max']:.3f}]")
    
    return metrics


def print_comparison_summary(results: Dict[str, Dict[str, Any]]) -> None:
    """æ‰“å°æ•°æ®é›†Aå’ŒBçš„å¯¹æ¯”æ‘˜è¦"""
    print("\n" + "="*80)
    print("ğŸ“Š è®ºæ–‡LRæ–¹æ³•åœ¨æ•°æ®é›†Aå’ŒBä¸Šçš„æµ‹è¯•ç»“æœå¯¹æ¯”")
    print("="*80)
    
    # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
    if len(results) >= 2:
        comparison_data = []
        
        for dataset_name, metrics in results.items():
            comparison_data.append({
                'æ•°æ®é›†': dataset_name,
                'å‡†ç¡®ç‡': f"{metrics['accuracy']:.4f}",
                'AUC': f"{metrics['auc']:.4f}", 
                'F1åˆ†æ•°': f"{metrics['f1']:.4f}",
                'ç²¾ç¡®ç‡': f"{metrics['precision']:.4f}",
                'å¬å›ç‡': f"{metrics['recall']:.4f}",
                'è´Ÿç±»å‡†ç¡®ç‡': f"{metrics['acc_negative']:.4f}",
                'æ­£ç±»å‡†ç¡®ç‡': f"{metrics['acc_positive']:.4f}",
                'é£é™©è¯„åˆ†å‡å€¼': f"{metrics['risk_score_mean']:.3f}",
                'ä½¿ç”¨ç‰¹å¾æ•°': f"{metrics['features_used']}/{metrics['features_used'] + metrics['features_missing']}",
                'æ€»æ ·æœ¬æ•°': metrics['sample_count']
            })
        
        comparison_df = pd.DataFrame(comparison_data)
        print(comparison_df.to_string(index=False))
        
        # æ€§èƒ½å¯¹æ¯”åˆ†æ
        print(f"\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”åˆ†æ:")
        
        dataset_names = list(results.keys())
        if len(dataset_names) == 2:
            dataset_A, dataset_B = dataset_names[0], dataset_names[1]
            metrics_A, metrics_B = results[dataset_A], results[dataset_B]
            
            auc_diff = metrics_A['auc'] - metrics_B['auc']
            acc_diff = metrics_A['accuracy'] - metrics_B['accuracy']
            f1_diff = metrics_A['f1'] - metrics_B['f1']
            
            print(f"  ğŸ¯ {dataset_A} vs {dataset_B}:")
            print(f"     AUCå·®å¼‚: {auc_diff:+.4f} ({'Aæ›´å¥½' if auc_diff > 0 else 'Bæ›´å¥½' if auc_diff < 0 else 'ç›¸å½“'})")
            print(f"     å‡†ç¡®ç‡å·®å¼‚: {acc_diff:+.4f} ({'Aæ›´å¥½' if acc_diff > 0 else 'Bæ›´å¥½' if acc_diff < 0 else 'ç›¸å½“'})")
            print(f"     F1å·®å¼‚: {f1_diff:+.4f} ({'Aæ›´å¥½' if f1_diff > 0 else 'Bæ›´å¥½' if f1_diff < 0 else 'ç›¸å½“'})")
            
            # ç‰¹å¾å¯ç”¨æ€§å¯¹æ¯”
            features_A = metrics_A['features_used']
            features_B = metrics_B['features_used']
            print(f"  ğŸ”§ ç‰¹å¾å¯ç”¨æ€§:")
            print(f"     æ•°æ®é›†Aå¯ç”¨ç‰¹å¾: {features_A}/11")
            print(f"     æ•°æ®é›†Bå¯ç”¨ç‰¹å¾: {features_B}/11")
            
            # æ ·æœ¬åˆ†å¸ƒå¯¹æ¯”
            print(f"  ğŸ“Š æ ·æœ¬åˆ†å¸ƒ:")
            print(f"     æ•°æ®é›†A: æ­£æ ·æœ¬{metrics_A['positive_samples']}, è´Ÿæ ·æœ¬{metrics_A['negative_samples']}")
            print(f"     æ•°æ®é›†B: æ­£æ ·æœ¬{metrics_B['positive_samples']}, è´Ÿæ ·æœ¬{metrics_B['negative_samples']}")


def save_results(results: Dict[str, Dict[str, Any]], output_dir: str) -> None:
    """ä¿å­˜æµ‹è¯•ç»“æœ"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_path}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœè¡¨æ ¼
    summary_data = []
    for dataset_name, metrics in results.items():
        summary_data.append({
            'dataset': dataset_name,
            **metrics
        })
    
    summary_df = pd.DataFrame(summary_data)
    summary_path = output_path / "paper_lr_results_datasets_AB.csv"
    summary_df.to_csv(summary_path, index=False, encoding='utf-8')
    print(f"  ğŸ“„ è¯¦ç»†ç»“æœä¿å­˜è‡³: {summary_path}")
    
    # ä¿å­˜JSONæ ¼å¼çš„ç»“æœ
    import json
    json_path = output_path / "paper_lr_results_datasets_AB.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    print(f"  ğŸ“„ JSONç»“æœä¿å­˜è‡³: {json_path}")


def main() -> Dict[str, Dict[str, Any]]:
    """ä¸»å‡½æ•°"""
    print("ğŸš€ è®ºæ–‡LRæ–¹æ³•åœ¨æ•°æ®é›†Aå’ŒBä¸Šçš„ç‹¬ç«‹æµ‹è¯•")
    print("="*80)
    
    # åŠ è½½æ•°æ®é›†
    datasets = load_datasets()
    
    if not datasets:
        print("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„æ•°æ®é›†æ–‡ä»¶")
        return {}
    
    # åˆ†æç‰¹å¾è¦†ç›–æƒ…å†µ
    paper_features, feature_coverage = analyze_feature_coverage(datasets)
    
    results = {}
    
    # åœ¨å„æ•°æ®é›†ä¸Šç‹¬ç«‹æµ‹è¯•è®ºæ–‡LRæ–¹æ³•
    for name, df in datasets.items():
        if feature_coverage[name]['coverage_rate'] > 0:
            metrics = test_paper_lr_on_dataset(name, df, paper_features)
            results[f'æ•°æ®é›†{name}'] = metrics
        else:
            print(f"âš ï¸  æ•°æ®é›†{name}ç¼ºå¤±æ‰€æœ‰å¿…è¦ç‰¹å¾ï¼Œè·³è¿‡æµ‹è¯•")
    
    # æ˜¾ç¤ºå¯¹æ¯”æ‘˜è¦
    if results:
        print_comparison_summary(results)
        
        # ä¿å­˜ç»“æœ
        save_results(results, "tests/results_paper_lr_AB")
        
        print("\nâœ… æµ‹è¯•å®Œæˆ!")
    else:
        print("\nâŒ æ— å¯ç”¨æµ‹è¯•ç»“æœ")
    
    return results


if __name__ == "__main__":
    results = main()
    
    # ä¸ºæ–¹ä¾¿è°ƒè¯•ï¼Œè¾“å‡ºå…³é”®æŒ‡æ ‡
    if results:
        print(f"\nğŸ¯ å…³é”®æŒ‡æ ‡æ‘˜è¦:")
        for name, metrics in results.items():
            print(f"  {name}: AUC={metrics['auc']:.4f}, å‡†ç¡®ç‡={metrics['accuracy']:.4f}, F1={metrics['f1']:.4f}") 