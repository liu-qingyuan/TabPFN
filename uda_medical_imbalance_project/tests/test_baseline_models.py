"""
UDA Medical Imbalance Project - åŸºçº¿æ¨¡åž‹æµ‹è¯•

æµ‹è¯•PKUPHå’ŒMayoæ¨¡åž‹çš„åŠŸèƒ½å’Œæ€§èƒ½ã€‚
"""

import pytest
import numpy as np
import pandas as pd
import os
import sys
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from modeling.baseline_models import PKUPHModel, MayoModel, get_baseline_model, evaluate_baseline_models


class TestPKUPHModel:
    """æµ‹è¯•PKUPHæ¨¡åž‹"""
    
    def setup_method(self):
        """è®¾ç½®æµ‹è¯•æ•°æ®"""
        self.model = PKUPHModel()
        self.n_samples = 100
        np.random.seed(42)
        
        # åˆ›å»ºåŒ…å«PKUPHæ‰€éœ€ç‰¹å¾çš„æµ‹è¯•æ•°æ®
        self.features = ['Feature2', 'Feature48', 'Feature49', 'Feature4', 'Feature50', 'Feature53']
        self.X = pd.DataFrame({
            feature: np.random.normal(0, 1, self.n_samples) for feature in self.features
        })
        self.y = np.random.randint(0, 2, self.n_samples)
    
    def test_model_initialization(self):
        """æµ‹è¯•æ¨¡åž‹åˆå§‹åŒ–"""
        assert self.model.intercept_ == -4.496
        assert len(self.model.features) == 6
        assert len(self.model.coefficients) == 6
        assert not self.model.is_fitted_
    
    def test_model_fit(self):
        """æµ‹è¯•æ¨¡åž‹æ‹Ÿåˆ"""
        fitted_model = self.model.fit(self.X, self.y)
        assert fitted_model is self.model
        assert self.model.is_fitted_
    
    def test_predict_proba_before_fit(self):
        """æµ‹è¯•æœªæ‹Ÿåˆæ—¶é¢„æµ‹æ¦‚çŽ‡åº”è¯¥æŠ¥é”™"""
        with pytest.raises(ValueError, match="æ¨¡åž‹å°šæœªæ‹Ÿåˆ"):
            self.model.predict_proba(self.X)
    
    def test_predict_proba_after_fit(self):
        """æµ‹è¯•æ‹ŸåˆåŽé¢„æµ‹æ¦‚çŽ‡"""
        self.model.fit(self.X, self.y)
        proba = self.model.predict_proba(self.X)
        
        assert proba.shape == (self.n_samples, 2)
        assert np.all(proba >= 0) and np.all(proba <= 1)
        assert np.allclose(proba.sum(axis=1), 1.0)
    
    def test_predict_after_fit(self):
        """æµ‹è¯•æ‹ŸåˆåŽé¢„æµ‹ç±»åˆ«"""
        self.model.fit(self.X, self.y)
        predictions = self.model.predict(self.X)
        
        assert predictions.shape == (self.n_samples,)
        assert np.all(np.isin(predictions, [0, 1]))
    
    def test_get_feature_names(self):
        """æµ‹è¯•èŽ·å–ç‰¹å¾åç§°"""
        feature_names = self.model.get_feature_names()
        assert feature_names == self.features
        assert feature_names is not self.model.features  # åº”è¯¥è¿”å›žå‰¯æœ¬
    
    def test_numpy_input(self):
        """æµ‹è¯•numpyæ•°ç»„è¾“å…¥"""
        self.model.fit(self.X, self.y)
        X_numpy = self.X.values
        
        proba = self.model.predict_proba(X_numpy)
        predictions = self.model.predict(X_numpy)
        
        assert proba.shape == (self.n_samples, 2)
        assert predictions.shape == (self.n_samples,)
    
    def test_missing_features_warning(self, capsys):
        """æµ‹è¯•ç¼ºå¤±ç‰¹å¾æ—¶çš„è­¦å‘Š"""
        # åˆ›å»ºç¼ºå°‘æŸäº›ç‰¹å¾çš„æ•°æ®
        X_incomplete = self.X[['Feature2', 'Feature48']].copy()
        self.model.fit(X_incomplete, self.y[:len(X_incomplete)])
        
        self.model.predict_proba(X_incomplete)
        captured = capsys.readouterr()
        assert "è­¦å‘Š" in captured.out


class TestMayoModel:
    """æµ‹è¯•Mayoæ¨¡åž‹"""
    
    def setup_method(self):
        """è®¾ç½®æµ‹è¯•æ•°æ®"""
        self.model = MayoModel()
        self.n_samples = 100
        np.random.seed(42)
        
        # åˆ›å»ºåŒ…å«Mayoæ‰€éœ€ç‰¹å¾çš„æµ‹è¯•æ•°æ®
        self.features = ['Feature2', 'Feature3', 'Feature5', 'Feature48', 'Feature49', 'Feature63']
        self.X = pd.DataFrame({
            feature: np.random.normal(0, 1, self.n_samples) for feature in self.features
        })
        self.y = np.random.randint(0, 2, self.n_samples)
    
    def test_model_initialization(self):
        """æµ‹è¯•æ¨¡åž‹åˆå§‹åŒ–"""
        assert self.model.intercept_ == -6.8272
        assert len(self.model.features) == 6
        assert len(self.model.coefficients) == 6
        assert not self.model.is_fitted_
    
    def test_model_fit(self):
        """æµ‹è¯•æ¨¡åž‹æ‹Ÿåˆ"""
        fitted_model = self.model.fit(self.X, self.y)
        assert fitted_model is self.model
        assert self.model.is_fitted_
    
    def test_predict_proba_after_fit(self):
        """æµ‹è¯•æ‹ŸåˆåŽé¢„æµ‹æ¦‚çŽ‡"""
        self.model.fit(self.X, self.y)
        proba = self.model.predict_proba(self.X)
        
        assert proba.shape == (self.n_samples, 2)
        assert np.all(proba >= 0) and np.all(proba <= 1)
        assert np.allclose(proba.sum(axis=1), 1.0)
    
    def test_predict_after_fit(self):
        """æµ‹è¯•æ‹ŸåˆåŽé¢„æµ‹ç±»åˆ«"""
        self.model.fit(self.X, self.y)
        predictions = self.model.predict(self.X)
        
        assert predictions.shape == (self.n_samples,)
        assert np.all(np.isin(predictions, [0, 1]))
    
    def test_get_feature_names(self):
        """æµ‹è¯•èŽ·å–ç‰¹å¾åç§°"""
        feature_names = self.model.get_feature_names()
        assert feature_names == self.features
        assert feature_names is not self.model.features  # åº”è¯¥è¿”å›žå‰¯æœ¬


class TestBaselineModelFactory:
    """æµ‹è¯•åŸºçº¿æ¨¡åž‹å·¥åŽ‚å‡½æ•°"""
    
    def test_get_baseline_model_pkuph(self):
        """æµ‹è¯•èŽ·å–PKUPHæ¨¡åž‹"""
        model = get_baseline_model('pkuph')
        assert isinstance(model, PKUPHModel)
        
        model = get_baseline_model('PKUPH')  # æµ‹è¯•å¤§å°å†™ä¸æ•æ„Ÿ
        assert isinstance(model, PKUPHModel)
    
    def test_get_baseline_model_mayo(self):
        """æµ‹è¯•èŽ·å–Mayoæ¨¡åž‹"""
        model = get_baseline_model('mayo')
        assert isinstance(model, MayoModel)
        
        model = get_baseline_model('MAYO')  # æµ‹è¯•å¤§å°å†™ä¸æ•æ„Ÿ
        assert isinstance(model, MayoModel)
    
    def test_get_baseline_model_invalid(self):
        """æµ‹è¯•èŽ·å–ä¸æ”¯æŒçš„æ¨¡åž‹"""
        with pytest.raises(ValueError, match="ä¸æ”¯æŒçš„åŸºçº¿æ¨¡åž‹"):
            get_baseline_model('invalid_model')


class TestBaselineModelEvaluation:
    """æµ‹è¯•åŸºçº¿æ¨¡åž‹è¯„ä¼°åŠŸèƒ½"""
    
    def setup_method(self):
        """è®¾ç½®æµ‹è¯•æ•°æ®"""
        self.n_samples = 200
        np.random.seed(42)
        
        # åˆ›å»ºåŒ…å«æ‰€æœ‰åŸºçº¿æ¨¡åž‹ç‰¹å¾çš„æµ‹è¯•æ•°æ®
        all_features = ['Feature2', 'Feature3', 'Feature4', 'Feature5', 
                       'Feature48', 'Feature49', 'Feature50', 'Feature53', 'Feature63']
        self.X = pd.DataFrame({
            feature: np.random.normal(0, 1, self.n_samples) for feature in all_features
        })
        self.y = np.random.randint(0, 2, self.n_samples)
        
        # åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•é›†
        split_idx = self.n_samples // 2
        self.X_train = self.X[:split_idx]
        self.y_train = self.y[:split_idx]
        self.X_test = self.X[split_idx:]
        self.y_test = self.y[split_idx:]
    
    def test_evaluate_baseline_models_default(self):
        """æµ‹è¯•é»˜è®¤è¯„ä¼°ï¼ˆPKUPHå’ŒMayoï¼‰"""
        results = evaluate_baseline_models(
            self.X_train, self.y_train, self.X_test, self.y_test
        )
        
        assert 'pkuph' in results
        assert 'mayo' in results
        
        for model_name, result in results.items():
            assert 'error' not in result
            assert 'model' in result
            assert 'predictions' in result
            assert 'probabilities' in result
            assert 'metrics' in result
            
            # æ£€æŸ¥æŒ‡æ ‡
            metrics = result['metrics']
            assert 'accuracy' in metrics
            assert 'auc' in metrics
            assert 'f1' in metrics
            assert 'precision' in metrics
            assert 'recall' in metrics
            
            # æ£€æŸ¥æŒ‡æ ‡èŒƒå›´
            assert 0 <= metrics['accuracy'] <= 1
            assert 0 <= metrics['auc'] <= 1
            assert 0 <= metrics['f1'] <= 1
            assert 0 <= metrics['precision'] <= 1
            assert 0 <= metrics['recall'] <= 1
    
    def test_evaluate_baseline_models_specific(self):
        """æµ‹è¯•è¯„ä¼°ç‰¹å®šæ¨¡åž‹"""
        results = evaluate_baseline_models(
            self.X_train, self.y_train, self.X_test, self.y_test,
            models=['pkuph']
        )
        
        assert len(results) == 1
        assert 'pkuph' in results
        assert 'mayo' not in results
    
    def test_evaluate_baseline_models_with_missing_features(self):
        """æµ‹è¯•ç¼ºå¤±ç‰¹å¾æ—¶çš„è¯„ä¼°"""
        # åªä¿ç•™éƒ¨åˆ†ç‰¹å¾
        X_incomplete = self.X[['Feature2', 'Feature48']].copy()
        
        results = evaluate_baseline_models(
            X_incomplete, self.y_train, X_incomplete, self.y_test
        )
        
        # åº”è¯¥ä»ç„¶èƒ½è¿è¡Œï¼Œä½†å¯èƒ½æœ‰è­¦å‘Š
        assert 'pkuph' in results
        assert 'mayo' in results


class TestModelPerformanceConsistency:
    """æµ‹è¯•æ¨¡åž‹æ€§èƒ½ä¸€è‡´æ€§"""
    
    def test_pkuph_model_deterministic(self):
        """æµ‹è¯•PKUPHæ¨¡åž‹çš„ç¡®å®šæ€§"""
        n_samples = 50
        features = ['Feature2', 'Feature48', 'Feature49', 'Feature4', 'Feature50', 'Feature53']
        
        # åˆ›å»ºå›ºå®šçš„æµ‹è¯•æ•°æ®
        np.random.seed(123)
        X = pd.DataFrame({
            feature: np.random.normal(0, 1, n_samples) for feature in features
        })
        y = np.random.randint(0, 2, n_samples)
        
        # å¤šæ¬¡è¿è¡Œåº”è¯¥å¾—åˆ°ç›¸åŒç»“æžœ
        model1 = PKUPHModel()
        model1.fit(X, y)
        pred1 = model1.predict_proba(X)
        
        model2 = PKUPHModel()
        model2.fit(X, y)
        pred2 = model2.predict_proba(X)
        
        np.testing.assert_array_almost_equal(pred1, pred2)
    
    def test_mayo_model_deterministic(self):
        """æµ‹è¯•Mayoæ¨¡åž‹çš„ç¡®å®šæ€§"""
        n_samples = 50
        features = ['Feature2', 'Feature3', 'Feature5', 'Feature48', 'Feature49', 'Feature63']
        
        # åˆ›å»ºå›ºå®šçš„æµ‹è¯•æ•°æ®
        np.random.seed(123)
        X = pd.DataFrame({
            feature: np.random.normal(0, 1, n_samples) for feature in features
        })
        y = np.random.randint(0, 2, n_samples)
        
        # å¤šæ¬¡è¿è¡Œåº”è¯¥å¾—åˆ°ç›¸åŒç»“æžœ
        model1 = MayoModel()
        model1.fit(X, y)
        pred1 = model1.predict_proba(X)
        
        model2 = MayoModel()
        model2.fit(X, y)
        pred2 = model2.predict_proba(X)
        
        np.testing.assert_array_almost_equal(pred1, pred2)


def test_baseline_models_integration():
    """é›†æˆæµ‹è¯•ï¼šæµ‹è¯•åŸºçº¿æ¨¡åž‹çš„å®Œæ•´å·¥ä½œæµç¨‹"""
    print("\nðŸ§ª è¿è¡ŒåŸºçº¿æ¨¡åž‹é›†æˆæµ‹è¯•...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    n_samples = 100
    np.random.seed(42)
    
    # åŒ…å«æ‰€æœ‰åŸºçº¿æ¨¡åž‹éœ€è¦çš„ç‰¹å¾
    all_features = ['Feature2', 'Feature3', 'Feature4', 'Feature5', 
                   'Feature48', 'Feature49', 'Feature50', 'Feature53', 'Feature63']
    X = pd.DataFrame({
        feature: np.random.normal(0, 1, n_samples) for feature in all_features
    })
    y = np.random.randint(0, 2, n_samples)
    
    print(f"æµ‹è¯•æ•°æ®: {X.shape}, æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y)}")
    
    # åˆ†å‰²æ•°æ®
    split_idx = n_samples // 2
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    # æµ‹è¯•æ¯ä¸ªæ¨¡åž‹
    models = ['pkuph', 'mayo']
    results = {}
    
    for model_name in models:
        print(f"\nðŸ“Š æµ‹è¯• {model_name.upper()} æ¨¡åž‹:")
        
        # åˆ›å»ºæ¨¡åž‹
        model = get_baseline_model(model_name)
        print(f"  ç‰¹å¾: {model.get_feature_names()}")
        
        # è®­ç»ƒå’Œé¢„æµ‹
        model.fit(X_train, y_train)
        predictions = model.predict(X_test)
        probabilities = model.predict_proba(X_test)
        
        # è®¡ç®—æŒ‡æ ‡
        from sklearn.metrics import accuracy_score, roc_auc_score, f1_score
        
        accuracy = accuracy_score(y_test, predictions)
        auc = roc_auc_score(y_test, probabilities[:, 1])
        f1 = f1_score(y_test, predictions, zero_division=0)
        
        print(f"  å‡†ç¡®çŽ‡: {accuracy:.4f}")
        print(f"  AUC: {auc:.4f}")
        print(f"  F1åˆ†æ•°: {f1:.4f}")
        
        results[model_name] = {
            'accuracy': accuracy,
            'auc': auc,
            'f1': f1
        }
    
    # ä½¿ç”¨è¯„ä¼°å‡½æ•°è¿›è¡Œå¯¹æ¯”
    print(f"\nðŸ”„ ä½¿ç”¨è¯„ä¼°å‡½æ•°è¿›è¡Œå¯¹æ¯”:")
    eval_results = evaluate_baseline_models(X_train, y_train, X_test, y_test)
    
    for model_name, result in eval_results.items():
        if 'error' not in result:
            metrics = result['metrics']
            print(f"  {model_name.upper()}: AUC={metrics['auc']:.4f}, "
                  f"ACC={metrics['accuracy']:.4f}, F1={metrics['f1']:.4f}")
        else:
            print(f"  {model_name.upper()}: é”™è¯¯ - {result['error']}")
    
    print("\nâœ… åŸºçº¿æ¨¡åž‹é›†æˆæµ‹è¯•å®Œæˆ!")
    return results


if __name__ == "__main__":
    # è¿è¡Œé›†æˆæµ‹è¯•
    test_baseline_models_integration()
    
    # è¿è¡Œpytestæµ‹è¯•
    print("\nðŸ§ª è¿è¡Œpytestæµ‹è¯•...")
    pytest.main([__file__, "-v"]) 