#!/usr/bin/env python3
"""
ç‰¹å¾æ‰«æåˆ†æè„šæœ¬ - è¿è¡Œä»best3åˆ°best58çš„æ‰€æœ‰ç‰¹å¾ç»„åˆåˆ†æ

è¿™ä¸ªè„šæœ¬è‡ªåŠ¨è¿è¡Œæ‰€æœ‰å¯èƒ½çš„ç‰¹å¾é…ç½®ï¼Œç”Ÿæˆæ€§èƒ½æ¯”è¾ƒå¯è§†åŒ–ï¼Œ
ä¾¿äºæ‰¾åˆ°æœ€ä¼˜çš„ç‰¹å¾ç»„åˆè¿›è¡ŒåŒ»ç–—æ•°æ®åŸŸé€‚åº”åˆ†æã€‚

è¿è¡Œç¤ºä¾‹: python scripts/run_feature_sweep_analysis.py
"""

import sys
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from datetime import datetime
import json
from typing import Dict, List, Tuple, Optional
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import logging

warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“å’Œæ ·å¼
plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.dpi'] = 150
plt.style.use('seaborn-v0_8-whitegrid')

from scripts.run_complete_analysis import CompleteAnalysisRunner

# Direct settings import to avoid yaml dependency
import importlib.util
def load_settings_direct():
    """Load settings module directly without complex imports"""
    settings_path = project_root / "config" / "settings.py"
    spec = importlib.util.spec_from_file_location("settings", settings_path)
    settings = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(settings)
    return settings

def get_features_by_type(feature_type: str):
    """Get features by type without complex imports - support all best3-best58"""
    settings = load_settings_direct()
    
    # ç›´æ¥ä½¿ç”¨settingsæ¨¡å—çš„get_features_by_typeå‡½æ•°
    try:
        return settings.get_features_by_type(feature_type)
    except Exception:
        # å¤‡é€‰æ–¹æ¡ˆï¼šå¦‚æœsettings.pyçš„get_features_by_typeå‡½æ•°å¤±è´¥ï¼Œå°è¯•ç›´æ¥è®¿é—®å±æ€§
        try:
            # åŠ¨æ€è·å–ç‰¹å¾é›†å±æ€§
            if feature_type == 'all63':
                return getattr(settings, 'ALL_63_FEATURES', [])
            elif feature_type == 'selected58':
                return getattr(settings, 'SELECTED_58_FEATURES', [])
            else:
                # å¯¹äºbestNç‰¹å¾é›†ï¼Œå°è¯•è·å–å¯¹åº”çš„å±æ€§
                attr_name = f"{feature_type.upper()}_FEATURES"
                return getattr(settings, attr_name, [])
        except Exception:
            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
            return []


class FeatureSweepAnalyzer:
    """ç‰¹å¾æ‰«æåˆ†æå™¨"""
    
    def __init__(
        self,
        feature_range: Tuple[int, int] = (3, 58),
        output_dir: Optional[str] = None,
        max_workers: int = None,
        verbose: bool = True
    ):
        """
        åˆå§‹åŒ–ç‰¹å¾æ‰«æåˆ†æå™¨
        
        Args:
            feature_range: ç‰¹å¾æ•°é‡èŒƒå›´ (min_features, max_features)
            output_dir: è¾“å‡ºç›®å½•
            max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        """
        self.feature_range = feature_range
        self.verbose = verbose
        
        # è®¾ç½®å¹¶è¡Œå·¥ä½œæ•°
        if max_workers is None:
            self.max_workers = min(4, mp.cpu_count() // 2)  # ä¿å®ˆçš„å¹¶è¡Œåº¦
        else:
            self.max_workers = max_workers
            
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if output_dir is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = f"results/feature_sweep_analysis_{timestamp}"
        
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        self.results_dir = self.output_dir / "individual_results"
        self.results_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        
        # å­˜å‚¨ç»“æœ
        self.sweep_results = {}
        self.performance_summary = {}
        
        if self.verbose:
            print(f"ğŸ”§ ç‰¹å¾æ‰«æåˆ†æå™¨åˆå§‹åŒ–")
            print(f"   ç‰¹å¾èŒƒå›´: best{feature_range[0]} ~ best{feature_range[1]}")
            print(f"   è¾“å‡ºç›®å½•: {output_dir}")
            print(f"   æœ€å¤§å¹¶è¡Œæ•°: {self.max_workers}")
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_file = self.output_dir / "feature_sweep.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler() if self.verbose else logging.NullHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def get_available_feature_sets(self) -> List[str]:
        """è·å–å¯ç”¨çš„ç‰¹å¾é›†åˆ—è¡¨"""
        available_sets = []
        min_features, max_features = self.feature_range
        
        for n_features in range(min_features, max_features + 1):
            feature_type = f"best{n_features}"
            try:
                features = get_features_by_type(feature_type)
                if features:
                    available_sets.append(feature_type)
                    if self.verbose:
                        print(f"âœ… {feature_type}: {len(features)}ä¸ªç‰¹å¾")
                else:
                    if self.verbose:
                        print(f"âš ï¸ {feature_type}: æœªå®šä¹‰")
            except Exception as e:
                if self.verbose:
                    print(f"âŒ {feature_type}: {e}")
        
        return available_sets
    
    def run_single_feature_analysis(self, feature_type: str) -> Dict:
        """è¿è¡Œå•ä¸ªç‰¹å¾é›†çš„åˆ†æ"""
        try:
            self.logger.info(f"å¼€å§‹åˆ†æç‰¹å¾é›†: {feature_type}")
            
            # åˆ›å»ºä¸ªä½“ç»“æœç›®å½•
            individual_output_dir = self.results_dir / feature_type
            
            # åˆ›å»ºåˆ†æè¿è¡Œå™¨
            runner = CompleteAnalysisRunner(
                feature_type=feature_type,
                scaler_type='none',  # ä½¿ç”¨é»˜è®¤é…ç½®
                imbalance_method='none',
                cv_folds=10,
                random_state=42,
                output_dir=str(individual_output_dir),
                verbose=False  # å…³é—­è¯¦ç»†è¾“å‡ºé¿å…æ—¥å¿—æ··ä¹±
            )
            
            # è¿è¡Œå®Œæ•´åˆ†æ
            results = runner.run_complete_analysis()
            
            if 'error' in results:
                self.logger.error(f"ç‰¹å¾é›† {feature_type} åˆ†æå¤±è´¥: {results['error']}")
                return {
                    'feature_type': feature_type,
                    'status': 'failed',
                    'error': results['error'],
                    'n_features': int(feature_type.replace('best', ''))
                }
            
            # æå–æ€§èƒ½æŒ‡æ ‡
            performance = self.extract_performance_metrics(results, feature_type)
            
            self.logger.info(f"ç‰¹å¾é›† {feature_type} åˆ†æå®Œæˆ")
            return performance
            
        except Exception as e:
            self.logger.error(f"ç‰¹å¾é›† {feature_type} æ‰§è¡Œå¼‚å¸¸: {e}")
            return {
                'feature_type': feature_type,
                'status': 'failed',
                'error': str(e),
                'n_features': int(feature_type.replace('best', ''))
            }
    
    def extract_performance_metrics(self, results: Dict, feature_type: str) -> Dict:
        """æå–æ€§èƒ½æŒ‡æ ‡"""
        performance = {
            'feature_type': feature_type,
            'n_features': int(feature_type.replace('best', '')),
            'status': 'success'
        }
        
        # 1. æå–æºåŸŸäº¤å‰éªŒè¯ç»“æœ (TabPFN)
        if 'source_domain_cv' in results:
            cv_results = results['source_domain_cv']
            
            # æ‰¾åˆ°TabPFNç»“æœ
            tabpfn_key = None
            for key in cv_results.keys():
                if 'tabpfn' in key.lower():
                    tabpfn_key = key
                    break
            
            if tabpfn_key and 'summary' in cv_results[tabpfn_key]:
                summary = cv_results[tabpfn_key]['summary']
                performance.update({
                    'source_auc': summary.get('auc_mean', 0),
                    'source_accuracy': summary.get('accuracy_mean', 0),
                    'source_f1': summary.get('f1_mean', 0),
                    'source_precision': summary.get('precision_mean', 0),
                    'source_recall': summary.get('recall_mean', 0),
                    'source_auc_std': summary.get('auc_std', 0)
                })
        
        # 2. æå–UDAç»“æœ
        if 'uda_methods' in results:
            uda_results = results['uda_methods']
            
            # TabPFNæ— UDAåŸºçº¿
            if 'TabPFN_NoUDA' in uda_results:
                baseline = uda_results['TabPFN_NoUDA']
                if 'error' not in baseline:
                    performance.update({
                        'target_baseline_auc': baseline.get('auc', 0),
                        'target_baseline_accuracy': baseline.get('accuracy', 0),
                        'target_baseline_f1': baseline.get('f1', 0)
                    })
            
            # TCAç»“æœ
            if 'TCA' in uda_results:
                tca = uda_results['TCA']
                if 'error' not in tca:
                    performance.update({
                        'target_tca_auc': tca.get('auc', 0),
                        'target_tca_accuracy': tca.get('accuracy', 0),
                        'target_tca_f1': tca.get('f1', 0)
                    })

                    # è®¡ç®—TCAç›¸å¯¹äºåŸºçº¿çš„æå‡
                    if 'target_baseline_auc' in performance and performance['target_baseline_auc'] > 0:
                        improvement = performance['target_tca_auc'] - performance['target_baseline_auc']
                        performance['tca_auc_improvement'] = improvement

            # SAç»“æœ
            if 'SA' in uda_results:
                sa = uda_results['SA']
                if 'error' not in sa:
                    performance.update({
                        'target_sa_auc': sa.get('auc', 0),
                        'target_sa_accuracy': sa.get('accuracy', 0),
                        'target_sa_f1': sa.get('f1', 0)
                    })

                    # è®¡ç®—SAç›¸å¯¹äºåŸºçº¿çš„æå‡
                    if 'target_baseline_auc' in performance and performance['target_baseline_auc'] > 0:
                        improvement = performance['target_sa_auc'] - performance['target_baseline_auc']
                        performance['sa_auc_improvement'] = improvement

            # CORALç»“æœ
            if 'CORAL' in uda_results:
                coral = uda_results['CORAL']
                if 'error' not in coral:
                    performance.update({
                        'target_coral_auc': coral.get('auc', 0),
                        'target_coral_accuracy': coral.get('accuracy', 0),
                        'target_coral_f1': coral.get('f1', 0)
                    })

                    # è®¡ç®—CORALç›¸å¯¹äºåŸºçº¿çš„æå‡
                    if 'target_baseline_auc' in performance and performance['target_baseline_auc'] > 0:
                        improvement = performance['target_coral_auc'] - performance['target_baseline_auc']
                        performance['coral_auc_improvement'] = improvement

            # KMMç»“æœ
            if 'KMM' in uda_results:
                kmm = uda_results['KMM']
                if 'error' not in kmm:
                    performance.update({
                        'target_kmm_auc': kmm.get('auc', 0),
                        'target_kmm_accuracy': kmm.get('accuracy', 0),
                        'target_kmm_f1': kmm.get('f1', 0)
                    })

                    # è®¡ç®—KMMç›¸å¯¹äºåŸºçº¿çš„æå‡
                    if 'target_baseline_auc' in performance and performance['target_baseline_auc'] > 0:
                        improvement = performance['target_kmm_auc'] - performance['target_baseline_auc']
                        performance['kmm_auc_improvement'] = improvement
        
        return performance
    
    def run_parallel_feature_sweep(self) -> Dict:
        """å¹¶è¡Œè¿è¡Œç‰¹å¾æ‰«æåˆ†æ"""
        if self.verbose:
            print(f"\nğŸš€ å¼€å§‹å¹¶è¡Œç‰¹å¾æ‰«æåˆ†æ")
            print("=" * 50)
        
        # è·å–å¯ç”¨ç‰¹å¾é›†
        available_sets = self.get_available_feature_sets()
        
        if not available_sets:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„ç‰¹å¾é›†é…ç½®")
        
        if self.verbose:
            print(f"ğŸ“Š å°†åˆ†æ {len(available_sets)} ä¸ªç‰¹å¾é›†")
            print(f"ğŸ”„ ä½¿ç”¨ {self.max_workers} ä¸ªå¹¶è¡Œè¿›ç¨‹")
        
        # å¹¶è¡Œæ‰§è¡Œåˆ†æ
        results = []
        
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_feature = {
                executor.submit(self.run_single_feature_analysis, feature_type): feature_type 
                for feature_type in available_sets
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_feature):
                feature_type = future_to_feature[future]
                try:
                    result = future.result()
                    results.append(result)
                    
                    status = "âœ…" if result['status'] == 'success' else "âŒ"
                    if self.verbose:
                        print(f"{status} {feature_type} å®Œæˆ")
                        
                except Exception as e:
                    self.logger.error(f"å¤„ç† {feature_type} ç»“æœæ—¶å‡ºé”™: {e}")
                    results.append({
                        'feature_type': feature_type,
                        'status': 'failed',
                        'error': str(e),
                        'n_features': int(feature_type.replace('best', ''))
                    })
        
        # æ•´ç†ç»“æœ
        self.sweep_results = {r['feature_type']: r for r in results}
        
        # ç”Ÿæˆæ€§èƒ½æ€»ç»“
        self.performance_summary = self.generate_performance_summary()
        
        return self.sweep_results
    
    def generate_performance_summary(self) -> pd.DataFrame:
        """ç”Ÿæˆæ€§èƒ½æ€»ç»“è¡¨"""
        summary_data = []
        
        for feature_type, result in self.sweep_results.items():
            if result['status'] == 'success':
                summary_data.append({
                    'feature_type': feature_type,
                    'n_features': result['n_features'],
                    'source_auc': result.get('source_auc', 0),
                    'source_accuracy': result.get('source_accuracy', 0),
                    'source_f1': result.get('source_f1', 0),
                    'target_baseline_auc': result.get('target_baseline_auc', 0),
                    'target_tca_auc': result.get('target_tca_auc', 0),
                    'target_sa_auc': result.get('target_sa_auc', 0),
                    'target_coral_auc': result.get('target_coral_auc', 0),
                    'target_kmm_auc': result.get('target_kmm_auc', 0),
                    'tca_improvement': result.get('tca_auc_improvement', 0),
                    'sa_improvement': result.get('sa_auc_improvement', 0),
                    'coral_improvement': result.get('coral_auc_improvement', 0),
                    'kmm_improvement': result.get('kmm_auc_improvement', 0)
                })
        
        if summary_data:
            df = pd.DataFrame(summary_data).sort_values('n_features')
            return df
        else:
            return pd.DataFrame()
    
    def plot_performance_comparison(self) -> str:
        """ç»˜åˆ¶æ€§èƒ½æ¯”è¾ƒå›¾è¡¨"""
        if self.verbose:
            print(f"\nğŸ“Š ç”Ÿæˆæ€§èƒ½æ¯”è¾ƒå¯è§†åŒ–")
        
        # å‡†å¤‡æ•°æ®
        df = self.performance_summary
        
        if df.empty:
            self.logger.warning("æ²¡æœ‰æˆåŠŸçš„ç»“æœå¯ç”¨äºå¯è§†åŒ–")
            return ""
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Feature Selection Performance Analysis\n(Best3 ~ Best58)', 
                     fontsize=16, fontweight='bold', y=0.98)
        
        # 1. AUCæ€§èƒ½æ›²çº¿ (å·¦ä¸Š)
        ax1 = axes[0, 0]
        
        # ç»˜åˆ¶æºåŸŸå’Œç›®æ ‡åŸŸAUC
        ax1.plot(df['n_features'], df['source_auc'], 'b-o',
                label='Source Domain (10-fold CV)', linewidth=2, markersize=5)
        ax1.plot(df['n_features'], df['target_baseline_auc'], 'r-s',
                label='Target Baseline (No UDA)', linewidth=2, markersize=4)
        ax1.plot(df['n_features'], df['target_tca_auc'], 'g-^',
                label='Target TCA', linewidth=2, markersize=4)
        ax1.plot(df['n_features'], df['target_sa_auc'], 'm-v',
                label='Target SA', linewidth=2, markersize=4)
        ax1.plot(df['n_features'], df['target_coral_auc'], 'c-<',
                label='Target CORAL', linewidth=2, markersize=4)
        ax1.plot(df['n_features'], df['target_kmm_auc'], 'y->',
                label='Target KMM', linewidth=2, markersize=4)
        
        ax1.set_xlabel('Number of Features')
        ax1.set_ylabel('AUC Score')
        ax1.set_title('AUC Performance vs Number of Features')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim(0.6, 1.0)
        
        # 2. UDAæ–¹æ³•æå‡æ•ˆæœ (å³ä¸Š)
        ax2 = axes[0, 1]

        # ç»˜åˆ¶æ‰€æœ‰UDAæ–¹æ³•ç›¸å¯¹äºåŸºçº¿çš„æå‡
        x = df['n_features']
        width = 0.2  # æŸ±çŠ¶å›¾å®½åº¦
        x_pos = np.arange(len(x))

        ax2.bar(x_pos - 1.5*width, df['tca_improvement'], width, label='TCA', alpha=0.8)
        ax2.bar(x_pos - 0.5*width, df['sa_improvement'], width, label='SA', alpha=0.8)
        ax2.bar(x_pos + 0.5*width, df['coral_improvement'], width, label='CORAL', alpha=0.8)
        ax2.bar(x_pos + 1.5*width, df['kmm_improvement'], width, label='KMM', alpha=0.8)

        ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.8)
        ax2.set_xlabel('Number of Features')
        ax2.set_ylabel('AUC Improvement vs Baseline')
        ax2.set_title('UDA Methods Domain Adaptation Improvement')
        ax2.set_xticks(x_pos)
        ax2.set_xticklabels(x.astype(int), rotation=45)
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. å‡†ç¡®ç‡å¯¹æ¯” (å·¦ä¸‹)
        ax3 = axes[1, 0]
        
        ax3.plot(df['n_features'], df['source_accuracy'], 'b-o', 
                label='Source Domain', linewidth=2, markersize=5)
        ax3.plot(df['n_features'], df['target_baseline_auc'], 'r-s',  # ä½¿ç”¨AUCä½œä¸ºå‚è€ƒ
                label='Target Baseline (AUC)', linewidth=2, markersize=4)
        
        ax3.set_xlabel('Number of Features')
        ax3.set_ylabel('Score')
        ax3.set_title('Accuracy Performance Comparison')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. æ€§èƒ½çƒ­åŠ›å›¾ (å³ä¸‹)
        ax4 = axes[1, 1]
        
        # å‡†å¤‡çƒ­åŠ›å›¾æ•°æ®
        heatmap_data = df[['source_auc', 'target_baseline_auc', 'target_tca_auc',
                          'target_sa_auc', 'target_coral_auc', 'target_kmm_auc']].T
        heatmap_data.columns = df['feature_type'].values
        
        # åªæ˜¾ç¤ºéƒ¨åˆ†ç‰¹å¾é›†ä»¥é¿å…è¿‡åº¦æ‹¥æŒ¤
        step = max(1, len(heatmap_data.columns) // 15)  # æœ€å¤šæ˜¾ç¤º15ä¸ªæ ‡ç­¾
        selected_cols = heatmap_data.columns[::step]
        heatmap_subset = heatmap_data[selected_cols]
        
        sns.heatmap(heatmap_subset, annot=True, cmap='RdYlGn', center=0.8,
                   fmt='.3f', cbar_kws={'label': 'AUC Score'}, ax=ax4)
        ax4.set_title('Performance Heatmap (Selected Features)')
        ax4.set_ylabel('Method')
        ax4.set_xlabel('Feature Set')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        plot_path = self.output_dir / "performance_comparison.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        if self.verbose:
            print(f"âœ… æ€§èƒ½æ¯”è¾ƒå›¾è¡¨å·²ä¿å­˜: {plot_path}")
        
        return str(plot_path)
    
    def generate_summary_report(self) -> str:
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        if self.verbose:
            print(f"\nğŸ“‹ ç”Ÿæˆæ€»ç»“æŠ¥å‘Š")
        
        report_content = []
        report_content.append("# Feature Sweep Analysis Report\n")
        report_content.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # åˆ†æé…ç½®
        report_content.append("## Analysis Configuration\n")
        report_content.append(f"- Feature Range: best{self.feature_range[0]} ~ best{self.feature_range[1]}")
        report_content.append(f"- Parallel Workers: {self.max_workers}")
        report_content.append(f"- Total Feature Sets: {len(self.sweep_results)}")
        
        # æˆåŠŸç‡ç»Ÿè®¡
        successful = sum(1 for r in self.sweep_results.values() if r['status'] == 'success')
        failed = len(self.sweep_results) - successful
        report_content.append(f"- Successful Analyses: {successful}")
        report_content.append(f"- Failed Analyses: {failed}\n")
        
        # æ€§èƒ½æ€»ç»“
        if not self.performance_summary.empty:
            report_content.append("## Performance Summary\n")
            
            # æœ€ä½³æ€§èƒ½
            best_source = self.performance_summary.loc[self.performance_summary['source_auc'].idxmax()]
            best_target_baseline = self.performance_summary.loc[self.performance_summary['target_baseline_auc'].idxmax()]
            best_target_tca = self.performance_summary.loc[self.performance_summary['target_tca_auc'].idxmax()]
            best_improvement = self.performance_summary.loc[self.performance_summary['tca_improvement'].idxmax()]
            
            report_content.append(f"### Best Performance Results\n")
            report_content.append(f"- **Best Source Domain**: {best_source['feature_type']} (AUC: {best_source['source_auc']:.4f})")
            report_content.append(f"- **Best Target Baseline**: {best_target_baseline['feature_type']} (AUC: {best_target_baseline['target_baseline_auc']:.4f})")
            report_content.append(f"- **Best Target TCA**: {best_target_tca['feature_type']} (AUC: {best_target_tca['target_tca_auc']:.4f})")
            report_content.append(f"- **Best TCA Improvement**: {best_improvement['feature_type']} (+{best_improvement['tca_improvement']:.4f})")
            
            # æ€§èƒ½è¶‹åŠ¿
            report_content.append(f"\n### Performance Trends\n")
            report_content.append(f"- Average Source AUC: {self.performance_summary['source_auc'].mean():.4f}")
            report_content.append(f"- Average Target Baseline AUC: {self.performance_summary['target_baseline_auc'].mean():.4f}")
            report_content.append(f"- Average Target TCA AUC: {self.performance_summary['target_tca_auc'].mean():.4f}")
            report_content.append(f"- Average TCA Improvement: {self.performance_summary['tca_improvement'].mean():.4f}")
            
            # è¯¦ç»†ç»“æœè¡¨
            report_content.append(f"\n### Detailed Results\n")
            report_content.append("| Feature Set | N Features | Source AUC | Target Baseline | Target TCA | TCA Improvement |")
            report_content.append("|-------------|------------|------------|-----------------|------------|-----------------|")
            
            for _, row in self.performance_summary.iterrows():
                report_content.append(f"| {row['feature_type']} | {row['n_features']} | {row['source_auc']:.4f} | {row['target_baseline_auc']:.4f} | {row['target_tca_auc']:.4f} | {row['tca_improvement']:+.4f} |")
        
        # å¤±è´¥çš„åˆ†æ
        failed_analyses = {k: v for k, v in self.sweep_results.items() if v['status'] == 'failed'}
        if failed_analyses:
            report_content.append(f"\n### Failed Analyses\n")
            for feature_type, result in failed_analyses.items():
                report_content.append(f"- **{feature_type}**: {result.get('error', 'Unknown error')}")
        
        # å»ºè®®
        if not self.performance_summary.empty:
            report_content.append(f"\n## Recommendations\n")
            
            # åŸºäºç»“æœçš„å»ºè®®
            best_overall = self.performance_summary.loc[
                (self.performance_summary['target_tca_auc'] + self.performance_summary['source_auc']).idxmax()
            ]
            
            report_content.append(f"- **Recommended Feature Set**: {best_overall['feature_type']}")
            report_content.append(f"  - Source Performance: {best_overall['source_auc']:.4f} AUC")
            report_content.append(f"  - Target Performance: {best_overall['target_tca_auc']:.4f} AUC")
            report_content.append(f"  - TCA Improvement: {best_overall['tca_improvement']:+.4f}")
            
            # å¹³è¡¡æ€§å»ºè®®
            if best_improvement['tca_improvement'] > 0.01:
                report_content.append(f"- **TCA Domain Adaptation** shows positive effects for most feature sets")
            else:
                report_content.append(f"- **Limited TCA Benefits** - consider alternative UDA methods")
        
        report_content.append(f"\nDetailed results and visualizations available in: {self.output_dir}")
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / "feature_sweep_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_content))
        
        if self.verbose:
            print(f"âœ… æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return str(report_file)
    
    def save_results(self):
        """ä¿å­˜æ‰€æœ‰ç»“æœ"""
        # ä¿å­˜åŸå§‹ç»“æœ
        results_file = self.output_dir / "sweep_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.sweep_results, f, indent=2, ensure_ascii=False, default=str)
        
        # ä¿å­˜æ€§èƒ½æ€»ç»“
        if not self.performance_summary.empty:
            summary_file = self.output_dir / "performance_summary.csv"
            self.performance_summary.to_csv(summary_file, index=False)
            
            summary_json = self.output_dir / "performance_summary.json"
            self.performance_summary.to_json(summary_json, orient='records', indent=2)
        
        if self.verbose:
            print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
    
    def run_complete_feature_sweep(self) -> Dict:
        """è¿è¡Œå®Œæ•´çš„ç‰¹å¾æ‰«æåˆ†æ"""
        try:
            if self.verbose:
                print(f"ğŸš€ å¼€å§‹å®Œæ•´ç‰¹å¾æ‰«æåˆ†æ")
                print("=" * 60)
            
            # 1. å¹¶è¡Œè¿è¡Œç‰¹å¾æ‰«æ
            sweep_results = self.run_parallel_feature_sweep()
            
            # 2. ç”Ÿæˆå¯è§†åŒ–
            plot_path = self.plot_performance_comparison()
            
            # 3. ç”ŸæˆæŠ¥å‘Š
            report_path = self.generate_summary_report()
            
            # 4. ä¿å­˜ç»“æœ
            self.save_results()
            
            if self.verbose:
                print(f"\nâœ… ç‰¹å¾æ‰«æåˆ†æå®Œæˆï¼")
                print(f"ğŸ“ ç»“æœç›®å½•: {self.output_dir}")
                print(f"ğŸ“Š æ€§èƒ½å›¾è¡¨: {plot_path}")
                print(f"ğŸ“‹ åˆ†ææŠ¥å‘Š: {report_path}")
            
            return {
                'status': 'success',
                'results': sweep_results,
                'summary': self.performance_summary.to_dict('records') if not self.performance_summary.empty else [],
                'output_dir': str(self.output_dir),
                'plot_path': plot_path,
                'report_path': report_path
            }
            
        except Exception as e:
            self.logger.error(f"å®Œæ•´ç‰¹å¾æ‰«æåˆ†æå¤±è´¥: {e}")
            if self.verbose:
                import traceback
                traceback.print_exc()
            
            return {
                'status': 'failed',
                'error': str(e),
                'output_dir': str(self.output_dir)
            }


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ç‰¹å¾æ‰«æåˆ†æ - ä»best3åˆ°best58')
    parser.add_argument('--min_features', type=int, default=3,
                        help='æœ€å°ç‰¹å¾æ•° (default: 3)')
    parser.add_argument('--max_features', type=int, default=58,
                        help='æœ€å¤§ç‰¹å¾æ•° (default: 58)')
    parser.add_argument('--max_workers', type=int, default=None,
                        help='æœ€å¤§å¹¶è¡Œå·¥ä½œæ•° (default: auto)')
    parser.add_argument('--output_dir', type=str, default=None,
                        help='è¾“å‡ºç›®å½• (default: auto-generated)')
    parser.add_argument('--quiet', action='store_true', default=False,
                        help='é™é»˜æ¨¡å¼')
    
    args = parser.parse_args()
    
    if not args.quiet:
        print("ğŸ”¬ TabPFNåŒ»ç–—æ•°æ®ç‰¹å¾æ‰«æåˆ†æ")
        print("=" * 60)
        print(f"ğŸ“‹ åˆ†æé…ç½®:")
        print(f"   ç‰¹å¾èŒƒå›´: best{args.min_features} ~ best{args.max_features}")
        print(f"   å¹¶è¡Œå·¥ä½œæ•°: {args.max_workers or 'auto'}")
        print(f"   è¾“å‡ºç›®å½•: {args.output_dir or 'auto-generated'}")
        print("=" * 60)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = FeatureSweepAnalyzer(
        feature_range=(args.min_features, args.max_features),
        output_dir=args.output_dir,
        max_workers=args.max_workers,
        verbose=not args.quiet
    )
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    results = analyzer.run_complete_feature_sweep()
    
    if results['status'] == 'success':
        if not args.quiet:
            print(f"\nğŸ‰ åˆ†ææˆåŠŸå®Œæˆï¼")
            print(f"ğŸ“ æŸ¥çœ‹ç»“æœ: {results['output_dir']}")
            print(f"ğŸ“Š æ€§èƒ½å›¾è¡¨: {results['plot_path']}")
            print(f"ğŸ“‹ åˆ†ææŠ¥å‘Š: {results['report_path']}")
        exit(0)
    else:
        if not args.quiet:
            print(f"\nâŒ åˆ†æå¤±è´¥: {results['error']}")
        exit(1)


if __name__ == "__main__":
    main()