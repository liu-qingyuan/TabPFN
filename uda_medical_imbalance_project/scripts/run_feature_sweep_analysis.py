#!/usr/bin/env python3
"""
ç‰¹å¾æ‰«æåˆ†æè„šæœ¬ - è¿è¡Œä»best3åˆ°best58çš„æ‰€æœ‰ç‰¹å¾ç»„åˆåˆ†æ

è¿™ä¸ªè„šæœ¬è‡ªåŠ¨è¿è¡Œæ‰€æœ‰å¯èƒ½çš„ç‰¹å¾é…ç½®ï¼Œç”Ÿæˆæ€§èƒ½æ¯”è¾ƒå¯è§†åŒ–ï¼Œ
ä¾¿äºæ‰¾åˆ°æœ€ä¼˜çš„ç‰¹å¾ç»„åˆè¿›è¡ŒåŒ»ç–—æ•°æ®åŸŸé€‚åº”åˆ†æã€‚

è¿è¡Œç¤ºä¾‹: python scripts/run_feature_sweep_analysis.py
"""

import sys
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from datetime import datetime
import json
from typing import Dict, List, Tuple, Optional
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import logging

warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“å’Œæ ·å¼
plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.dpi'] = 150
plt.style.use('seaborn-v0_8-whitegrid')

from scripts.run_complete_analysis import CompleteAnalysisRunner

# Direct settings import to avoid yaml dependency
import importlib.util
def load_settings_direct():
    """Load settings module directly without complex imports"""
    settings_path = project_root / "config" / "settings.py"
    spec = importlib.util.spec_from_file_location("settings", settings_path)
    settings = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(settings)
    return settings

def get_features_by_type(feature_type: str):
    """Get features by type without complex imports - support all best3-best58"""
    settings = load_settings_direct()
    
    # ç›´æ¥ä½¿ç”¨settingsæ¨¡å—çš„get_features_by_typeå‡½æ•°
    try:
        return settings.get_features_by_type(feature_type)
    except Exception:
        # å¤‡é€‰æ–¹æ¡ˆï¼šå¦‚æœsettings.pyçš„get_features_by_typeå‡½æ•°å¤±è´¥ï¼Œå°è¯•ç›´æ¥è®¿é—®å±æ€§
        try:
            # åŠ¨æ€è·å–ç‰¹å¾é›†å±æ€§
            if feature_type == 'all63':
                return getattr(settings, 'ALL_63_FEATURES', [])
            elif feature_type == 'selected58':
                return getattr(settings, 'SELECTED_58_FEATURES', [])
            else:
                # å¯¹äºbestNç‰¹å¾é›†ï¼Œå°è¯•è·å–å¯¹åº”çš„å±æ€§
                attr_name = f"{feature_type.upper()}_FEATURES"
                return getattr(settings, attr_name, [])
        except Exception:
            # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
            return []


class FeatureSweepAnalyzer:
    """ç‰¹å¾æ‰«æåˆ†æå™¨"""
    
    def __init__(
        self,
        feature_range: Tuple[int, int] = (3, 58),
        output_dir: Optional[str] = None,
        max_workers: int = None,
        verbose: bool = True
    ):
        """
        åˆå§‹åŒ–ç‰¹å¾æ‰«æåˆ†æå™¨
        
        Args:
            feature_range: ç‰¹å¾æ•°é‡èŒƒå›´ (min_features, max_features)
            output_dir: è¾“å‡ºç›®å½•
            max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        """
        self.feature_range = feature_range
        self.verbose = verbose
        
        # è®¾ç½®å¹¶è¡Œå·¥ä½œæ•°
        if max_workers is None:
            self.max_workers = min(4, mp.cpu_count() // 2)  # ä¿å®ˆçš„å¹¶è¡Œåº¦
        else:
            self.max_workers = max_workers
            
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if output_dir is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = f"results/feature_sweep_analysis_{timestamp}"
        
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        self.results_dir = self.output_dir / "individual_results"
        self.results_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        
        # å­˜å‚¨ç»“æœ
        self.sweep_results = {}
        self.performance_summary = {}
        
        if self.verbose:
            print(f"ğŸ”§ ç‰¹å¾æ‰«æåˆ†æå™¨åˆå§‹åŒ–")
            print(f"   ç‰¹å¾èŒƒå›´: best{feature_range[0]} ~ best{feature_range[1]}")
            print(f"   è¾“å‡ºç›®å½•: {output_dir}")
            print(f"   æœ€å¤§å¹¶è¡Œæ•°: {self.max_workers}")
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_file = self.output_dir / "feature_sweep.log"
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler() if self.verbose else logging.NullHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def get_available_feature_sets(self) -> List[str]:
        """è·å–å¯ç”¨çš„ç‰¹å¾é›†åˆ—è¡¨"""
        available_sets = []
        min_features, max_features = self.feature_range
        
        for n_features in range(min_features, max_features + 1):
            feature_type = f"best{n_features}"
            try:
                features = get_features_by_type(feature_type)
                if features:
                    available_sets.append(feature_type)
                    if self.verbose:
                        print(f"âœ… {feature_type}: {len(features)}ä¸ªç‰¹å¾")
                else:
                    if self.verbose:
                        print(f"âš ï¸ {feature_type}: æœªå®šä¹‰")
            except Exception as e:
                if self.verbose:
                    print(f"âŒ {feature_type}: {e}")
        
        return available_sets
    
    def run_single_feature_analysis(self, feature_type: str) -> Dict:
        """è¿è¡Œå•ä¸ªç‰¹å¾é›†çš„åˆ†æ"""
        try:
            self.logger.info(f"å¼€å§‹åˆ†æç‰¹å¾é›†: {feature_type}")
            
            # åˆ›å»ºä¸ªä½“ç»“æœç›®å½•
            individual_output_dir = self.results_dir / feature_type
            
            # åˆ›å»ºåˆ†æè¿è¡Œå™¨
            runner = CompleteAnalysisRunner(
                feature_type=feature_type,
                scaler_type='none',  # ä½¿ç”¨é»˜è®¤é…ç½®
                imbalance_method='none',
                cv_folds=10,
                random_state=42,
                output_dir=str(individual_output_dir),
                verbose=False  # å…³é—­è¯¦ç»†è¾“å‡ºé¿å…æ—¥å¿—æ··ä¹±
            )
            
            # è¿è¡Œå®Œæ•´åˆ†æ
            results = runner.run_complete_analysis()
            
            if 'error' in results:
                self.logger.error(f"ç‰¹å¾é›† {feature_type} åˆ†æå¤±è´¥: {results['error']}")
                return {
                    'feature_type': feature_type,
                    'status': 'failed',
                    'error': results['error'],
                    'n_features': int(feature_type.replace('best', ''))
                }
            
            # æå–æ€§èƒ½æŒ‡æ ‡
            performance = self.extract_performance_metrics(results, feature_type)
            
            self.logger.info(f"ç‰¹å¾é›† {feature_type} åˆ†æå®Œæˆ")
            return performance
            
        except Exception as e:
            self.logger.error(f"ç‰¹å¾é›† {feature_type} æ‰§è¡Œå¼‚å¸¸: {e}")
            return {
                'feature_type': feature_type,
                'status': 'failed',
                'error': str(e),
                'n_features': int(feature_type.replace('best', ''))
            }
    
    def extract_performance_metrics(self, results: Dict, feature_type: str) -> Dict:
        """æå–æ€§èƒ½æŒ‡æ ‡"""
        performance = {
            'feature_type': feature_type,
            'n_features': int(feature_type.replace('best', '')),
            'status': 'success'
        }
        
        # 1. æå–æºåŸŸäº¤å‰éªŒè¯ç»“æœ (TabPFN)
        if 'source_domain_cv' in results:
            cv_results = results['source_domain_cv']
            
            # æ‰¾åˆ°TabPFNç»“æœ
            tabpfn_key = None
            for key in cv_results.keys():
                if 'tabpfn' in key.lower():
                    tabpfn_key = key
                    break
            
            if tabpfn_key and 'summary' in cv_results[tabpfn_key]:
                summary = cv_results[tabpfn_key]['summary']
                performance.update({
                    'source_auc': summary.get('auc_mean', 0),
                    'source_accuracy': summary.get('accuracy_mean', 0),
                    'source_f1': summary.get('f1_mean', 0),
                    'source_precision': summary.get('precision_mean', 0),
                    'source_recall': summary.get('recall_mean', 0),
                    'source_auc_std': summary.get('auc_std', 0)
                })
        
        # 2. æå–UDAç»“æœ
        if 'uda_methods' in results:
            uda_results = results['uda_methods']
            
            # TabPFNæ— UDAåŸºçº¿
            if 'TabPFN_NoUDA' in uda_results:
                baseline = uda_results['TabPFN_NoUDA']
                if 'error' not in baseline:
                    performance.update({
                        'target_baseline_auc': baseline.get('auc', 0),
                        'target_baseline_accuracy': baseline.get('accuracy', 0),
                        'target_baseline_f1': baseline.get('f1', 0)
                    })
            
            # TCAç»“æœ
            if 'TCA' in uda_results:
                tca = uda_results['TCA']
                if 'error' not in tca:
                    performance.update({
                        'target_tca_auc': tca.get('auc', 0),
                        'target_tca_accuracy': tca.get('accuracy', 0),
                        'target_tca_f1': tca.get('f1', 0)
                    })

                    # è®¡ç®—TCAç›¸å¯¹äºåŸºçº¿çš„æå‡
                    if 'target_baseline_auc' in performance and performance['target_baseline_auc'] > 0:
                        improvement = performance['target_tca_auc'] - performance['target_baseline_auc']
                        performance['tca_auc_improvement'] = improvement

            # SAç»“æœ
            if 'SA' in uda_results:
                sa = uda_results['SA']
                if 'error' not in sa:
                    performance.update({
                        'target_sa_auc': sa.get('auc', 0),
                        'target_sa_accuracy': sa.get('accuracy', 0),
                        'target_sa_f1': sa.get('f1', 0)
                    })

                    # è®¡ç®—SAç›¸å¯¹äºåŸºçº¿çš„æå‡
                    if 'target_baseline_auc' in performance and performance['target_baseline_auc'] > 0:
                        improvement = performance['target_sa_auc'] - performance['target_baseline_auc']
                        performance['sa_auc_improvement'] = improvement

            # CORALç»“æœ
            if 'CORAL' in uda_results:
                coral = uda_results['CORAL']
                if 'error' not in coral:
                    performance.update({
                        'target_coral_auc': coral.get('auc', 0),
                        'target_coral_accuracy': coral.get('accuracy', 0),
                        'target_coral_f1': coral.get('f1', 0)
                    })

                    # è®¡ç®—CORALç›¸å¯¹äºåŸºçº¿çš„æå‡
                    if 'target_baseline_auc' in performance and performance['target_baseline_auc'] > 0:
                        improvement = performance['target_coral_auc'] - performance['target_baseline_auc']
                        performance['coral_auc_improvement'] = improvement

            # KMMç»“æœ
            if 'KMM' in uda_results:
                kmm = uda_results['KMM']
                if 'error' not in kmm:
                    performance.update({
                        'target_kmm_auc': kmm.get('auc', 0),
                        'target_kmm_accuracy': kmm.get('accuracy', 0),
                        'target_kmm_f1': kmm.get('f1', 0)
                    })

                    # è®¡ç®—KMMç›¸å¯¹äºåŸºçº¿çš„æå‡
                    if 'target_baseline_auc' in performance and performance['target_baseline_auc'] > 0:
                        improvement = performance['target_kmm_auc'] - performance['target_baseline_auc']
                        performance['kmm_auc_improvement'] = improvement
        
        return performance
    
    def run_parallel_feature_sweep(self) -> Dict:
        """å¹¶è¡Œè¿è¡Œç‰¹å¾æ‰«æåˆ†æ"""
        if self.verbose:
            print(f"\nğŸš€ å¼€å§‹å¹¶è¡Œç‰¹å¾æ‰«æåˆ†æ")
            print("=" * 50)
        
        # è·å–å¯ç”¨ç‰¹å¾é›†
        available_sets = self.get_available_feature_sets()
        
        if not available_sets:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„ç‰¹å¾é›†é…ç½®")
        
        if self.verbose:
            print(f"ğŸ“Š å°†åˆ†æ {len(available_sets)} ä¸ªç‰¹å¾é›†")
            print(f"ğŸ”„ ä½¿ç”¨ {self.max_workers} ä¸ªå¹¶è¡Œè¿›ç¨‹")
        
        # å¹¶è¡Œæ‰§è¡Œåˆ†æ
        results = []
        
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_feature = {
                executor.submit(self.run_single_feature_analysis, feature_type): feature_type 
                for feature_type in available_sets
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_feature):
                feature_type = future_to_feature[future]
                try:
                    result = future.result()
                    results.append(result)
                    
                    status = "âœ…" if result['status'] == 'success' else "âŒ"
                    if self.verbose:
                        print(f"{status} {feature_type} å®Œæˆ")
                        
                except Exception as e:
                    self.logger.error(f"å¤„ç† {feature_type} ç»“æœæ—¶å‡ºé”™: {e}")
                    results.append({
                        'feature_type': feature_type,
                        'status': 'failed',
                        'error': str(e),
                        'n_features': int(feature_type.replace('best', ''))
                    })
        
        # æ•´ç†ç»“æœ
        self.sweep_results = {r['feature_type']: r for r in results}
        
        # ç”Ÿæˆæ€§èƒ½æ€»ç»“
        self.performance_summary = self.generate_performance_summary()
        
        return self.sweep_results
    
    def generate_performance_summary(self) -> pd.DataFrame:
        """ç”Ÿæˆæ€§èƒ½æ€»ç»“è¡¨"""
        summary_data = []
        
        for feature_type, result in self.sweep_results.items():
            if result['status'] == 'success':
                summary_data.append({
                    'feature_type': feature_type,
                    'n_features': result['n_features'],
                    'source_auc': result.get('source_auc', 0),
                    'source_accuracy': result.get('source_accuracy', 0),
                    'source_f1': result.get('source_f1', 0),
                    'target_baseline_auc': result.get('target_baseline_auc', 0),
                    'target_tca_auc': result.get('target_tca_auc', 0),
                    'target_sa_auc': result.get('target_sa_auc', 0),
                    'target_coral_auc': result.get('target_coral_auc', 0),
                    'target_kmm_auc': result.get('target_kmm_auc', 0),
                    'tca_improvement': result.get('tca_auc_improvement', 0),
                    'sa_improvement': result.get('sa_auc_improvement', 0),
                    'coral_improvement': result.get('coral_auc_improvement', 0),
                    'kmm_improvement': result.get('kmm_auc_improvement', 0)
                })
        
        if summary_data:
            df = pd.DataFrame(summary_data).sort_values('n_features')
            return df
        else:
            return pd.DataFrame()
    
    def plot_performance_comparison(self) -> str:
        """ç»¼åˆæ€§èƒ½æ¯”è¾ƒå›¾è¡¨ç”Ÿæˆå™¨ - ç”Ÿæˆæ‰€æœ‰UDAæ–¹æ³•çš„ä¸ªåˆ«å›¾è¡¨å’Œæ±‡æ€»å›¾è¡¨"""
        if self.verbose:
            print(f"\nğŸ“Š å¼€å§‹ç”Ÿæˆç»¼åˆæ€§èƒ½å¯è§†åŒ–åˆ†æ")

        # å‡†å¤‡æ•°æ®
        df = self.performance_summary

        if df.empty:
            self.logger.warning("æ²¡æœ‰æˆåŠŸçš„ç»“æœå¯ç”¨äºå¯è§†åŒ–")
            return ""

        generated_plots = []

        # 1. ç”Ÿæˆæ¯ä¸ªUDAæ–¹æ³•çš„å•ç‹¬æ€§èƒ½åˆ†æå›¾è¡¨
        uda_methods = ['TCA', 'SA', 'CORAL', 'KMM']

        if self.verbose:
            print("\nğŸ“ˆ ç”Ÿæˆä¸ªåˆ«UDAæ–¹æ³•æ€§èƒ½åˆ†æå›¾è¡¨:")

        for method in uda_methods:
            try:
                individual_plot_path = self.plot_individual_uda_performance(method)
                if individual_plot_path:
                    generated_plots.append(individual_plot_path)
                    if self.verbose:
                        print(f"  âœ… {method}æ–¹æ³•å›¾è¡¨: {Path(individual_plot_path).name}")
            except Exception as e:
                self.logger.warning(f"ç”Ÿæˆ{method}ä¸ªåˆ«å›¾è¡¨æ—¶å‡ºé”™: {e}")

        # 2. ç”Ÿæˆç»¼åˆæ±‡æ€»æ€§èƒ½å›¾è¡¨
        if self.verbose:
            print("\nğŸ“Š ç”Ÿæˆç»¼åˆæ±‡æ€»æ€§èƒ½å›¾è¡¨:")

        try:
            summary_plot_path = self.plot_comprehensive_summary()
            if summary_plot_path:
                generated_plots.append(summary_plot_path)
                if self.verbose:
                    print(f"  âœ… ç»¼åˆæ±‡æ€»å›¾è¡¨: {Path(summary_plot_path).name}")
        except Exception as e:
            self.logger.warning(f"ç”Ÿæˆç»¼åˆæ±‡æ€»å›¾è¡¨æ—¶å‡ºé”™: {e}")

        # 3. ç”ŸæˆUDAæ–¹æ³•æ’ååˆ†æå›¾è¡¨
        if self.verbose:
            print("\nğŸ† ç”ŸæˆUDAæ–¹æ³•æ’ååˆ†æå›¾è¡¨:")

        try:
            ranking_plot_path = self.plot_uda_ranking_analysis()
            if ranking_plot_path:
                generated_plots.append(ranking_plot_path)
                if self.verbose:
                    print(f"  âœ… æ’ååˆ†æå›¾è¡¨: {Path(ranking_plot_path).name}")
        except Exception as e:
            self.logger.warning(f"ç”Ÿæˆæ’ååˆ†æå›¾è¡¨æ—¶å‡ºé”™: {e}")

        # 4. ç”ŸæˆåŸå§‹ç‰ˆæœ¬çš„æ€§èƒ½æ¯”è¾ƒå›¾è¡¨ï¼ˆä¿æŒå‘åå…¼å®¹æ€§ï¼‰
        if self.verbose:
            print("\nğŸ“‹ ç”ŸæˆåŸå§‹æ€§èƒ½æ¯”è¾ƒå›¾è¡¨ï¼ˆå‘åå…¼å®¹ï¼‰:")

        try:
            original_plot_path = self._plot_original_performance_comparison()
            if original_plot_path:
                generated_plots.append(original_plot_path)
                if self.verbose:
                    print(f"  âœ… åŸå§‹æ¯”è¾ƒå›¾è¡¨: {Path(original_plot_path).name}")
        except Exception as e:
            self.logger.warning(f"ç”ŸæˆåŸå§‹æ¯”è¾ƒå›¾è¡¨æ—¶å‡ºé”™: {e}")

        # ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Š
        if self.verbose:
            print("\nğŸ“ æ€§èƒ½åˆ†ææ€»ç»“:")
            best_methods = self.identify_best_method_per_feature_set()
            if best_methods:
                print("  å„ç‰¹å¾é›†æœ€ä½³UDAæ–¹æ³•:")
                for feature_set, method_info in best_methods.items():
                    print(f"    {feature_set}: {method_info['method']} (AUC: {method_info['auc']:.3f})")

        # è¿”å›ä¸»è¦çš„ç»¼åˆæ±‡æ€»å›¾è¡¨è·¯å¾„
        if generated_plots:
            main_plot = next((path for path in generated_plots if "comprehensive_summary" in path), generated_plots[0])
            if self.verbose:
                print(f"\nğŸ¯ ç”Ÿæˆäº† {len(generated_plots)} ä¸ªå›¾è¡¨æ–‡ä»¶")
                print(f"âœ… ä¸»è¦å›¾è¡¨è·¯å¾„: {main_plot}")
            return main_plot
        else:
            self.logger.error("æœªèƒ½ç”Ÿæˆä»»ä½•å›¾è¡¨")
            return ""

    def _plot_original_performance_comparison(self) -> str:
        """ç”ŸæˆåŸå§‹ç‰ˆæœ¬çš„æ€§èƒ½æ¯”è¾ƒå›¾è¡¨ï¼ˆä¿æŒå‘åå…¼å®¹æ€§ï¼‰"""
        df = self.performance_summary

        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Feature Selection Performance Analysis (Original)\n(Best3 ~ Best58)',
                     fontsize=16, fontweight='bold', y=0.98)

        # 1. AUCæ€§èƒ½æ›²çº¿ (å·¦ä¸Š)
        ax1 = axes[0, 0]

        # ç»˜åˆ¶æºåŸŸå’Œç›®æ ‡åŸŸAUC
        ax1.plot(df['n_features'], df['source_auc'], 'b-o',
                label='Source Domain (10-fold CV)', linewidth=2, markersize=5)
        ax1.plot(df['n_features'], df['target_baseline_auc'], 'r-s',
                label='Target Baseline (No UDA)', linewidth=2, markersize=4)
        ax1.plot(df['n_features'], df['target_tca_auc'], 'g-^',
                label='Target TCA', linewidth=2, markersize=4)
        ax1.plot(df['n_features'], df['target_sa_auc'], 'm-v',
                label='Target SA', linewidth=2, markersize=4)
        ax1.plot(df['n_features'], df['target_coral_auc'], 'c-<',
                label='Target CORAL', linewidth=2, markersize=4)
        ax1.plot(df['n_features'], df['target_kmm_auc'], 'y->',
                label='Target KMM', linewidth=2, markersize=4)

        ax1.set_xlabel('Number of Features')
        ax1.set_ylabel('AUC Score')
        ax1.set_title('AUC Performance vs Number of Features')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim(0.6, 1.0)

        # 2. UDAæ–¹æ³•æå‡æ•ˆæœ (å³ä¸Š)
        ax2 = axes[0, 1]

        # ç»˜åˆ¶æ‰€æœ‰UDAæ–¹æ³•ç›¸å¯¹äºåŸºçº¿çš„æå‡
        x = df['n_features']
        width = 0.2  # æŸ±çŠ¶å›¾å®½åº¦
        x_pos = np.arange(len(x))

        ax2.bar(x_pos - 1.5*width, df['tca_improvement'], width, label='TCA', alpha=0.8)
        ax2.bar(x_pos - 0.5*width, df['sa_improvement'], width, label='SA', alpha=0.8)
        ax2.bar(x_pos + 0.5*width, df['coral_improvement'], width, label='CORAL', alpha=0.8)
        ax2.bar(x_pos + 1.5*width, df['kmm_improvement'], width, label='KMM', alpha=0.8)

        ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.8)
        ax2.set_xlabel('Number of Features')
        ax2.set_ylabel('AUC Improvement vs Baseline')
        ax2.set_title('UDA Methods Domain Adaptation Improvement')
        ax2.set_xticks(x_pos)
        ax2.set_xticklabels(x.astype(int), rotation=45)
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # 3. å‡†ç¡®ç‡å¯¹æ¯” (å·¦ä¸‹)
        ax3 = axes[1, 0]

        ax3.plot(df['n_features'], df['source_accuracy'], 'b-o',
                label='Source Domain', linewidth=2, markersize=5)
        ax3.plot(df['n_features'], df['target_baseline_auc'], 'r-s',  # ä½¿ç”¨AUCä½œä¸ºå‚è€ƒ
                label='Target Baseline (AUC)', linewidth=2, markersize=4)

        ax3.set_xlabel('Number of Features')
        ax3.set_ylabel('Score')
        ax3.set_title('Accuracy Performance Comparison')
        ax3.legend()
        ax3.grid(True, alpha=0.3)

        # 4. æ€§èƒ½çƒ­åŠ›å›¾ (å³ä¸‹)
        ax4 = axes[1, 1]

        # å‡†å¤‡çƒ­åŠ›å›¾æ•°æ®
        heatmap_data = df[['source_auc', 'target_baseline_auc', 'target_tca_auc',
                          'target_sa_auc', 'target_coral_auc', 'target_kmm_auc']].T
        heatmap_data.columns = df['feature_type'].values

        # åªæ˜¾ç¤ºéƒ¨åˆ†ç‰¹å¾é›†ä»¥é¿å…è¿‡åº¦æ‹¥æŒ¤
        step = max(1, len(heatmap_data.columns) // 15)  # æœ€å¤šæ˜¾ç¤º15ä¸ªæ ‡ç­¾
        selected_cols = heatmap_data.columns[::step]
        heatmap_subset = heatmap_data[selected_cols]

        sns.heatmap(heatmap_subset, annot=True, cmap='RdYlGn', center=0.8,
                   fmt='.3f', cbar_kws={'label': 'AUC Score'}, ax=ax4)
        ax4.set_title('Performance Heatmap (Selected Features)')
        ax4.set_ylabel('Method')
        ax4.set_xlabel('Feature Set')

        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        plot_path = self.output_dir / "performance_comparison.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()

        return str(plot_path)

    def plot_individual_uda_performance(self, method_name: str) -> str:
        """ç»˜åˆ¶å•ä¸ªUDAæ–¹æ³•çš„è¯¦ç»†æ€§èƒ½åˆ†æå›¾è¡¨"""
        if self.verbose:
            print(f"\nğŸ“Š ç”Ÿæˆ{method_name}æ–¹æ³•è¯¦ç»†æ€§èƒ½åˆ†æ")

        # å‡†å¤‡æ•°æ®
        df = self.performance_summary

        if df.empty:
            self.logger.warning("æ²¡æœ‰æˆåŠŸçš„ç»“æœå¯ç”¨äºå¯è§†åŒ–")
            return ""

        # åˆ›å»º2x2å­å›¾å¸ƒå±€
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(f'{method_name} Method Performance Analysis\n(Feature Selection Sweep)',
                     fontsize=16, fontweight='bold', y=0.98)

        # æ•°æ®åˆ—åæ˜ å°„
        method_mapping = {
            'TCA': ('target_tca_auc', 'tca_improvement'),
            'SA': ('target_sa_auc', 'sa_improvement'),
            'CORAL': ('target_coral_auc', 'coral_improvement'),
            'KMM': ('target_kmm_auc', 'kmm_improvement')
        }

        if method_name not in method_mapping:
            self.logger.error(f"æœªçŸ¥çš„UDAæ–¹æ³•: {method_name}")
            return ""

        auc_col, improvement_col = method_mapping[method_name]

        # 1. AUCæ€§èƒ½å¯¹æ¯” (å·¦ä¸Š)
        ax1 = axes[0, 0]
        ax1.plot(df['n_features'], df['source_auc'], 'b-o',
                label='Source Domain (10-fold CV)', linewidth=2, markersize=5)
        ax1.plot(df['n_features'], df['target_baseline_auc'], 'r-s',
                label='Target Baseline (No UDA)', linewidth=2, markersize=4)
        ax1.plot(df['n_features'], df[auc_col], 'g-^',
                label=f'Target {method_name}', linewidth=2, markersize=4)

        ax1.set_xlabel('Number of Features')
        ax1.set_ylabel('AUC Score')
        ax1.set_title(f'{method_name} vs Baseline AUC Performance')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim(0.6, 1.0)

        # 2. æå‡æ•ˆæœåˆ†æ (å³ä¸Š)
        ax2 = axes[0, 1]
        improvement_data = df[improvement_col].values
        colors = ['green' if x > 0 else 'red' for x in improvement_data]

        bars = ax2.bar(df['n_features'], improvement_data,
                      color=colors, alpha=0.7, edgecolor='black', linewidth=0.5)
        ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.8)
        ax2.set_xlabel('Number of Features')
        ax2.set_ylabel('AUC Improvement vs Baseline')
        ax2.set_title(f'{method_name} Domain Adaptation Improvement')
        ax2.grid(True, alpha=0.3)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, improvement in zip(bars, improvement_data):
            if abs(improvement) > 0.001:  # åªæ˜¾ç¤ºæœ‰æ„ä¹‰çš„æ”¹è¿›
                ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.001,
                        f'{improvement:.3f}', ha='center', va='bottom', fontsize=8)

        # 3. æ€§èƒ½ç¨³å®šæ€§åˆ†æ (å·¦ä¸‹)
        ax3 = axes[1, 0]

        # è®¡ç®—æ€§èƒ½å˜åŒ–ç‡
        auc_values = df[auc_col].values
        performance_std = np.std(auc_values)
        performance_mean = np.mean(auc_values)
        cv = performance_std / performance_mean if performance_mean > 0 else 0

        ax3.plot(df['n_features'], df[auc_col], 'g-o', linewidth=2, markersize=5)
        ax3.axhline(y=performance_mean, color='orange', linestyle='--',
                   label=f'Mean AUC: {performance_mean:.3f}')
        ax3.fill_between(df['n_features'],
                        performance_mean - performance_std,
                        performance_mean + performance_std,
                        alpha=0.2, color='orange',
                        label=f'Â±1 STD: {performance_std:.3f}')

        ax3.set_xlabel('Number of Features')
        ax3.set_ylabel('AUC Score')
        ax3.set_title(f'{method_name} Performance Stability\n(CV: {cv:.3f})')
        ax3.legend()
        ax3.grid(True, alpha=0.3)

        # 4. æœ€ä½³ç‰¹å¾æ•°è¯†åˆ« (å³ä¸‹)
        ax4 = axes[1, 1]

        # æ‰¾åˆ°æœ€ä½³æ€§èƒ½çš„ç‰¹å¾æ•°
        best_idx = np.argmax(df[auc_col].values)
        best_n_features = df['n_features'].iloc[best_idx]
        best_auc = df[auc_col].iloc[best_idx]
        best_improvement = df[improvement_col].iloc[best_idx]

        # ç»˜åˆ¶æ€§èƒ½æ›²çº¿å¹¶é«˜äº®æœ€ä½³ç‚¹
        ax4.plot(df['n_features'], df[auc_col], 'g-o', linewidth=2, markersize=4)
        ax4.scatter([best_n_features], [best_auc], color='red', s=100, zorder=5,
                   label=f'Best: {best_n_features} features')

        # æ·»åŠ æœ€ä½³ç‚¹ä¿¡æ¯
        ax4.annotate(f'Best Performance\n{best_n_features} features\nAUC: {best_auc:.3f}\nImprovement: +{best_improvement:.3f}',
                    xy=(best_n_features, best_auc), xytext=(10, 10),
                    textcoords='offset points', bbox=dict(boxstyle='round,pad=0.3', fc='yellow', alpha=0.7),
                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))

        ax4.set_xlabel('Number of Features')
        ax4.set_ylabel('AUC Score')
        ax4.set_title(f'{method_name} Optimal Feature Selection')
        ax4.legend()
        ax4.grid(True, alpha=0.3)

        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        plot_path = self.output_dir / f"performance_comparison_{method_name}.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()

        if self.verbose:
            print(f"âœ… {method_name}è¯¦ç»†æ€§èƒ½å›¾è¡¨å·²ä¿å­˜: {plot_path}")

        return str(plot_path)

    def identify_best_method_per_feature_set(self) -> Dict:
        """è¯†åˆ«æ¯ä¸ªç‰¹å¾é›†ä¸‹çš„æœ€ä½³UDAæ–¹æ³•"""
        df = self.performance_summary

        if df.empty:
            return {}

        best_methods = {}
        uda_methods = ['TCA', 'SA', 'CORAL', 'KMM']
        uda_columns = ['target_tca_auc', 'target_sa_auc', 'target_coral_auc', 'target_kmm_auc']

        for _, row in df.iterrows():
            feature_type = row['feature_type']
            n_features = row['n_features']

            # è·å–å„æ–¹æ³•çš„AUC
            aucs = [row[col] for col in uda_columns]
            best_idx = np.argmax(aucs)
            best_method = uda_methods[best_idx]
            best_auc = aucs[best_idx]

            best_methods[feature_type] = {
                'method': best_method,
                'auc': best_auc,
                'n_features': n_features,
                'improvement': row[f'{best_method.lower()}_improvement']
            }

        return best_methods

    def plot_comprehensive_summary(self) -> str:
        """ç»˜åˆ¶å¢å¼ºçš„ç»¼åˆæ±‡æ€»æ€§èƒ½å›¾è¡¨"""
        if self.verbose:
            print(f"\nğŸ“Š ç”Ÿæˆå¢å¼ºç»¼åˆæ±‡æ€»æ€§èƒ½å›¾è¡¨")

        # å‡†å¤‡æ•°æ®
        df = self.performance_summary

        if df.empty:
            self.logger.warning("æ²¡æœ‰æˆåŠŸçš„ç»“æœå¯ç”¨äºå¯è§†åŒ–")
            return ""

        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Comprehensive UDA Methods Performance Analysis\n(Feature Selection Sweep)',
                     fontsize=16, fontweight='bold', y=0.98)

        # 1. æ‰€æœ‰UDAæ–¹æ³•AUCæ€§èƒ½æ›²çº¿å¯¹æ¯” (å·¦ä¸Š)
        ax1 = axes[0, 0]

        # ç»˜åˆ¶æºåŸŸå’Œç›®æ ‡åŸŸAUC
        ax1.plot(df['n_features'], df['source_auc'], 'b-o',
                label='Source Domain (10-fold CV)', linewidth=2, markersize=5)
        ax1.plot(df['n_features'], df['target_baseline_auc'], 'r-s',
                label='Target Baseline (No UDA)', linewidth=2, markersize=4)
        ax1.plot(df['n_features'], df['target_tca_auc'], 'g-^',
                label='Target TCA', linewidth=2, markersize=4)
        ax1.plot(df['n_features'], df['target_sa_auc'], 'm-v',
                label='Target SA', linewidth=2, markersize=4)
        ax1.plot(df['n_features'], df['target_coral_auc'], 'c-<',
                label='Target CORAL', linewidth=2, markersize=4)
        ax1.plot(df['n_features'], df['target_kmm_auc'], 'y->',
                label='Target KMM', linewidth=2, markersize=4)

        ax1.set_xlabel('Number of Features')
        ax1.set_ylabel('AUC Score')
        ax1.set_title('All UDA Methods AUC Performance Comparison')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim(0.6, 1.0)

        # 2. æ‰€æœ‰æ–¹æ³•æå‡æ•ˆæœå¹¶æ’å¯¹æ¯” (å³ä¸Š)
        ax2 = axes[0, 1]

        # ç»˜åˆ¶æ‰€æœ‰UDAæ–¹æ³•ç›¸å¯¹äºåŸºçº¿çš„æå‡
        x = df['n_features']
        width = 0.2  # æŸ±çŠ¶å›¾å®½åº¦
        x_pos = np.arange(len(x))

        ax2.bar(x_pos - 1.5*width, df['tca_improvement'], width, label='TCA', alpha=0.8)
        ax2.bar(x_pos - 0.5*width, df['sa_improvement'], width, label='SA', alpha=0.8)
        ax2.bar(x_pos + 0.5*width, df['coral_improvement'], width, label='CORAL', alpha=0.8)
        ax2.bar(x_pos + 1.5*width, df['kmm_improvement'], width, label='KMM', alpha=0.8)

        ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.8)
        ax2.set_xlabel('Number of Features')
        ax2.set_ylabel('AUC Improvement vs Baseline')
        ax2.set_title('UDA Methods Improvement Comparison')
        ax2.set_xticks(x_pos)
        ax2.set_xticklabels(x.astype(int), rotation=45)
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # 3. æœ€ä½³æ–¹æ³•è¯†åˆ«å›¾ (å·¦ä¸‹)
        ax3 = axes[1, 0]

        # è·å–æ¯ä¸ªç‰¹å¾æ•°ä¸‹çš„æœ€ä½³æ–¹æ³•
        best_methods = self.identify_best_method_per_feature_set()
        uda_methods = ['TCA', 'SA', 'CORAL', 'KMM']
        method_colors = {'TCA': 'green', 'SA': 'magenta', 'CORAL': 'cyan', 'KMM': 'orange'}

        # åˆ›å»ºæœ€ä½³æ–¹æ³•çš„æ•£ç‚¹å›¾
        for feature_type, info in best_methods.items():
            method = info['method']
            n_features = info['n_features']
            auc = info['auc']

            # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨è¯¥æ–¹æ³•çš„å›¾ä¾‹
            existing_labels = [item.get_text() for item in ax3.get_legend_handles_labels()[1]] if ax3.get_legend() else []
            label = method if method not in existing_labels else ""
            ax3.scatter([n_features], [auc], color=method_colors[method],
                       s=100, alpha=0.7, label=label)

        # ç»˜åˆ¶è¿æ¥çº¿æ˜¾ç¤ºæœ€ä½³æ–¹æ³•çš„å˜åŒ–
        sorted_features = sorted([(info['n_features'], info['method'], info['auc']) for info in best_methods.values()])
        prev_method = None
        for i, (n_feat, method, auc) in enumerate(sorted_features):
            if i > 0:
                prev_n_feat, prev_method, prev_auc = sorted_features[i-1]
                ax3.plot([prev_n_feat, n_feat], [prev_auc, auc], 'k--', alpha=0.3, linewidth=1)

        ax3.set_xlabel('Number of Features')
        ax3.set_ylabel('Best AUC Score')
        ax3.set_title('Best Performing UDA Method per Feature Set')
        ax3.legend()
        ax3.grid(True, alpha=0.3)

        # 4. æ–¹æ³•ç¨³å®šæ€§åˆ†æ (å³ä¸‹)
        ax4 = axes[1, 1]

        # è®¡ç®—å„æ–¹æ³•åœ¨ä¸åŒç‰¹å¾æ•°ä¸‹çš„æ–¹å·®
        uda_columns = ['target_tca_auc', 'target_sa_auc', 'target_coral_auc', 'target_kmm_auc']
        method_names = ['TCA', 'SA', 'CORAL', 'KMM']

        stability_data = []
        for i, col in enumerate(uda_columns):
            values = df[col].values
            mean_val = np.mean(values)
            std_val = np.std(values)
            cv = std_val / mean_val if mean_val > 0 else 0
            stability_data.append({
                'method': method_names[i],
                'mean': mean_val,
                'std': std_val,
                'cv': cv
            })

        # ç»˜åˆ¶ç¨³å®šæ€§æŸ±çŠ¶å›¾
        methods = [data['method'] for data in stability_data]
        cvs = [data['cv'] for data in stability_data]
        means = [data['mean'] for data in stability_data]

        bars = ax4.bar(methods, cvs, alpha=0.7, color=['green', 'magenta', 'cyan', 'orange'])
        ax4.set_xlabel('UDA Methods')
        ax4.set_ylabel('Coefficient of Variation (CV)')
        ax4.set_title('Method Performance Stability\n(Lower CV = More Stable)')
        ax4.grid(True, alpha=0.3)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, cv, mean in zip(bars, cvs, means):
            ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.001,
                    f'CV: {cv:.3f}\nÎ¼: {mean:.3f}', ha='center', va='bottom', fontsize=8)

        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        plot_path = self.output_dir / "comprehensive_performance_summary.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()

        if self.verbose:
            print(f"âœ… å¢å¼ºç»¼åˆæ±‡æ€»æ€§èƒ½å›¾è¡¨å·²ä¿å­˜: {plot_path}")

        return str(plot_path)

    def plot_uda_ranking_analysis(self) -> str:
        """ç»˜åˆ¶UDAæ–¹æ³•æ’ååˆ†æå›¾è¡¨"""
        if self.verbose:
            print(f"\nğŸ“Š ç”ŸæˆUDAæ–¹æ³•æ’ååˆ†æå›¾è¡¨")

        # å‡†å¤‡æ•°æ®
        df = self.performance_summary

        if df.empty:
            self.logger.warning("æ²¡æœ‰æˆåŠŸçš„ç»“æœå¯ç”¨äºå¯è§†åŒ–")
            return ""

        # åˆ›å»ºå›¾è¡¨
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
        fig.suptitle('UDA Methods Ranking Analysis\n(Feature Selection Sweep)',
                     fontsize=16, fontweight='bold', y=0.95)

        # 1. å„UDAæ–¹æ³•çš„æ’åå˜åŒ–è¶‹åŠ¿ (å·¦)
        uda_columns = ['target_tca_auc', 'target_sa_auc', 'target_coral_auc', 'target_kmm_auc']
        method_names = ['TCA', 'SA', 'CORAL', 'KMM']
        colors = ['green', 'magenta', 'cyan', 'orange']

        rankings = []
        for _, row in df.iterrows():
            aucs = [row[col] for col in uda_columns]
            # è®¡ç®—æ’å (1=æœ€å¥½, 4=æœ€å·®)
            rank_indices = np.argsort(aucs)[::-1]  # é™åºæ’åˆ—çš„ç´¢å¼•
            ranks = [0] * len(aucs)
            for rank, idx in enumerate(rank_indices):
                ranks[idx] = rank + 1
            rankings.append(ranks)

        rankings = np.array(rankings).T  # è½¬ç½®ä½¿æ¯è¡Œä»£è¡¨ä¸€ä¸ªæ–¹æ³•

        # ç»˜åˆ¶æ’åå˜åŒ–è¶‹åŠ¿
        for i, (method, color) in enumerate(zip(method_names, colors)):
            ax1.plot(df['n_features'], rankings[i], 'o-', color=color,
                    label=method, linewidth=2, markersize=6)

        ax1.set_xlabel('Number of Features')
        ax1.set_ylabel('Ranking (1=Best, 4=Worst)')
        ax1.set_title('UDA Methods Ranking Trends')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim(0.5, 4.5)
        ax1.invert_yaxis()  # å€’è½¬yè½´ï¼Œä½¿1åœ¨é¡¶éƒ¨

        # 2. èƒœç‡ç»Ÿè®¡ (å³)
        # è®¡ç®—å„æ–¹æ³•åœ¨å¤šå°‘ä¸ªç‰¹å¾é›†ä¸Šè¡¨ç°æœ€ä½³
        best_methods = self.identify_best_method_per_feature_set()
        win_counts = {method: 0 for method in method_names}

        for info in best_methods.values():
            win_counts[info['method']] += 1

        total_sets = len(best_methods)
        win_rates = [win_counts[method] / total_sets * 100 for method in method_names]

        bars = ax2.bar(method_names, win_rates, alpha=0.7, color=colors)
        ax2.set_xlabel('UDA Methods')
        ax2.set_ylabel('Win Rate (%)')
        ax2.set_title('UDA Methods Win Rate\n(% of Feature Sets Where Method Performs Best)')
        ax2.grid(True, alpha=0.3)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, rate, count in zip(bars, win_rates, [win_counts[method] for method in method_names]):
            ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1,
                    f'{rate:.1f}%\n({count}/{total_sets})', ha='center', va='bottom', fontsize=10)

        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        plot_path = self.output_dir / "uda_methods_ranking.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()

        if self.verbose:
            print(f"âœ… UDAæ–¹æ³•æ’ååˆ†æå›¾è¡¨å·²ä¿å­˜: {plot_path}")

        return str(plot_path)

    def generate_summary_report(self) -> str:
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        if self.verbose:
            print(f"\nğŸ“‹ ç”Ÿæˆæ€»ç»“æŠ¥å‘Š")
        
        report_content = []
        report_content.append("# Feature Sweep Analysis Report\n")
        report_content.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # åˆ†æé…ç½®
        report_content.append("## Analysis Configuration\n")
        report_content.append(f"- Feature Range: best{self.feature_range[0]} ~ best{self.feature_range[1]}")
        report_content.append(f"- Parallel Workers: {self.max_workers}")
        report_content.append(f"- Total Feature Sets: {len(self.sweep_results)}")
        
        # æˆåŠŸç‡ç»Ÿè®¡
        successful = sum(1 for r in self.sweep_results.values() if r['status'] == 'success')
        failed = len(self.sweep_results) - successful
        report_content.append(f"- Successful Analyses: {successful}")
        report_content.append(f"- Failed Analyses: {failed}\n")
        
        # æ€§èƒ½æ€»ç»“
        if not self.performance_summary.empty:
            report_content.append("## Performance Summary\n")
            
            # æœ€ä½³æ€§èƒ½
            best_source = self.performance_summary.loc[self.performance_summary['source_auc'].idxmax()]
            best_target_baseline = self.performance_summary.loc[self.performance_summary['target_baseline_auc'].idxmax()]
            best_target_tca = self.performance_summary.loc[self.performance_summary['target_tca_auc'].idxmax()]
            best_improvement = self.performance_summary.loc[self.performance_summary['tca_improvement'].idxmax()]
            
            report_content.append(f"### Best Performance Results\n")
            report_content.append(f"- **Best Source Domain**: {best_source['feature_type']} (AUC: {best_source['source_auc']:.4f})")
            report_content.append(f"- **Best Target Baseline**: {best_target_baseline['feature_type']} (AUC: {best_target_baseline['target_baseline_auc']:.4f})")
            report_content.append(f"- **Best Target TCA**: {best_target_tca['feature_type']} (AUC: {best_target_tca['target_tca_auc']:.4f})")
            report_content.append(f"- **Best TCA Improvement**: {best_improvement['feature_type']} (+{best_improvement['tca_improvement']:.4f})")
            
            # æ€§èƒ½è¶‹åŠ¿
            report_content.append(f"\n### Performance Trends\n")
            report_content.append(f"- Average Source AUC: {self.performance_summary['source_auc'].mean():.4f}")
            report_content.append(f"- Average Target Baseline AUC: {self.performance_summary['target_baseline_auc'].mean():.4f}")
            report_content.append(f"- Average Target TCA AUC: {self.performance_summary['target_tca_auc'].mean():.4f}")
            report_content.append(f"- Average TCA Improvement: {self.performance_summary['tca_improvement'].mean():.4f}")
            
            # è¯¦ç»†ç»“æœè¡¨
            report_content.append(f"\n### Detailed Results\n")
            report_content.append("| Feature Set | N Features | Source AUC | Target Baseline | Target TCA | TCA Improvement |")
            report_content.append("|-------------|------------|------------|-----------------|------------|-----------------|")
            
            for _, row in self.performance_summary.iterrows():
                report_content.append(f"| {row['feature_type']} | {row['n_features']} | {row['source_auc']:.4f} | {row['target_baseline_auc']:.4f} | {row['target_tca_auc']:.4f} | {row['tca_improvement']:+.4f} |")
        
        # å¤±è´¥çš„åˆ†æ
        failed_analyses = {k: v for k, v in self.sweep_results.items() if v['status'] == 'failed'}
        if failed_analyses:
            report_content.append(f"\n### Failed Analyses\n")
            for feature_type, result in failed_analyses.items():
                report_content.append(f"- **{feature_type}**: {result.get('error', 'Unknown error')}")
        
        # å»ºè®®
        if not self.performance_summary.empty:
            report_content.append(f"\n## Recommendations\n")
            
            # åŸºäºç»“æœçš„å»ºè®®
            best_overall = self.performance_summary.loc[
                (self.performance_summary['target_tca_auc'] + self.performance_summary['source_auc']).idxmax()
            ]
            
            report_content.append(f"- **Recommended Feature Set**: {best_overall['feature_type']}")
            report_content.append(f"  - Source Performance: {best_overall['source_auc']:.4f} AUC")
            report_content.append(f"  - Target Performance: {best_overall['target_tca_auc']:.4f} AUC")
            report_content.append(f"  - TCA Improvement: {best_overall['tca_improvement']:+.4f}")
            
            # å¹³è¡¡æ€§å»ºè®®
            if best_improvement['tca_improvement'] > 0.01:
                report_content.append(f"- **TCA Domain Adaptation** shows positive effects for most feature sets")
            else:
                report_content.append(f"- **Limited TCA Benefits** - consider alternative UDA methods")
        
        report_content.append(f"\nDetailed results and visualizations available in: {self.output_dir}")
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / "feature_sweep_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_content))
        
        if self.verbose:
            print(f"âœ… æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return str(report_file)
    
    def save_results(self):
        """ä¿å­˜æ‰€æœ‰ç»“æœ"""
        # ä¿å­˜åŸå§‹ç»“æœ
        results_file = self.output_dir / "sweep_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.sweep_results, f, indent=2, ensure_ascii=False, default=str)
        
        # ä¿å­˜æ€§èƒ½æ€»ç»“
        if not self.performance_summary.empty:
            summary_file = self.output_dir / "performance_summary.csv"
            self.performance_summary.to_csv(summary_file, index=False)
            
            summary_json = self.output_dir / "performance_summary.json"
            self.performance_summary.to_json(summary_json, orient='records', indent=2)
        
        if self.verbose:
            print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
    
    def run_complete_feature_sweep(self) -> Dict:
        """è¿è¡Œå®Œæ•´çš„ç‰¹å¾æ‰«æåˆ†æ"""
        try:
            if self.verbose:
                print(f"ğŸš€ å¼€å§‹å®Œæ•´ç‰¹å¾æ‰«æåˆ†æ")
                print("=" * 60)
            
            # 1. å¹¶è¡Œè¿è¡Œç‰¹å¾æ‰«æ
            sweep_results = self.run_parallel_feature_sweep()
            
            # 2. ç”Ÿæˆå¯è§†åŒ–
            plot_path = self.plot_performance_comparison()
            
            # 3. ç”ŸæˆæŠ¥å‘Š
            report_path = self.generate_summary_report()
            
            # 4. ä¿å­˜ç»“æœ
            self.save_results()
            
            if self.verbose:
                print(f"\nâœ… ç‰¹å¾æ‰«æåˆ†æå®Œæˆï¼")
                print(f"ğŸ“ ç»“æœç›®å½•: {self.output_dir}")
                print(f"ğŸ“Š æ€§èƒ½å›¾è¡¨: {plot_path}")
                print(f"ğŸ“‹ åˆ†ææŠ¥å‘Š: {report_path}")
            
            return {
                'status': 'success',
                'results': sweep_results,
                'summary': self.performance_summary.to_dict('records') if not self.performance_summary.empty else [],
                'output_dir': str(self.output_dir),
                'plot_path': plot_path,
                'report_path': report_path
            }
            
        except Exception as e:
            self.logger.error(f"å®Œæ•´ç‰¹å¾æ‰«æåˆ†æå¤±è´¥: {e}")
            if self.verbose:
                import traceback
                traceback.print_exc()
            
            return {
                'status': 'failed',
                'error': str(e),
                'output_dir': str(self.output_dir)
            }


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ç‰¹å¾æ‰«æåˆ†æ - ä»best3åˆ°best58')
    parser.add_argument('--min_features', type=int, default=3,
                        help='æœ€å°ç‰¹å¾æ•° (default: 3)')
    parser.add_argument('--max_features', type=int, default=58,
                        help='æœ€å¤§ç‰¹å¾æ•° (default: 58)')
    parser.add_argument('--max_workers', type=int, default=None,
                        help='æœ€å¤§å¹¶è¡Œå·¥ä½œæ•° (default: auto)')
    parser.add_argument('--output_dir', type=str, default=None,
                        help='è¾“å‡ºç›®å½• (default: auto-generated)')
    parser.add_argument('--quiet', action='store_true', default=False,
                        help='é™é»˜æ¨¡å¼')
    
    args = parser.parse_args()
    
    if not args.quiet:
        print("ğŸ”¬ TabPFNåŒ»ç–—æ•°æ®ç‰¹å¾æ‰«æåˆ†æ")
        print("=" * 60)
        print(f"ğŸ“‹ åˆ†æé…ç½®:")
        print(f"   ç‰¹å¾èŒƒå›´: best{args.min_features} ~ best{args.max_features}")
        print(f"   å¹¶è¡Œå·¥ä½œæ•°: {args.max_workers or 'auto'}")
        print(f"   è¾“å‡ºç›®å½•: {args.output_dir or 'auto-generated'}")
        print("=" * 60)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = FeatureSweepAnalyzer(
        feature_range=(args.min_features, args.max_features),
        output_dir=args.output_dir,
        max_workers=args.max_workers,
        verbose=not args.quiet
    )
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    results = analyzer.run_complete_feature_sweep()
    
    if results['status'] == 'success':
        if not args.quiet:
            print(f"\nğŸ‰ åˆ†ææˆåŠŸå®Œæˆï¼")
            print(f"ğŸ“ æŸ¥çœ‹ç»“æœ: {results['output_dir']}")
            print(f"ğŸ“Š æ€§èƒ½å›¾è¡¨: {results['plot_path']}")
            print(f"ğŸ“‹ åˆ†ææŠ¥å‘Š: {results['report_path']}")
        exit(0)
    else:
        if not args.quiet:
            print(f"\nâŒ åˆ†æå¤±è´¥: {results['error']}")
        exit(1)


if __name__ == "__main__":
    main()