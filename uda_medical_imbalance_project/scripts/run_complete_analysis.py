#!/usr/bin/env python3
"""
å®Œæ•´çš„åŒ»ç–—æ•°æ®UDAåˆ†ææµç¨‹

è¿™ä¸ªè„šæœ¬æä¾›å®Œæ•´çš„åˆ†ææµç¨‹ï¼š
1. æºåŸŸ10æŠ˜äº¤å‰éªŒè¯å¯¹æ¯”ï¼ˆTabPFNã€è®ºæ–‡æ–¹æ³•ã€åŸºçº¿æ¨¡å‹ï¼‰
2. UDAåŸŸé€‚åº”æ–¹æ³•å¯¹æ¯”ï¼ˆåŸºäºADAPTåº“ï¼‰
3. å¯è§†åŒ–åˆ†æå’Œç»“æœå¯¹æ¯”

è¿è¡Œç¤ºä¾‹: python scripts/run_complete_analysis.py
"""

# TODO: ä¼˜åŒ–æµç¨‹ å°†é¡¹ç›®æ¨¡å—åŒ– æé«˜ä»£ç å¤ç”¨æ€§

import sys
from pathlib import Path
import numpy as np
import pandas as pd
import warnings
from datetime import datetime
import json
from typing import Dict, List, Tuple, Optional
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from data.loader import MedicalDataLoader
from uda.adapt_methods import is_adapt_available


class CompleteAnalysisRunner:
    """å®Œæ•´åˆ†ææµç¨‹è¿è¡Œå™¨"""
    
    def __init__(
        self,
        feature_set: str = 'best8',
        scaler_type: str = 'none',  # ä¸ä½¿ç”¨æ ‡å‡†åŒ–
        imbalance_method: str = 'none',  # ä¸ä½¿ç”¨ä¸å¹³è¡¡å¤„ç†
        cv_folds: int = 10,
        random_state: int = 42,
        output_dir: Optional[str] = None,
        verbose: bool = True
    ):
        """
        åˆå§‹åŒ–åˆ†æè¿è¡Œå™¨
        
        Args:
            feature_set: ç‰¹å¾é›†é€‰æ‹© ('best7', 'best8', 'best9', 'best10', 'all')
            scaler_type: æ ‡å‡†åŒ–æ–¹æ³• ('standard', 'robust', 'none')
            imbalance_method: ä¸å¹³è¡¡å¤„ç†æ–¹æ³•
            cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°
            random_state: éšæœºç§å­
            output_dir: è¾“å‡ºç›®å½•
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        """
        self.feature_set = feature_set
        self.scaler_type = scaler_type
        self.imbalance_method = imbalance_method
        self.cv_folds = cv_folds
        self.random_state = random_state
        self.verbose = verbose
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if output_dir is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = f"results/complete_analysis_{timestamp}"
        
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # å­˜å‚¨ç»“æœ
        self.results = {
            'config': {
                'feature_set': feature_set,
                'scaler_type': scaler_type,
                'imbalance_method': imbalance_method,
                'cv_folds': cv_folds,
                'random_state': random_state
            },
            'source_domain_cv': {},
            'uda_methods': {},
            'visualizations': {}
        }
        
        if self.verbose:
            print(f"ğŸ”§ å®Œæ•´åˆ†ææµç¨‹åˆå§‹åŒ–")
            print(f"   ç‰¹å¾é›†: {feature_set}")
            print(f"   æ ‡å‡†åŒ–: {scaler_type}")
            print(f"   ä¸å¹³è¡¡å¤„ç†: {imbalance_method}")
            print(f"   äº¤å‰éªŒè¯: {cv_folds}æŠ˜")
            print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    
    def load_data_for_cv(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, List[str]]:
        """åŠ è½½åŒ»ç–—æ•°æ®"""
        if self.verbose:
            print(f"\nğŸ“Š åŠ è½½åŒ»ç–—æ•°æ®...")
        
        loader = MedicalDataLoader()
        
        try:
            # ä¸ºäº†æ”¯æŒæ‰€æœ‰æ¨¡å‹ï¼Œéœ€è¦åŠ è½½åŒ…å«æ‰€æœ‰ç‰¹å¾çš„æ•°æ®é›†
            # TabPFNä¼šä»ä¸­é€‰æ‹©å­é›†ç‰¹å¾ï¼ŒåŸºçº¿æ¨¡å‹ä½¿ç”¨å…¨éƒ¨ç‰¹å¾
            # æ³¨æ„ï¼šæ•°æ®é›†Båªæœ‰58ä¸ªç‰¹å¾ï¼Œæ‰€ä»¥ä½¿ç”¨selected58ç‰¹å¾é›†
            
            # é¦–å…ˆå°è¯•åŠ è½½selected58ç‰¹å¾é›†ï¼ˆæ”¯æŒæ‰€æœ‰æ¨¡å‹ï¼‰
            data_A = loader.load_dataset('A', feature_type='selected58')
            data_B = loader.load_dataset('B', feature_type='selected58')
            
            # æå–ç‰¹å¾å’Œæ ‡ç­¾
            X_A = pd.DataFrame(data_A['X'], columns=data_A['feature_names'])
            y_A = pd.Series(data_A['y'])
            X_B = pd.DataFrame(data_B['X'], columns=data_B['feature_names'])
            y_B = pd.Series(data_B['y'])
            
            # éªŒè¯ç‰¹å¾ä¸€è‡´æ€§
            if data_A['feature_names'] != data_B['feature_names']:
                raise ValueError(f"æºåŸŸå’Œç›®æ ‡åŸŸç‰¹å¾ä¸ä¸€è‡´:\næºåŸŸ: {data_A['feature_names']}\nç›®æ ‡åŸŸ: {data_B['feature_names']}")
            
            # ä½¿ç”¨åŸå§‹ç‰¹å¾é¡ºåºï¼Œä¿æŒä¸€è‡´æ€§
            common_features = data_A['feature_names']
            
            if self.verbose:
                print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ:")
                print(f"   æºåŸŸA: {X_A.shape}, ç±»åˆ«åˆ†å¸ƒ: {dict(y_A.value_counts().sort_index())}")
                print(f"   ç›®æ ‡åŸŸB: {X_B.shape}, ç±»åˆ«åˆ†å¸ƒ: {dict(y_B.value_counts().sort_index())}")
                print(f"   åŠ è½½ç‰¹å¾é›†: selected58 (æ”¯æŒæ‰€æœ‰æ¨¡å‹)")
                print(f"   TabPFNå°†ä»ä¸­é€‰æ‹©: {self.feature_set} ç‰¹å¾")
                print(f"   åŸºçº¿æ¨¡å‹å°†ä½¿ç”¨: selected58 ç‰¹å¾")
                print(f"   ç‰¹å¾æ€»æ•°: {len(common_features)}")
            
            return X_A.values, y_A.values.astype(int), X_B.values, y_B.values.astype(int), common_features
            
        except Exception as e:
            if self.verbose:
                print(f"âŒ åŠ è½½æŒ‡å®šç‰¹å¾é›†å¤±è´¥: {e}")
                print(f"   å°è¯•ä½¿ç”¨all63ç‰¹å¾é›†ä½œä¸ºå¤‡é€‰...")
            
            # å¤‡é€‰æ–¹æ¡ˆï¼šå¦‚æœselected58ä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨best8
            try:
                fallback_feature_set = 'best8'
                if self.verbose:
                    print(f"   å°è¯•ä½¿ç”¨{fallback_feature_set}ç‰¹å¾é›†ä½œä¸ºå¤‡é€‰...")
                
                data_A = loader.load_dataset('A', feature_type=fallback_feature_set)
                data_B = loader.load_dataset('B', feature_type=fallback_feature_set)
                
                # æå–ç‰¹å¾å’Œæ ‡ç­¾
                X_A = pd.DataFrame(data_A['X'], columns=data_A['feature_names'])
                y_A = pd.Series(data_A['y'])
                X_B = pd.DataFrame(data_B['X'], columns=data_B['feature_names'])
                y_B = pd.Series(data_B['y'])
                
                # éªŒè¯ç‰¹å¾ä¸€è‡´æ€§
                if data_A['feature_names'] != data_B['feature_names']:
                    raise ValueError(f"æºåŸŸå’Œç›®æ ‡åŸŸç‰¹å¾ä¸ä¸€è‡´:\næºåŸŸ: {data_A['feature_names']}\nç›®æ ‡åŸŸ: {data_B['feature_names']}")
                
                common_features = data_A['feature_names']
                
                if self.verbose:
                    print(f"âœ… ä½¿ç”¨{fallback_feature_set}ç‰¹å¾é›†åŠ è½½å®Œæˆ:")
                    print(f"   æºåŸŸA: {X_A.shape}, ç±»åˆ«åˆ†å¸ƒ: {dict(y_A.value_counts().sort_index())}")
                    print(f"   ç›®æ ‡åŸŸB: {X_B.shape}, ç±»åˆ«åˆ†å¸ƒ: {dict(y_B.value_counts().sort_index())}")
                    print(f"   ç‰¹å¾æ•°é‡: {len(common_features)}")
                
                return X_A.values, y_A.values.astype(int), X_B.values, y_B.values.astype(int), common_features
                
            except Exception as e2:
                raise RuntimeError(f"æ•°æ®åŠ è½½å¤±è´¥ï¼Œå°è¯•äº†selected58å’Œ{fallback_feature_set}ç‰¹å¾é›†éƒ½å¤±è´¥:\nåŸå§‹é”™è¯¯: {e}\nå¤‡é€‰é”™è¯¯: {e2}")
    
    def load_data_for_uda(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, List[str]]:
        """
        ä¸“é—¨ä¸ºUDAåˆ†æåŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        åŒ…æ‹¬ç‰¹å¾ç­›é€‰ã€æ ‡å‡†åŒ–å’Œä¸å¹³è¡¡å¤„ç†
        """
        if self.verbose:
            print(f"\nğŸ“Š ä¸ºUDAåˆ†æåŠ è½½å’Œé¢„å¤„ç†æ•°æ®")
            print("=" * 50)
        
        from data.loader import MedicalDataLoader
        loader = MedicalDataLoader()
        
        try:
            # åŠ è½½æŒ‡å®šç‰¹å¾é›†çš„æ•°æ®
            if self.verbose:
                print(f"   åŠ è½½ç‰¹å¾é›†: {self.feature_set}")
            
            data_A = loader.load_dataset('A', feature_type=self.feature_set)
            data_B = loader.load_dataset('B', feature_type=self.feature_set)
            
            # æå–ç‰¹å¾å’Œæ ‡ç­¾
            X_A = pd.DataFrame(data_A['X'], columns=data_A['feature_names'])
            y_A = pd.Series(data_A['y'])
            X_B = pd.DataFrame(data_B['X'], columns=data_B['feature_names'])
            y_B = pd.Series(data_B['y'])
            
            # éªŒè¯ç‰¹å¾ä¸€è‡´æ€§
            if data_A['feature_names'] != data_B['feature_names']:
                raise ValueError(f"æºåŸŸå’Œç›®æ ‡åŸŸç‰¹å¾ä¸ä¸€è‡´:\næºåŸŸ: {data_A['feature_names']}\nç›®æ ‡åŸŸ: {data_B['feature_names']}")
            
            # ç¡®ä¿Aå’ŒBæ•°æ®é›†ä½¿ç”¨ç›¸åŒçš„ç‰¹å¾åˆ—ï¼ˆç‰¹å¾å¯¹é½ï¼‰
            common_features = list(set(X_A.columns) & set(X_B.columns))
            if len(common_features) != len(X_A.columns) or len(common_features) != len(X_B.columns):
                if self.verbose:
                    print(f"âš  è­¦å‘Š: Aå’ŒBæ•°æ®é›†ç‰¹å¾ä¸å®Œå…¨ä¸€è‡´")
                    print(f"  Aç‰¹å¾: {list(X_A.columns)}")
                    print(f"  Bç‰¹å¾: {list(X_B.columns)}")
                    print(f"  å…±åŒç‰¹å¾: {common_features}")
                # ä½¿ç”¨å…±åŒç‰¹å¾
                X_A = X_A[common_features]
                X_B = X_B[common_features]
                common_features = list(X_A.columns)
            else:
                common_features = list(X_A.columns)
            
            if self.verbose:
                print(f"âœ… åŸå§‹æ•°æ®åŠ è½½å®Œæˆ:")
                print(f"   æºåŸŸA: {X_A.shape}, ç±»åˆ«åˆ†å¸ƒ: {dict(y_A.value_counts().sort_index())}")
                print(f"   ç›®æ ‡åŸŸB: {X_B.shape}, ç±»åˆ«åˆ†å¸ƒ: {dict(y_B.value_counts().sort_index())}")
                print(f"   ç‰¹å¾åˆ—è¡¨: {common_features}")
                print(f"   ç‰¹å¾æ•°é‡: {len(common_features)}")
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            X_source = X_A.values
            y_source = y_A.values.astype(int)
            X_target = X_B.values
            y_target = y_B.values.astype(int)
            
            # åº”ç”¨é¢„å¤„ç†ï¼ˆæ ‡å‡†åŒ–å’Œä¸å¹³è¡¡å¤„ç†ï¼‰
            X_source_processed, y_source_processed, X_target_processed = self._preprocess_uda_data(
                X_source, y_source, X_target, common_features
            )
            
            return X_source_processed, y_source_processed, X_target_processed, y_target.astype(int), common_features
            
        except Exception as e:
            # å¤‡é€‰æ–¹æ¡ˆï¼šå¦‚æœæŒ‡å®šç‰¹å¾é›†ä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨best8
            try:
                fallback_feature_set = 'best8' if self.feature_set != 'best8' else 'best7'
                if self.verbose:
                    print(f"   å°è¯•ä½¿ç”¨{fallback_feature_set}ç‰¹å¾é›†ä½œä¸ºå¤‡é€‰...")
                
                data_A = loader.load_dataset('A', feature_type=fallback_feature_set)
                data_B = loader.load_dataset('B', feature_type=fallback_feature_set)
                
                # æå–ç‰¹å¾å’Œæ ‡ç­¾
                X_A = pd.DataFrame(data_A['X'], columns=data_A['feature_names'])
                y_A = pd.Series(data_A['y'])
                X_B = pd.DataFrame(data_B['X'], columns=data_B['feature_names'])
                y_B = pd.Series(data_B['y'])
                
                # éªŒè¯ç‰¹å¾ä¸€è‡´æ€§
                if data_A['feature_names'] != data_B['feature_names']:
                    raise ValueError(f"æºåŸŸå’Œç›®æ ‡åŸŸç‰¹å¾ä¸ä¸€è‡´:\næºåŸŸ: {data_A['feature_names']}\nç›®æ ‡åŸŸ: {data_B['feature_names']}")
                
                common_features = data_A['feature_names']
                
                if self.verbose:
                    print(f"âœ… ä½¿ç”¨{fallback_feature_set}ç‰¹å¾é›†åŠ è½½å®Œæˆ:")
                    print(f"   æºåŸŸA: {X_A.shape}, ç±»åˆ«åˆ†å¸ƒ: {dict(y_A.value_counts().sort_index())}")
                    print(f"   ç›®æ ‡åŸŸB: {X_B.shape}, ç±»åˆ«åˆ†å¸ƒ: {dict(y_B.value_counts().sort_index())}")
                    print(f"   ç‰¹å¾æ•°é‡: {len(common_features)}")
                
                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                X_source = X_A.values
                y_source = y_A.values.astype(int)
                X_target = X_B.values
                y_target = y_B.values.astype(int)
                
                # åº”ç”¨é¢„å¤„ç†
                X_source_processed, y_source_processed, X_target_processed = self._preprocess_uda_data(
                    X_source, y_source, X_target, common_features
                )
                
                return X_source_processed, y_source_processed, X_target_processed, y_target.astype(int), common_features
                
            except Exception as e2:
                raise RuntimeError(f"UDAæ•°æ®åŠ è½½å¤±è´¥ï¼Œå°è¯•äº†{self.feature_set}å’Œ{fallback_feature_set}ç‰¹å¾é›†éƒ½å¤±è´¥:\nåŸå§‹é”™è¯¯: {e}\nå¤‡é€‰é”™è¯¯: {e2}")
    
    def _preprocess_uda_data(self, X_source: np.ndarray, y_source: np.ndarray, X_target: np.ndarray, 
                           feature_names: List[str]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        å¯¹UDAæ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼ˆæ ‡å‡†åŒ–å’Œä¸å¹³è¡¡å¤„ç†ï¼‰
        å‚è€ƒtest_cross_validation.pyä¸­CrossValidationEvaluatorçš„åšæ³•
        """
        if self.verbose:
            print(f"\nğŸ”§ UDAæ•°æ®é¢„å¤„ç†:")
            print(f"   æ ‡å‡†åŒ–æ–¹æ³•: {self.scaler_type}")
            print(f"   ä¸å¹³è¡¡å¤„ç†: {self.imbalance_method}")
        
        from config.settings import get_categorical_features
        
        # è·å–ç±»åˆ«ç‰¹å¾ç´¢å¼•
        categorical_features = get_categorical_features(self.feature_set)
        categorical_indices = [i for i, name in enumerate(feature_names) if name in categorical_features]
        
        # 1. æ ‡å‡†åŒ–å¤„ç† - UDAæ•°æ®ä¸ä½¿ç”¨æ ‡å‡†åŒ–
        # å¯¹äºUDAåˆ†æï¼Œè·³è¿‡æ ‡å‡†åŒ–æ­¥éª¤ï¼Œä½¿ç”¨åŸå§‹ç‰¹å¾
        X_source_scaled = X_source.copy()
        X_target_scaled = X_target.copy()
        if self.verbose:
            print(f"   âš  UDAåˆ†æè·³è¿‡æ ‡å‡†åŒ–ï¼Œä½¿ç”¨åŸå§‹ç‰¹å¾")
        
        # 2. ä¸å¹³è¡¡å¤„ç†ï¼ˆåªå¯¹æºåŸŸæ•°æ®ï¼‰
        if self.imbalance_method != 'none':
            try:
                if self.imbalance_method == 'smote':
                    from imblearn.over_sampling import SMOTE
                    sampler = SMOTE(random_state=self.random_state)
                elif self.imbalance_method == 'smotenc':
                    from imblearn.over_sampling import SMOTENC
                    sampler = SMOTENC(categorical_features=categorical_indices, random_state=self.random_state)
                elif self.imbalance_method == 'borderline_smote':
                    from imblearn.over_sampling import BorderlineSMOTE
                    sampler = BorderlineSMOTE(random_state=self.random_state)
                elif self.imbalance_method == 'kmeans_smote':
                    from imblearn.over_sampling import KMeansSMOTE
                    sampler = KMeansSMOTE(random_state=self.random_state)
                elif self.imbalance_method == 'adasyn':
                    from imblearn.over_sampling import ADASYN
                    sampler = ADASYN(random_state=self.random_state)
                elif self.imbalance_method == 'smote_tomek':
                    from imblearn.combine import SMOTETomek
                    sampler = SMOTETomek(random_state=self.random_state)
                elif self.imbalance_method == 'random_under':
                    from imblearn.under_sampling import RandomUnderSampler
                    sampler = RandomUnderSampler(random_state=self.random_state)
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„ä¸å¹³è¡¡å¤„ç†æ–¹æ³•: {self.imbalance_method}")
                
                # åº”ç”¨ä¸å¹³è¡¡å¤„ç†
                X_source_resampled, y_source_resampled = sampler.fit_resample(X_source_scaled, y_source)
                
                # ç¡®ä¿è¿”å›numpyæ•°ç»„
                X_source_resampled = np.array(X_source_resampled)
                y_source_resampled = np.array(y_source_resampled)
                
                if self.verbose:
                    print(f"   âœ… ä¸å¹³è¡¡å¤„ç†å®Œæˆ")
                    print(f"   æºåŸŸæ ·æœ¬æ•°å˜åŒ–: {len(y_source)} -> {len(y_source_resampled)}")
                    print(f"   æºåŸŸç±»åˆ«åˆ†å¸ƒ: {dict(pd.Series(y_source_resampled).value_counts().sort_index())}")
                
            except Exception as e:
                if self.verbose:
                    print(f"   âš  ä¸å¹³è¡¡å¤„ç†å¤±è´¥: {e}, ä½¿ç”¨åŸå§‹æ•°æ®")
                X_source_resampled = X_source_scaled
                y_source_resampled = y_source
        else:
            X_source_resampled = X_source_scaled
            y_source_resampled = y_source
            if self.verbose:
                print(f"   âš  è·³è¿‡ä¸å¹³è¡¡å¤„ç†")
        
        return X_source_resampled, y_source_resampled, X_target_scaled
    
    def run_source_domain_cv(self, X_source: np.ndarray, y_source: np.ndarray, feature_names: List[str]) -> Dict:
        """è¿è¡ŒæºåŸŸ10æŠ˜äº¤å‰éªŒè¯å¯¹æ¯”"""
        if self.verbose:
            print(f"\nğŸ”¬ æºåŸŸ10æŠ˜äº¤å‰éªŒè¯å¯¹æ¯”")
            print("=" * 50)
        
        # è½¬æ¢ä¸ºDataFrameï¼Œä½¿ç”¨æ­£ç¡®çš„ç‰¹å¾åç§°
        X_df = pd.DataFrame(X_source, columns=feature_names)
        y_series = pd.Series(y_source)
        
        if self.verbose:
            print(f"   æ•°æ®å½¢çŠ¶: {X_df.shape}")
            print(f"   ç‰¹å¾åˆ—è¡¨: {list(X_df.columns)}")
        
        # è¿è¡Œæ¨¡å‹å¯¹æ¯”ï¼ŒåŒ…å«æ‰€æœ‰åŸºçº¿æ¨¡å‹
        # TabPFNä½¿ç”¨æŒ‡å®šç‰¹å¾é›†ï¼ŒåŸºçº¿æ¨¡å‹ä½¿ç”¨selected58ç‰¹å¾é›†
        
        if self.verbose:
            print(f"   TabPFNå°†ä½¿ç”¨: {self.feature_set} ç‰¹å¾é›† + {self.scaler_type} æ ‡å‡†åŒ– + {self.imbalance_method} ä¸å¹³è¡¡å¤„ç†")
            print(f"   åŸºçº¿æ¨¡å‹å°†ä½¿ç”¨: selected58 ç‰¹å¾é›† + æ— é¢„å¤„ç†")
        
        # å¯¼å…¥å¹¶ä½¿ç”¨run_model_comparison_cvå‡½æ•°
        try:
            from evaluation.cross_validation import run_model_comparison_cv
            
            cv_results = run_model_comparison_cv(
                X_df, y_series,
                feature_set=self.feature_set,  # TabPFNä½¿ç”¨æŒ‡å®šç‰¹å¾é›†ï¼ŒåŸºçº¿æ¨¡å‹åœ¨å†…éƒ¨ä½¿ç”¨selected58
                scaler_type=self.scaler_type,  # TabPFNä½¿ç”¨æŒ‡å®šæ ‡å‡†åŒ–æ–¹æ³•
                imbalance_method=self.imbalance_method,  # TabPFNä½¿ç”¨æŒ‡å®šä¸å¹³è¡¡å¤„ç†æ–¹æ³•
                cv_folds=self.cv_folds,
                random_state=self.random_state,
                verbose=self.verbose
            )
        except ImportError as e:
            if self.verbose:
                print(f"âŒ æ— æ³•å¯¼å…¥äº¤å‰éªŒè¯æ¨¡å—: {e}")
            return {}
        
        # ä¿å­˜ç»“æœ
        self.results['source_domain_cv'] = cv_results
        
        # ä¿å­˜è¯¦ç»†ç»“æœåˆ°æ–‡ä»¶
        cv_results_file = self.output_dir / "source_domain_cv_results.json"
        with open(cv_results_file, 'w', encoding='utf-8') as f:
            json.dump(cv_results, f, indent=2, ensure_ascii=False, default=str)
        
        if self.verbose:
            print(f"ğŸ“ æºåŸŸCVç»“æœå·²ä¿å­˜: {cv_results_file}")
        
        return cv_results
    
    def run_uda_methods(
        self, 
        X_source: np.ndarray, 
        y_source: np.ndarray,
        X_target: np.ndarray, 
        y_target: np.ndarray,
        feature_names: List[str]
    ) -> Dict:
        """è¿è¡ŒUDAæ–¹æ³•å¯¹æ¯”"""
        if self.verbose:
            print(f"\nğŸ”„ UDAæ–¹æ³•å¯¹æ¯”åˆ†æ")
            print("=" * 50)
        
        # æ£€æŸ¥ADAPTåº“å¯ç”¨æ€§
        if not is_adapt_available():
            print("âŒ ADAPTåº“ä¸å¯ç”¨ï¼Œè·³è¿‡UDAåˆ†æ")
            return {}
        
        # é€‰æ‹©è¦æµ‹è¯•çš„UDAæ–¹æ³•
        # uda_methods_to_test = ['TCA', 'SA', 'CORAL', 'KMM']
        uda_methods_to_test = ['TCA']
        uda_results = {}
        
        # åˆ›å»ºåŸºç¡€ä¼°è®¡å™¨
        try:
            from tabpfn import TabPFNClassifier
            base_estimator = TabPFNClassifier(
                n_estimators=32, 
                random_state=self.random_state,
                ignore_pretraining_limits=True  # å…è®¸è¶…è¿‡500ä¸ªç‰¹å¾
            )
            if self.verbose:
                print("âœ… ä½¿ç”¨TabPFNä½œä¸ºåŸºç¡€ä¼°è®¡å™¨ (ignore_pretraining_limits=True)")
        except ImportError:
            from sklearn.linear_model import LogisticRegression
            base_estimator = LogisticRegression(penalty=None, random_state=self.random_state, max_iter=1000)
            if self.verbose:
                print("âš  ä½¿ç”¨LogisticRegressionä½œä¸ºfallback")
        
        # 1. é¦–å…ˆæµ‹è¯•æ— UDAçš„TabPFNåŸºçº¿ï¼ˆç›´æ¥åœ¨ç›®æ ‡åŸŸä¸Šæµ‹è¯•ï¼‰
        if self.verbose:
            print(f"\n--- æµ‹è¯•åŸºçº¿: æ— UDAçš„TabPFN ---")
        
        try:
            # ç›´æ¥ç”¨TabPFNåœ¨æºåŸŸè®­ç»ƒï¼Œç›®æ ‡åŸŸæµ‹è¯•ï¼ˆæ— åŸŸé€‚åº”ï¼‰
            from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score
            
            # è®­ç»ƒTabPFNæ¨¡å‹
            baseline_model = base_estimator
            baseline_model.fit(X_source, y_source)
            
            # åœ¨ç›®æ ‡åŸŸä¸Šé¢„æµ‹
            y_pred = baseline_model.predict(X_target)
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            accuracy = accuracy_score(y_target, y_pred)
            f1 = f1_score(y_target, y_pred, average='binary')
            precision = precision_score(y_target, y_pred, average='binary')
            recall = recall_score(y_target, y_pred, average='binary')
            
            try:
                y_proba = baseline_model.predict_proba(X_target)
                if y_proba is not None and len(y_proba.shape) > 1:
                    auc = roc_auc_score(y_target, y_proba[:, 1])
                    # ä¿å­˜é¢„æµ‹æ•°æ®ç”¨äºROCæ›²çº¿ç»˜åˆ¶
                    y_proba_for_roc = y_proba[:, 1] if len(y_proba.shape) > 1 else y_proba
                else:
                    auc = np.nan
                    y_proba_for_roc = None
            except:
                auc = np.nan
                y_proba_for_roc = None
            
            # å­˜å‚¨åŸºçº¿ç»“æœ
            baseline_results = {
                'method_name': 'TabPFN_NoUDA',
                'accuracy': float(accuracy),
                'auc': float(auc) if not np.isnan(auc) else None,
                'f1': float(f1),
                'precision': float(precision),
                'recall': float(recall),
                'is_baseline': True,
                'y_true': y_target.tolist() if hasattr(y_target, 'tolist') else list(y_target),
                'y_pred_proba': y_proba_for_roc.tolist() if y_proba_for_roc is not None else None
            }
            
            uda_results['TabPFN_NoUDA'] = baseline_results
            
            if self.verbose:
                print(f"âœ… TabPFNåŸºçº¿ å®Œæˆ:")
                print(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
                if not np.isnan(auc):
                    print(f"   AUC: {auc:.4f}")
                print(f"   F1: {f1:.4f}")
                
        except Exception as e:
            if self.verbose:
                print(f"âŒ TabPFNåŸºçº¿ å¤±è´¥: {e}")
            uda_results['TabPFN_NoUDA'] = {'error': str(e), 'is_baseline': True}
        
        # 2. æµ‹è¯•ä¼ ç»ŸåŸºçº¿æ¨¡å‹ï¼ˆPKUPHã€Mayoã€Paper_LRï¼‰- åªåœ¨ç›®æ ‡åŸŸBä¸Šæµ‹è¯•
        baseline_models = ['PKUPH', 'Mayo', 'Paper_LR']
        
        # åŠ è½½ç›®æ ‡åŸŸBçš„selected58ç‰¹å¾é›†æ•°æ®ï¼ˆç”¨äºä¼ ç»ŸåŸºçº¿æ¨¡å‹ï¼‰
        try:
            from data.loader import MedicalDataLoader
            loader = MedicalDataLoader()
            data_B_selected58 = loader.load_dataset('B', feature_type='selected58')
            X_target_selected58 = pd.DataFrame(data_B_selected58['X'], columns=data_B_selected58['feature_names'])
            y_target_selected58 = pd.Series(data_B_selected58['y'])
            
            if self.verbose:
                print(f"\n--- æµ‹è¯•ä¼ ç»ŸåŸºçº¿æ¨¡å‹ï¼ˆä»…åœ¨ç›®æ ‡åŸŸBä¸Šæµ‹è¯•ï¼‰---")
                print(f"   ç›®æ ‡åŸŸBæ•°æ®: {X_target_selected58.shape}")
                print(f"   ç‰¹å¾é›†: selected58 ({len(data_B_selected58['feature_names'])}ä¸ªç‰¹å¾)")
        
        except Exception as e:
            if self.verbose:
                print(f"âŒ æ— æ³•åŠ è½½ç›®æ ‡åŸŸBçš„selected58æ•°æ®: {e}")
            X_target_selected58 = None
            y_target_selected58 = None
        
        if X_target_selected58 is not None:
            for model_name in baseline_models:
                if self.verbose:
                    print(f"\n--- æµ‹è¯•åŸºçº¿æ¨¡å‹: {model_name} ---")
                
                try:
                    # åŸºçº¿æ¨¡å‹ä½¿ç”¨10æŠ˜äº¤å‰éªŒè¯
                    from evaluation.cross_validation import CrossValidationEvaluator
                    
                    # åˆ›å»ºåŸºçº¿æ¨¡å‹è¯„ä¼°å™¨
                    evaluator = CrossValidationEvaluator(
                        model_type=model_name.lower(),
                        feature_set='selected58',  # å¼ºåˆ¶ä½¿ç”¨selected58ç‰¹å¾é›†
                        scaler_type='none',        # åŸºçº¿æ¨¡å‹ä¸ä½¿ç”¨æ ‡å‡†åŒ–
                        imbalance_method='none',   # åŸºçº¿æ¨¡å‹ä¸ä½¿ç”¨ä¸å¹³è¡¡å¤„ç†
                        cv_folds=10,
                        random_state=self.random_state,
                        verbose=False
                    )
                    
                    if self.verbose:
                        print(f"   æ¨¡å‹é…ç½®: {model_name}")
                        print(f"   ç‰¹å¾é›†: selected58")
                        print(f"   å®é™…ä½¿ç”¨ç‰¹å¾æ•°: {len(evaluator.features)}")
                        print(f"   å®é™…ä½¿ç”¨ç‰¹å¾: {evaluator.features}")
                        print(f"   æµ‹è¯•æ–¹å¼: 10æŠ˜äº¤å‰éªŒè¯")
                    
                    # åŸºçº¿æ¨¡å‹åœ¨ç›®æ ‡åŸŸBä¸Šè¿›è¡Œ10æŠ˜äº¤å‰éªŒè¯
                    cv_result = evaluator.run_cross_validation(X_target_selected58, y_target_selected58)
                    
                    if cv_result['summary'] and 'auc_mean' in cv_result['summary']:
                        summary = cv_result['summary']
                        
                        # ä»äº¤å‰éªŒè¯ç»“æœä¸­æå–é¢„æµ‹æ•°æ®
                        prediction_data = {}
                        if 'predictions' in cv_result and cv_result['predictions']:
                            pred_data = cv_result['predictions']
                            if 'y_true' in pred_data and 'y_pred_proba' in pred_data and pred_data['y_pred_proba']:
                                prediction_data = {
                                    'y_true': pred_data['y_true'],
                                    'y_pred_proba': pred_data['y_pred_proba']
                                }
                        
                        # å­˜å‚¨åŸºçº¿æ¨¡å‹ç»“æœ
                        baseline_model_results = {
                            'method_name': model_name,
                            'accuracy': float(summary.get('accuracy_mean', 0)),
                            'auc': float(summary.get('auc_mean', 0)) if summary.get('auc_mean') is not None else None,
                            'f1': float(summary.get('f1_mean', 0)),
                            'precision': float(summary.get('precision_mean', 0)),
                            'recall': float(summary.get('recall_mean', 0)),
                            'is_baseline': True,
                            'test_type': 'target_domain_cv',  # æ ‡è®°ä¸ºç›®æ ‡åŸŸäº¤å‰éªŒè¯
                            'baseline_category': 'traditional_baseline',  # æ ‡è®°ä¸ºä¼ ç»ŸåŸºçº¿
                            'feature_set_used': 'selected58',  # è®°å½•ä½¿ç”¨çš„ç‰¹å¾é›†
                            'actual_features_count': len(evaluator.features),  # è®°å½•å®é™…ç‰¹å¾æ•°
                        }
                        
                        # æ·»åŠ é¢„æµ‹æ•°æ®
                        baseline_model_results.update(prediction_data)
                        
                        uda_results[model_name] = baseline_model_results
                        
                        if self.verbose:
                            print(f"âœ… {model_name}åŸºçº¿ å®Œæˆ:")
                            print(f"   å‡†ç¡®ç‡: {baseline_model_results['accuracy']:.4f}")
                            if baseline_model_results['auc'] is not None:
                                print(f"   AUC: {baseline_model_results['auc']:.4f}")
                            print(f"   F1: {baseline_model_results['f1']:.4f}")
                            print(f"   å®é™…ç‰¹å¾æ•°: {baseline_model_results['actual_features_count']}")
                            print(f"   æµ‹è¯•æ ·æœ¬æ•°: {len(y_target_selected58)}")
                    else:
                        raise ValueError(f"{model_name}æ¨¡å‹æœªè¿”å›æœ‰æ•ˆçš„æ€§èƒ½æŒ‡æ ‡")
                        
                except Exception as e:
                    if self.verbose:
                        print(f"âŒ {model_name}åŸºçº¿ å¤±è´¥: {e}")
                    uda_results[model_name] = {
                        'error': str(e), 
                        'method_name': model_name,
                        'is_baseline': True,
                        'test_type': 'target_domain_cv',
                        'baseline_category': 'traditional_baseline'
                    }
        
        # 3. æµ‹è¯•æœºå™¨å­¦ä¹ åŸºçº¿æ¨¡å‹ï¼ˆä½¿ç”¨ä¸TabPFNç›¸åŒçš„ç‰¹å¾é›†å’Œé¢„å¤„ç†ï¼‰- åªåœ¨ç›®æ ‡åŸŸBä¸Šæµ‹è¯•
        ml_baseline_models = ['SVM', 'DT', 'RF', 'GBDT', 'XGBoost']
        
        # å‡†å¤‡ç›®æ ‡åŸŸBçš„é¢„å¤„ç†æ•°æ®ï¼ˆä¸TabPFNä½¿ç”¨ç›¸åŒçš„ç‰¹å¾é›†ï¼‰
        try:
            # ä½¿ç”¨ä¸UDAåˆ†æç›¸åŒçš„é¢„å¤„ç†æ•°æ®
            X_target_df = pd.DataFrame(X_target, columns=feature_names)
            y_target_series = pd.Series(y_target)
            
            if self.verbose:
                print(f"\n--- æµ‹è¯•æœºå™¨å­¦ä¹ åŸºçº¿æ¨¡å‹ï¼ˆä»…åœ¨ç›®æ ‡åŸŸBä¸Šæµ‹è¯•ï¼‰---")
                print(f"   ç›®æ ‡åŸŸBæ•°æ®: {X_target_df.shape}")
                print(f"   ç‰¹å¾é›†: {self.feature_set} ({len(feature_names)}ä¸ªç‰¹å¾)")
                print(f"   é¢„å¤„ç†: {self.scaler_type} æ ‡å‡†åŒ– + {self.imbalance_method} ä¸å¹³è¡¡å¤„ç†")
            
            for model_name in ml_baseline_models:
                if self.verbose:
                    print(f"\n--- æµ‹è¯•æœºå™¨å­¦ä¹ åŸºçº¿æ¨¡å‹: {model_name} ---")
                
                try:
                    # æ£€æŸ¥XGBoostå¯ç”¨æ€§
                    if model_name.lower() == 'xgboost':
                        try:
                            import xgboost as xgb  # ä½¿ç”¨åˆ«åé¿å…æœªä½¿ç”¨è­¦å‘Š
                            if self.verbose:
                                print(f"   âœ… XGBoostå¯ç”¨ï¼Œç‰ˆæœ¬: {xgb.__version__}")
                        except ImportError:
                            if self.verbose:
                                print(f"   âš  XGBoostä¸å¯ç”¨ï¼Œè·³è¿‡")
                            continue
                    
                    # ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°å™¨
                    from evaluation.cross_validation import CrossValidationEvaluator
                    
                    # åˆ›å»ºæœºå™¨å­¦ä¹ åŸºçº¿æ¨¡å‹è¯„ä¼°å™¨ï¼ˆä½¿ç”¨ä¸TabPFNç›¸åŒçš„é…ç½®ï¼‰
                    evaluator = CrossValidationEvaluator(
                        model_type=model_name.lower(),
                        feature_set=self.feature_set,      # ä½¿ç”¨ä¸TabPFNç›¸åŒçš„ç‰¹å¾é›†
                        scaler_type=self.scaler_type,      # ä½¿ç”¨ä¸TabPFNç›¸åŒçš„æ ‡å‡†åŒ–
                        imbalance_method=self.imbalance_method,  # ä½¿ç”¨ä¸TabPFNç›¸åŒçš„ä¸å¹³è¡¡å¤„ç†
                        cv_folds=10,
                        random_state=self.random_state,
                        verbose=False
                    )
                    
                    if self.verbose:
                        print(f"   æ¨¡å‹é…ç½®: {model_name}")
                        print(f"   ç‰¹å¾é›†: {self.feature_set}")
                        print(f"   ç‰¹å¾æ•°é‡: {len(evaluator.features)}")
                        print(f"   é¢„å¤„ç†: {self.scaler_type} + {self.imbalance_method}")
                    
                    # è¿è¡Œ10æŠ˜äº¤å‰éªŒè¯ï¼ˆåœ¨ç›®æ ‡åŸŸBä¸Šï¼‰
                    cv_result = evaluator.run_cross_validation(X_target_df, y_target_series)
                    
                    if cv_result['summary'] and 'auc_mean' in cv_result['summary']:
                        summary = cv_result['summary']
                        
                        # ä»äº¤å‰éªŒè¯ç»“æœä¸­æå–é¢„æµ‹æ•°æ®
                        prediction_data = {}
                        if 'predictions' in cv_result and cv_result['predictions']:
                            pred_data = cv_result['predictions']
                            if 'y_true' in pred_data and 'y_pred_proba' in pred_data and pred_data['y_pred_proba']:
                                prediction_data = {
                                    'y_true': pred_data['y_true'],
                                    'y_pred_proba': pred_data['y_pred_proba']
                                }
                        
                        # å­˜å‚¨æœºå™¨å­¦ä¹ åŸºçº¿æ¨¡å‹ç»“æœ
                        ml_baseline_results = {
                            'method_name': model_name,
                            'accuracy': float(summary.get('accuracy_mean', 0)),
                            'auc': float(summary.get('auc_mean', 0)) if summary.get('auc_mean') is not None else None,
                            'f1': float(summary.get('f1_mean', 0)),
                            'precision': float(summary.get('precision_mean', 0)),
                            'recall': float(summary.get('recall_mean', 0)),
                            'is_baseline': True,
                            'test_type': 'target_domain_cv',  # æ ‡è®°ä¸ºç›®æ ‡åŸŸäº¤å‰éªŒè¯
                            'baseline_category': 'ml_baseline'  # æ ‡è®°ä¸ºæœºå™¨å­¦ä¹ åŸºçº¿
                        }
                        
                        # æ·»åŠ é¢„æµ‹æ•°æ®
                        ml_baseline_results.update(prediction_data)
                        
                        uda_results[model_name] = ml_baseline_results
                        
                        if self.verbose:
                            print(f"âœ… {model_name}åŸºçº¿ å®Œæˆ:")
                            print(f"   å‡†ç¡®ç‡: {ml_baseline_results['accuracy']:.4f}")
                            if ml_baseline_results['auc'] is not None:
                                print(f"   AUC: {ml_baseline_results['auc']:.4f}")
                            print(f"   F1: {ml_baseline_results['f1']:.4f}")
                    else:
                        raise ValueError(f"{model_name}æ¨¡å‹æœªè¿”å›æœ‰æ•ˆçš„æ€§èƒ½æŒ‡æ ‡")
                        
                except Exception as e:
                    if self.verbose:
                        print(f"âŒ {model_name}åŸºçº¿ å¤±è´¥: {e}")
                    uda_results[model_name] = {
                        'error': str(e), 
                        'method_name': model_name,
                        'is_baseline': True,
                        'test_type': 'target_domain_cv',
                        'baseline_category': 'ml_baseline'
                    }
        
        except Exception as e:
            if self.verbose:
                print(f"âŒ æ— æ³•æµ‹è¯•æœºå™¨å­¦ä¹ åŸºçº¿æ¨¡å‹: {e}")
        
        # 4. ç„¶åæµ‹è¯•å„ç§UDAæ–¹æ³•
        for method_name in uda_methods_to_test:
            if self.verbose:
                print(f"\n--- æµ‹è¯•UDAæ–¹æ³•: {method_name} ---")
            
            try:
                # æ•°æ®å·²ç»åœ¨load_data_for_udaä¸­é¢„å¤„ç†å®Œæˆï¼Œç›´æ¥ä½¿ç”¨
                if self.verbose:
                    print(f"  ä½¿ç”¨é¢„å¤„ç†åçš„æ•°æ®: {len(feature_names)}ä¸ªç‰¹å¾")
                    print(f"  ç‰¹å¾åˆ—è¡¨: {feature_names}")
                
                # ä½¿ç”¨create_uda_processorä¾¿æ·å‡½æ•°åˆ›å»ºUDAå¤„ç†å™¨
                from preprocessing.uda_processor import create_uda_processor
                
                processor = create_uda_processor(
                    method_name=method_name,
                    base_estimator=base_estimator,
                    save_results=False
                )
                
                # é’ˆå¯¹ä¸åŒæ–¹æ³•ä¼˜åŒ–å‚æ•°ï¼ˆå‚è€ƒreal_data_visualization.pyï¼‰
                if method_name == 'TCA':
                    # TCAå‚æ•°ä¼˜åŒ–ï¼šé’ˆå¯¹åŒ»ç–—æ•°æ®çš„å°æ ·æœ¬ã€é«˜ç»´ç‰¹å¾
                    processor.config.method_params.update({
                        'n_components': None,  
                        'mu': 0.1,  # è¾ƒå°çš„muå€¼ï¼Œå‡å°‘æ­£åˆ™åŒ–ï¼Œé€‚åˆå°æ ·æœ¬
                        'kernel': 'linear'  # çº¿æ€§æ ¸ï¼Œé€‚åˆåŒ»ç–—ç‰¹å¾
                    })
                    if self.verbose:
                        print(f"  TCAå‚æ•°ä¼˜åŒ–: n_components=None, mu=1, kernel=linear")
                elif method_name == 'SA':
                    # SAå‚æ•°ä¼˜åŒ–
                    processor.config.method_params.update({
                        'n_components': None  # è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç»„ä»¶æ•°
                    })
                    if self.verbose:
                        print(f"  SAå‚æ•°ä¼˜åŒ–: n_components=auto")
                
                if self.verbose:
                    print(f"  åˆ›å»º{method_name}å¤„ç†å™¨æˆåŠŸ")
                
                # è¿è¡ŒUDAæ–¹æ³•ï¼ˆä½¿ç”¨é¢„å¤„ç†åçš„æ•°æ®ï¼‰
                uda_method, method_results = processor.fit_transform(
                    X_source, y_source, X_target, y_target
                )
                
                if self.verbose:
                    print(f"  {method_name}æ‹Ÿåˆå®Œæˆ")
                    
                    # æ˜¾ç¤ºå®é™…ä½¿ç”¨çš„n_componentsæ•°é‡ï¼ˆä¸­é—´ç»“æœï¼‰
                    if hasattr(uda_method, 'adapt_model'):
                        adapt_model = uda_method.adapt_model
                        if method_name == 'TCA' and hasattr(adapt_model, 'vectors_'):
                            actual_n_components = adapt_model.vectors_.shape[1]
                            print(f"  TCAå®é™…n_components: {actual_n_components} (è¾“å…¥æ—¶è®¾ä¸ºNone)")
                        elif method_name == 'SA' and hasattr(adapt_model, 'pca_src_'):
                            actual_n_components = adapt_model.pca_src_.n_components_
                            print(f"  SAå®é™…n_components: {actual_n_components} (è¾“å…¥æ—¶è®¾ä¸ºNone)")
                        elif method_name in ['TCA', 'SA'] and hasattr(adapt_model, 'n_components'):
                            print(f"  {method_name}é…ç½®n_components: {adapt_model.n_components}")
                    
                    print(f"  æ€§èƒ½ç»“æœ: {method_results}")
                
                # éªŒè¯ç»“æœæœ‰æ•ˆæ€§
                if not method_results or 'accuracy' not in method_results:
                    raise ValueError(f"{method_name}æ–¹æ³•æœªè¿”å›æœ‰æ•ˆçš„æ€§èƒ½æŒ‡æ ‡")
                
                # æ€§èƒ½æŒ‡æ ‡å·²ç»ä»UDAProcessorè·å–
                
                # ç”Ÿæˆå¯è§†åŒ–åˆ†æ
                method_output_dir = self.output_dir / f"uda_{method_name}"
                method_output_dir.mkdir(exist_ok=True)
                
                try:
                    from preprocessing.uda_visualizer import create_uda_visualizer
                    visualizer = create_uda_visualizer(
                        save_plots=True,
                        output_dir=str(method_output_dir)
                    )
                    
                    # è½¬æ¢uda_methodç±»å‹ä»¥åŒ¹é…visualizerçš„æœŸæœ›
                    viz_results = visualizer.visualize_domain_adaptation_complete(
                        X_source, y_source,
                        X_target, y_target,
                        uda_method=uda_method,
                        method_name=method_name
                    )
                except Exception as viz_error:
                    if self.verbose:
                        print(f"  âš  å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {viz_error}")
                    viz_results = {'error': str(viz_error)}
                
                # è·å–é¢„æµ‹æ•°æ®ç”¨äºROCæ›²çº¿ç»˜åˆ¶
                try:
                    # ä½¿ç”¨UDAæ–¹æ³•è¿›è¡Œé¢„æµ‹
                    y_pred = uda_method.predict(X_target)
                    y_pred_proba = uda_method.predict_proba(X_target)
                    
                    # å¦‚æœpredict_probaè¿”å›äºŒç»´æ•°ç»„ï¼Œå–æ­£ç±»æ¦‚ç‡
                    if y_pred_proba is not None and len(y_pred_proba.shape) > 1 and y_pred_proba.shape[1] > 1:
                        y_pred_proba_for_roc = y_pred_proba[:, 1]
                    else:
                        y_pred_proba_for_roc = y_pred_proba
                    
                    prediction_data = {
                        'y_true': y_target.tolist() if hasattr(y_target, 'tolist') else list(y_target),
                        'y_pred_proba': y_pred_proba_for_roc.tolist() if y_pred_proba_for_roc is not None else None
                    }
                except Exception as pred_error:
                    if self.verbose:
                        print(f"  âš  æ— æ³•è·å–é¢„æµ‹æ•°æ®: {pred_error}")
                    prediction_data = {}
                
                # å­˜å‚¨ç»“æœï¼ˆä»UDAProcessorè·å–çš„ç»“æœï¼‰
                final_results = {
                    'method_name': method_name,
                    'accuracy': method_results.get('accuracy', 0),
                    'auc': method_results.get('auc', None),
                    'f1': method_results.get('f1', 0),
                    'precision': method_results.get('precision', 0),
                    'recall': method_results.get('recall', 0),
                    'output_dir': str(method_output_dir),
                    'visualization_results': viz_results,
                    'is_baseline': False  # UDAæ–¹æ³•ä¸æ˜¯åŸºçº¿
                }
                
                # æ·»åŠ é¢„æµ‹æ•°æ®
                final_results.update(prediction_data)
                
                uda_results[method_name] = final_results
                
                if self.verbose:
                    print(f"âœ… {method_name} å®Œæˆ:")
                    print(f"   å‡†ç¡®ç‡: {final_results['accuracy']:.4f}")
                    if final_results['auc'] is not None:
                        print(f"   AUC: {final_results['auc']:.4f}")
                    print(f"   F1: {final_results['f1']:.4f}")
                
            except Exception as e:
                if self.verbose:
                    print(f"âŒ {method_name} å¤±è´¥: {e}")
                    import traceback
                    print(f"  è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
                    traceback.print_exc()
                
                uda_results[method_name] = {
                    'error': str(e),
                    'method_name': method_name,
                    'accuracy': 0,
                    'auc': None,
                    'f1': 0,
                    'precision': 0,
                    'recall': 0
                }
        
        # ä¿å­˜UDAç»“æœ
        self.results['uda_methods'] = uda_results
        
        uda_results_file = self.output_dir / "uda_methods_results.json"
        with open(uda_results_file, 'w', encoding='utf-8') as f:
            json.dump(uda_results, f, indent=2, ensure_ascii=False, default=str)
        
        if self.verbose:
            print(f"ğŸ“ UDAç»“æœå·²ä¿å­˜: {uda_results_file}")
        
        return uda_results
    
    def generate_comparison_visualizations(self) -> Dict:
        """ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–å›¾è¡¨"""
        if self.verbose:
            print(f"\nğŸ“Š ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–å›¾è¡¨")
            print("=" * 50)
        
        # ä½¿ç”¨æ–°çš„å¯è§†åŒ–æ¨¡å—
        try:
            from preprocessing.analysis_visualizer import create_analysis_visualizer
        except ImportError as e:
            if self.verbose:
                print(f"âŒ æ— æ³•å¯¼å…¥å¯è§†åŒ–æ¨¡å—: {e}")
            return {}
        
        visualizer = create_analysis_visualizer(
            output_dir=str(self.output_dir),
            save_plots=True,
            show_plots=self.verbose  # åªåœ¨verboseæ¨¡å¼ä¸‹æ˜¾ç¤ºå›¾è¡¨
        )
        
        # æ”¶é›†é¢„æµ‹æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        cv_predictions = {}
        uda_predictions = {}
        
        # ä»CVç»“æœä¸­æå–é¢„æµ‹æ•°æ®
        if 'source_domain_cv' in self.results:
            for exp_name, result in self.results['source_domain_cv'].items():
                # ä¼˜å…ˆä½¿ç”¨predictionså­—æ®µä¸­çš„åˆå¹¶æ•°æ®
                if 'predictions' in result and result['predictions']:
                    predictions = result['predictions']
                    if 'y_true' in predictions and 'y_pred_proba' in predictions:
                        y_true = predictions['y_true']
                        y_pred_proba = predictions['y_pred_proba']
                        
                        if y_true and y_pred_proba:
                            cv_predictions[exp_name] = {
                                'y_true': y_true,
                                'y_pred_proba': y_pred_proba
                            }
                # å¦‚æœæ²¡æœ‰predictionså­—æ®µï¼Œåˆ™ä»fold_resultsä¸­æå–
                elif 'fold_results' in result:
                    all_y_true = []
                    all_y_pred_proba = []
                    for fold_result in result['fold_results']:
                        if 'y_true' in fold_result and 'y_pred_proba' in fold_result:
                            all_y_true.extend(fold_result['y_true'])
                            all_y_pred_proba.extend(fold_result['y_pred_proba'])
                    
                    if all_y_true and all_y_pred_proba:
                        cv_predictions[exp_name] = {
                            'y_true': all_y_true,
                            'y_pred_proba': all_y_pred_proba
                        }
        
        # ä»UDAç»“æœä¸­æå–é¢„æµ‹æ•°æ®
        if 'uda_methods' in self.results:
            for method_name, result in self.results['uda_methods'].items():
                if 'y_true' in result and 'y_pred_proba' in result:
                    uda_predictions[method_name] = {
                        'y_true': result['y_true'],
                        'y_pred_proba': result['y_pred_proba'],
                        'is_baseline': result.get('is_baseline', False),
                        'baseline_category': result.get('baseline_category', None)
                    }
        
        # ç”Ÿæˆæ‰€æœ‰å¯è§†åŒ–å›¾è¡¨
        viz_results = visualizer.generate_all_visualizations(
            cv_results=self.results['source_domain_cv'],
            uda_results=self.results['uda_methods'],
            cv_predictions=cv_predictions,
            uda_predictions=uda_predictions
        )
        
        if self.verbose:
            for viz_name, viz_path in viz_results.items():
                if viz_path:
                    print(f"âœ… {viz_name} å·²ä¿å­˜: {viz_path}")
                else:
                    print(f"âš  {viz_name} ç”Ÿæˆå¤±è´¥")
        
        # ç”ŸæˆNatureæ ‡å‡†ç»„åˆå›¾åƒ (ä½¿ç”¨åŸç”Ÿmatplotlibæ–¹æ³•)
        try:
            if self.verbose:
                print(f"\nğŸ“Š ç”ŸæˆNatureæ ‡å‡†ç»„åˆå›¾åƒ...")
            
            # ç›´æ¥è°ƒç”¨visualizerçš„ç»„åˆå›¾åƒæ–¹æ³•
            combined_figure_path = visualizer.plot_combined_analysis_figure(
                cv_results=self.results['source_domain_cv'],
                uda_results=self.results['uda_methods'],
                cv_predictions=cv_predictions,
                uda_predictions=uda_predictions
            )
            
            if combined_figure_path:
                viz_results['combined_analysis_figure'] = combined_figure_path
                if self.verbose:
                    print(f"âœ… ç»„åˆåˆ†æå›¾åƒç”Ÿæˆå®Œæˆ: {combined_figure_path}")
            else:
                viz_results['combined_analysis_figure'] = None
                if self.verbose:
                    print(f"âš ï¸ ç»„åˆåˆ†æå›¾åƒç”Ÿæˆå¤±è´¥")
                    
        except Exception as e:
            if self.verbose:
                print(f"âš ï¸ ç»„åˆå›¾åƒç”Ÿæˆå‡ºé”™: {e}")
            viz_results['combined_analysis_figure'] = None
        
        self.results['visualizations'] = viz_results
        return viz_results
    
# åŸæ¥çš„å¯è§†åŒ–æ–¹æ³•å·²ç§»åŠ¨åˆ° preprocessing/analysis_visualizer.py æ¨¡å—ä¸­
    
    def generate_final_report(self) -> str:
        """ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š"""
        if self.verbose:
            print(f"\nğŸ“‹ ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š")
            print("=" * 50)
        
        report_content = []
        report_content.append("# å®Œæ•´åŒ»ç–—æ•°æ®UDAåˆ†ææŠ¥å‘Š\n")
        report_content.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # é…ç½®ä¿¡æ¯
        report_content.append("## åˆ†æé…ç½®\n")
        config = self.results['config']
        report_content.append(f"- ç‰¹å¾é›†: {config['feature_set']}")
        report_content.append(f"- æ ‡å‡†åŒ–æ–¹æ³•: {config['scaler_type']}")
        report_content.append(f"- ä¸å¹³è¡¡å¤„ç†: {config['imbalance_method']}")
        report_content.append(f"- äº¤å‰éªŒè¯æŠ˜æ•°: {config['cv_folds']}")
        report_content.append(f"- éšæœºç§å­: {config['random_state']}\n")
        
        # æºåŸŸCVç»“æœ
        if self.results['source_domain_cv']:
            report_content.append("## æºåŸŸ10æŠ˜äº¤å‰éªŒè¯ç»“æœ\n")
            cv_results = self.results['source_domain_cv']
            
            report_content.append("| æ–¹æ³• | AUC | Accuracy | F1 | Precision | Recall |")
            report_content.append("|------|-----|----------|----|-----------| -------|")
            
            for exp_name, result in cv_results.items():
                if 'summary' in result and result['summary']:
                    # å¤„ç†æ–¹æ³•åç§°æ˜¾ç¤º
                    raw_method_name = exp_name.split('_')[0].upper()
                    if raw_method_name == 'PAPER':
                        method_name = 'Paper_LR'
                    else:
                        method_name = raw_method_name
                    summary = result['summary']
                    
                    auc = summary.get('auc_mean', 0)
                    acc = summary.get('accuracy_mean', 0)
                    f1 = summary.get('f1_mean', 0)
                    prec = summary.get('precision_mean', 0)
                    rec = summary.get('recall_mean', 0)
                    
                    report_content.append(f"| {method_name} | {auc:.4f} | {acc:.4f} | {f1:.4f} | {prec:.4f} | {rec:.4f} |")
            
            report_content.append("")
        
        # UDAæ–¹æ³•ç»“æœ
        if self.results['uda_methods']:
            report_content.append("## UDAæ–¹æ³•å¯¹æ¯”ç»“æœ\n")
            uda_results = self.results['uda_methods']
            successful_methods = {k: v for k, v in uda_results.items() if 'error' not in v}
            
            if successful_methods:
                report_content.append("| æ–¹æ³• | AUC | Accuracy | F1 | Precision | Recall | ç±»å‹ |")
                report_content.append("|------|-----|----------|----|-----------| -------|------|")
                
                # æŒ‰ç±»å‹åˆ†ç»„æ˜¾ç¤ºç»“æœ
                tabpfn_baseline = {}
                traditional_baselines = {}
                ml_baselines = {}
                uda_methods = {}
                
                for method, result in successful_methods.items():
                    if result.get('is_baseline', False):
                        if method == 'TabPFN_NoUDA':
                            tabpfn_baseline[method] = result
                        elif result.get('baseline_category') == 'ml_baseline':
                            ml_baselines[method] = result
                        elif result.get('baseline_category') == 'traditional_baseline':
                            traditional_baselines[method] = result
                        else:
                            # å…¼å®¹æ—§æ ¼å¼ï¼ŒPKUPHã€Mayoã€Paper_LRç­‰
                            traditional_baselines[method] = result
                    else:
                        uda_methods[method] = result
                
                # å…ˆæ˜¾ç¤ºTabPFNåŸºçº¿
                for method, result in tabpfn_baseline.items():
                    auc = result.get('auc', 0) if result.get('auc') is not None else 0
                    acc = result.get('accuracy', 0)
                    f1 = result.get('f1', 0)
                    prec = result.get('precision', 0)
                    rec = result.get('recall', 0)
                    
                    report_content.append(f"| {method} | {auc:.4f} | {acc:.4f} | {f1:.4f} | {prec:.4f} | {rec:.4f} | TabPFNåŸºçº¿ |")
                
                # æ˜¾ç¤ºä¼ ç»ŸåŸºçº¿æ–¹æ³•
                for method, result in traditional_baselines.items():
                    auc = result.get('auc', 0) if result.get('auc') is not None else 0
                    acc = result.get('accuracy', 0)
                    f1 = result.get('f1', 0)
                    prec = result.get('precision', 0)
                    rec = result.get('recall', 0)
                    
                    report_content.append(f"| {method} | {auc:.4f} | {acc:.4f} | {f1:.4f} | {prec:.4f} | {rec:.4f} | ä¼ ç»ŸåŸºçº¿ |")
                
                # æ˜¾ç¤ºæœºå™¨å­¦ä¹ åŸºçº¿æ–¹æ³•
                for method, result in ml_baselines.items():
                    auc = result.get('auc', 0) if result.get('auc') is not None else 0
                    acc = result.get('accuracy', 0)
                    f1 = result.get('f1', 0)
                    prec = result.get('precision', 0)
                    rec = result.get('recall', 0)
                    
                    report_content.append(f"| {method} | {auc:.4f} | {acc:.4f} | {f1:.4f} | {prec:.4f} | {rec:.4f} | æœºå™¨å­¦ä¹ åŸºçº¿ |")
                
                # å†æ˜¾ç¤ºUDAæ–¹æ³•
                for method, result in uda_methods.items():
                    auc = result.get('auc', 0) if result.get('auc') is not None else 0
                    acc = result.get('accuracy', 0)
                    f1 = result.get('f1', 0)
                    prec = result.get('precision', 0)
                    rec = result.get('recall', 0)
                    
                    report_content.append(f"| {method} | {auc:.4f} | {acc:.4f} | {f1:.4f} | {prec:.4f} | {rec:.4f} | UDAæ–¹æ³• |")
                
                report_content.append("")
            
            # å¤±è´¥çš„æ–¹æ³•
            failed_methods = {k: v for k, v in uda_results.items() if 'error' in v}
            if failed_methods:
                report_content.append("### å¤±è´¥çš„æ–¹æ³•\n")
                for method, result in failed_methods.items():
                    method_type = "åŸºçº¿æ–¹æ³•" if result.get('is_baseline', False) else "UDAæ–¹æ³•"
                    report_content.append(f"- {method} ({method_type}): {result['error']}")
                report_content.append("")
        
        # ç»“è®ºå’Œå»ºè®®
        report_content.append("## ç»“è®ºå’Œå»ºè®®\n")
        
        # æ‰¾å‡ºæœ€ä½³æ–¹æ³•
        best_source_method = ""
        best_source_auc = 0
        
        if self.results['source_domain_cv']:
            for exp_name, result in self.results['source_domain_cv'].items():
                if 'summary' in result and result['summary']:
                    auc = result['summary'].get('auc_mean', 0)
                    if auc > best_source_auc:
                        best_source_auc = auc
                        raw_method_name = exp_name.split('_')[0].upper()
                        if raw_method_name == 'PAPER':
                            best_source_method = 'Paper_LR'
                        else:
                            best_source_method = raw_method_name
        
        # æ‰¾å‡ºUDAæ–¹æ³•ä¸­çš„æœ€ä½³æ–¹æ³•å’ŒåŸºçº¿
        best_uda_method = ""
        best_uda_auc = 0
        baseline_auc = 0
        
        if self.results['uda_methods']:
            successful_uda = {k: v for k, v in self.results['uda_methods'].items() if 'error' not in v}
            
            # è·å–TabPFN_NoUDAåŸºçº¿ç»“æœ
            if 'TabPFN_NoUDA' in successful_uda:
                baseline_result = successful_uda['TabPFN_NoUDA']
                baseline_auc = baseline_result.get('auc', 0) if baseline_result.get('auc') is not None else 0
            
            # æ‰¾å‡ºæœ€ä½³UDAæ–¹æ³•ï¼ˆæ’é™¤åŸºçº¿ï¼‰
            for method, result in successful_uda.items():
                if method != 'TabPFN_NoUDA':  # æ’é™¤åŸºçº¿
                    auc = result.get('auc', 0) if result.get('auc') is not None else 0
                    if auc > best_uda_auc:
                        best_uda_auc = auc
                        best_uda_method = method
        
        if best_source_method:
            report_content.append(f"- **æœ€ä½³æºåŸŸæ–¹æ³•**: {best_source_method} (AUC: {best_source_auc:.4f})")
        
        if baseline_auc > 0:
            report_content.append(f"- **TabPFNæ— UDAåŸºçº¿**: TabPFN_NoUDA (AUC: {baseline_auc:.4f})")
        
        if best_uda_method:
            report_content.append(f"- **æœ€ä½³UDAæ–¹æ³•**: {best_uda_method} (AUC: {best_uda_auc:.4f})")
        
        # æ¯”è¾ƒæœ€ä½³UDAæ–¹æ³•ä¸TabPFNæ— UDAåŸºçº¿
        if baseline_auc > 0 and best_uda_auc > 0:
            improvement = best_uda_auc - baseline_auc
            if improvement > 0:
                report_content.append(f"- **åŸŸé€‚åº”æ•ˆæœ**: {best_uda_method}ç›¸æ¯”TabPFNæ— UDAåŸºçº¿æå‡äº† {improvement:.4f} AUC")
            else:
                report_content.append(f"- **åŸŸé€‚åº”æ•ˆæœ**: {best_uda_method}ç›¸æ¯”TabPFNæ— UDAåŸºçº¿ä¸‹é™äº† {abs(improvement):.4f} AUC")
        
        report_content.append(f"\nè¯¦ç»†ç»“æœå’Œå¯è§†åŒ–å›¾è¡¨è¯·æŸ¥çœ‹: {self.output_dir}")
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / "analysis_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_content))
        
        if self.verbose:
            print(f"ğŸ“ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            print("\n" + "="*60)
            print("åˆ†ææŠ¥å‘Šé¢„è§ˆ:")
            print("="*60)
            for line in report_content[:20]:  # æ˜¾ç¤ºå‰20è¡Œ
                print(line)
            if len(report_content) > 20:
                print("...")
            print("="*60)
        
        return str(report_file)
    
    def run_complete_analysis(self) -> Dict:
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        if self.verbose:
            print(f"ğŸš€ å¼€å§‹å®Œæ•´åˆ†ææµç¨‹")
            print("=" * 60)
        
        try:
            # 1. åŠ è½½æ•°æ®ç”¨äºæºåŸŸäº¤å‰éªŒè¯ï¼ˆåŸºçº¿æ¨¡å‹éœ€è¦selected58ç‰¹å¾é›†ï¼‰
            X_source_cv, y_source_cv, X_target_cv, y_target_cv, feature_names_cv = self.load_data_for_cv()
            
            # 2. æºåŸŸ10æŠ˜äº¤å‰éªŒè¯
            self.run_source_domain_cv(X_source_cv, y_source_cv, feature_names_cv)
            
            # 3. åŠ è½½æ•°æ®ç”¨äºUDAåˆ†æï¼ˆä½¿ç”¨æŒ‡å®šç‰¹å¾é›†ï¼ŒåŒ…å«é¢„å¤„ç†ï¼‰
            X_source_uda, y_source_uda, X_target_uda, y_target_uda, feature_names_uda = self.load_data_for_uda()
            
            # 4. UDAæ–¹æ³•å¯¹æ¯”
            self.run_uda_methods(X_source_uda, y_source_uda, X_target_uda, y_target_uda, feature_names_uda)
            
            # 5. ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–
            self.generate_comparison_visualizations()
            
            # 6. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            report_file = self.generate_final_report()
            
            # 7. ä¿å­˜å®Œæ•´ç»“æœ
            complete_results_file = self.output_dir / "complete_results.json"
            with open(complete_results_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False, default=str)
            
            if self.verbose:
                print(f"\nâœ… å®Œæ•´åˆ†ææµç¨‹å®Œæˆï¼")
                print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
                print(f"ğŸ“‹ åˆ†ææŠ¥å‘Š: {report_file}")
                print(f"ğŸ“Š å®Œæ•´ç»“æœ: {complete_results_file}")
            
            return self.results
            
        except Exception as e:
            if self.verbose:
                print(f"âŒ åˆ†ææµç¨‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {'error': str(e)}


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¥ å®Œæ•´åŒ»ç–—æ•°æ®UDAåˆ†ææµç¨‹")
    print("=" * 60)
    
    # åˆ›å»ºåˆ†æè¿è¡Œå™¨
    runner = CompleteAnalysisRunner(
        feature_set='best8',
        scaler_type='none',  # ä¸ä½¿ç”¨æ ‡å‡†åŒ–
        imbalance_method='none',  # ä¸ä½¿ç”¨ä¸å¹³è¡¡å¤„ç†
        cv_folds=10,
        random_state=42,
        verbose=True
    )
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    results = runner.run_complete_analysis()
    
    if 'error' not in results:
        print(f"\nğŸ‰ åˆ†ææˆåŠŸå®Œæˆï¼")
        print(f"ğŸ“ æŸ¥çœ‹ç»“æœç›®å½•: {runner.output_dir}")
    else:
        print(f"\nâŒ åˆ†æå¤±è´¥: {results['error']}")


if __name__ == "__main__":
    main() 