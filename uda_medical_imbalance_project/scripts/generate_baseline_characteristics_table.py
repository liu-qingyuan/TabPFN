#!/usr/bin/env python3
"""
ç”ŸæˆåŸºçº¿ç‰¹å¾è¡¨æ ¼ï¼ˆBaseline Characteristics Tableï¼‰

ç”¨äºç”Ÿæˆä¸´åºŠç ”ç©¶ä¸­å¸¸è§çš„åŸºçº¿ç‰¹å¾è¡¨æ ¼ï¼Œå±•ç¤ºä¸åŒé˜Ÿåˆ—æ ·æœ¬çš„äººå£ç»Ÿè®¡å­¦/ä¸´åºŠç‰¹å¾åˆ†å¸ƒã€‚
å¸¸è§äºã€ŠLancetã€‹ã€ŠJAMAã€‹ã€ŠNEJMã€‹ç­‰ä¸´åºŠç ”ç©¶ä¸­ã€‚

ç‰¹å¾è¡¨è¾¾æ–¹å¼ï¼š
- è¿ç»­å˜é‡ï¼šå‡å€¼ Â± æ ‡å‡†å·®
- ç±»åˆ«å˜é‡ï¼šé¢‘æ•° + ç™¾åˆ†æ¯”
- åˆ†ç»„ï¼šTrain cohort (A) / Test cohort (B)

è¿è¡Œç¤ºä¾‹: python scripts/generate_baseline_characteristics_table.py
"""

import sys
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from data.loader import MedicalDataLoader
from config.settings import CAT_FEATURE_NAMES


class BaselineCharacteristicsGenerator:
    """åŸºçº¿ç‰¹å¾è¡¨æ ¼ç”Ÿæˆå™¨"""
    
    def __init__(
        self,
        feature_type: str = 'best8',
        output_dir: Optional[str] = None,
        verbose: bool = True
    ):
        """
        åˆå§‹åŒ–åŸºçº¿ç‰¹å¾è¡¨æ ¼ç”Ÿæˆå™¨
        
        Args:
            feature_type: ç‰¹å¾é›†ç±»å‹ ('all63', 'selected58', 'best3', 'best4', ..., 'best58')
            output_dir: è¾“å‡ºç›®å½•
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        """
        self.feature_type = feature_type
        self.verbose = verbose
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if output_dir is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = project_root / "results" / f"baseline_characteristics_{timestamp}"
        
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½ç‰¹å¾æ˜ å°„è¡¨
        self.feature_mapping = self._load_feature_mapping()
        
        if self.verbose:
            print(f"ğŸ¥ åŸºçº¿ç‰¹å¾è¡¨æ ¼ç”Ÿæˆå™¨åˆå§‹åŒ–")
            print(f"   ç‰¹å¾é›†: {feature_type}")
            print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    
    def _load_feature_mapping(self) -> Dict[str, str]:
        """åŠ è½½ç‰¹å¾æ˜ å°„è¡¨"""
        try:
            mapping_file = Path("data/Feature_Ranking_with_Original_Names.csv")
            if not mapping_file.exists():
                # å°è¯•ä»é¡¹ç›®æ ¹ç›®å½•æŸ¥æ‰¾
                mapping_file = project_root.parent / "data" / "Feature_Ranking_with_Original_Names.csv"
            
            if mapping_file.exists():
                df = pd.read_csv(mapping_file)
                mapping = dict(zip(df['Feature'], df['Original Feature Name']))
                if self.verbose:
                    print(f"âœ… ç‰¹å¾æ˜ å°„è¡¨åŠ è½½æˆåŠŸ: {len(mapping)} ä¸ªç‰¹å¾")
                return mapping
            else:
                if self.verbose:
                    print("âš  ç‰¹å¾æ˜ å°„è¡¨æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤ç‰¹å¾å")
                return {}
        except Exception as e:
            if self.verbose:
                print(f"âš  ç‰¹å¾æ˜ å°„è¡¨åŠ è½½å¤±è´¥: {e}")
            return {}
    
    def _get_original_feature_name(self, feature: str) -> str:
        """è·å–ç‰¹å¾çš„åŸå§‹åç§°"""
        if feature in self.feature_mapping:
            return self.feature_mapping[feature]
        else:
            # å¦‚æœæ²¡æœ‰æ˜ å°„ï¼Œè¿”å›åŸå§‹ç‰¹å¾å
            return feature
    
    def _is_categorical_feature(self, feature: str) -> bool:
        """åˆ¤æ–­ç‰¹å¾æ˜¯å¦ä¸ºç±»åˆ«ç‰¹å¾"""
        return feature in CAT_FEATURE_NAMES
    
    def _calculate_continuous_stats(self, data: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—è¿ç»­å˜é‡ç»Ÿè®¡é‡"""
        return {
            'mean': np.mean(data),
            'std': np.std(data, ddof=1),  # æ ·æœ¬æ ‡å‡†å·®
            'median': np.median(data),
            'q25': np.percentile(data, 25),
            'q75': np.percentile(data, 75),
            'min': np.min(data),
            'max': np.max(data),
            'n_valid': len(data) - np.isnan(data).sum()
        }
    
    def _calculate_categorical_stats(self, data: np.ndarray) -> Dict[str, Any]:
        """è®¡ç®—ç±»åˆ«å˜é‡ç»Ÿè®¡é‡"""
        # ç§»é™¤ç¼ºå¤±å€¼
        valid_data = data[~np.isnan(data)]
        unique_values, counts = np.unique(valid_data, return_counts=True)
        
        total_valid = len(valid_data)
        
        # è®¡ç®—é¢‘æ•°å’Œç™¾åˆ†æ¯”
        freq_dict = {}
        for value, count in zip(unique_values, counts):
            freq_dict[f'category_{int(value)}'] = {
                'count': count,
                'percentage': (count / total_valid * 100) if total_valid > 0 else 0
            }
        
        return {
            'categories': freq_dict,
            'n_valid': total_valid,
            'n_missing': len(data) - total_valid,
            'n_categories': len(unique_values)
        }
    
    def _format_continuous_display(self, stats: Dict[str, float]) -> str:
        """æ ¼å¼åŒ–è¿ç»­å˜é‡çš„æ˜¾ç¤º"""
        return f"{stats['mean']:.2f} Â± {stats['std']:.2f}"
    
    def _format_categorical_display(self, stats: Dict[str, Any]) -> List[str]:
        """æ ¼å¼åŒ–ç±»åˆ«å˜é‡çš„æ˜¾ç¤º"""
        display_lines = []
        
        # è·å–æ‰€æœ‰ç±»åˆ«å€¼å¹¶æ’åº
        categories = sorted(stats['categories'].items(), key=lambda x: int(x[0].replace('category_', '')))
        
        for category, info in categories:
            count = info['count']
            percentage = info['percentage']
            category_value = int(category.replace('category_', ''))
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºäºŒåˆ†ç±»ç‰¹å¾
            total_categories = len(stats['categories'])
            
            if total_categories == 2:
                # äºŒåˆ†ç±»ç‰¹å¾ï¼šä½¿ç”¨ No/Negative å’Œ Yes/Positive
                if category_value == 0:
                    category_display = "No/Negative"
                elif category_value == 1:
                    category_display = "Yes/Positive"
                else:
                    category_display = f"Category {category_value}"
            else:
                # å¤šåˆ†ç±»ç‰¹å¾ï¼šç›´æ¥ä½¿ç”¨åŸå§‹ç±»åˆ«å€¼
                category_display = f"Category {category_value}"
            
            display_lines.append(f"  {category_display}: {count} ({percentage:.1f}%)")
        
        return display_lines
    
    def load_datasets(self) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """åŠ è½½æ•°æ®é›†Aå’ŒB"""
        if self.verbose:
            print(f"\nğŸ“Š åŠ è½½æ•°æ®é›†...")
        
        loader = MedicalDataLoader()
        
        # åŠ è½½æ•°æ®é›†Aï¼ˆæºåŸŸï¼ŒTrain cohortï¼‰
        data_A = loader.load_dataset('A', feature_type=self.feature_type)
        
        # åŠ è½½æ•°æ®é›†Bï¼ˆç›®æ ‡åŸŸï¼ŒTest cohortï¼‰
        data_B = loader.load_dataset('B', feature_type=self.feature_type)
        
        if self.verbose:
            print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ:")
            print(f"   Train cohort (A): {data_A['n_samples']} æ ·æœ¬")
            print(f"   Test cohort (B): {data_B['n_samples']} æ ·æœ¬")
            print(f"   ç‰¹å¾æ•°é‡: {data_A['n_features']}")
            print(f"   ç‰¹å¾åˆ—è¡¨: {data_A['feature_names']}")
        
        return data_A, data_B
    
    def generate_baseline_table(self) -> pd.DataFrame:
        """ç”ŸæˆåŸºçº¿ç‰¹å¾è¡¨æ ¼"""
        if self.verbose:
            print(f"\nğŸ“‹ ç”ŸæˆåŸºçº¿ç‰¹å¾è¡¨æ ¼...")
        
        # åŠ è½½æ•°æ®
        data_A, data_B = self.load_datasets()
        
        # æå–ç‰¹å¾å’Œæ ‡ç­¾
        X_A = data_A['X']
        y_A = data_A['y']
        X_B = data_B['X']
        y_B = data_B['y']
        feature_names = data_A['feature_names']
        
        # åˆ›å»ºç»“æœåˆ—è¡¨
        table_data = []
        
        # å¤„ç†æ¯ä¸ªç‰¹å¾
        for i, feature in enumerate(feature_names):
            # è·å–åŸå§‹ç‰¹å¾å
            original_name = self._get_original_feature_name(feature)
            
            # æå–ç‰¹å¾æ•°æ®
            feature_data_A = X_A[:, i]
            feature_data_B = X_B[:, i]
            
            # åˆ¤æ–­ç‰¹å¾ç±»å‹
            is_categorical = self._is_categorical_feature(feature)
            
            if is_categorical:
                # ç±»åˆ«ç‰¹å¾
                stats_A = self._calculate_categorical_stats(feature_data_A)
                stats_B = self._calculate_categorical_stats(feature_data_B)
                
                # æ ¼å¼åŒ–æ˜¾ç¤º
                display_A = self._format_categorical_display(stats_A)
                display_B = self._format_categorical_display(stats_B)
                
                # æ·»åŠ ä¸»è¡Œ
                table_data.append({
                    'Feature': original_name,
                    'Feature_Code': feature,
                    'Type': 'Categorical',
                    'Train_Cohort_A': f"n = {stats_A['n_valid']}",
                    'Test_Cohort_B': f"n = {stats_B['n_valid']}",
                    'Is_Subrow': False
                })
                
                # æ·»åŠ å­è¡Œï¼ˆå„ç±»åˆ«ï¼‰
                max_categories = max(len(display_A), len(display_B))
                for j in range(max_categories):
                    sub_A = display_A[j] if j < len(display_A) else ""
                    sub_B = display_B[j] if j < len(display_B) else ""
                    
                    table_data.append({
                        'Feature': "",
                        'Feature_Code': feature,
                        'Type': 'Categorical_Sub',
                        'Train_Cohort_A': sub_A,
                        'Test_Cohort_B': sub_B,
                        'Is_Subrow': True
                    })
            
            else:
                # è¿ç»­ç‰¹å¾
                stats_A = self._calculate_continuous_stats(feature_data_A)
                stats_B = self._calculate_continuous_stats(feature_data_B)
                
                # æ ¼å¼åŒ–æ˜¾ç¤º
                display_A = self._format_continuous_display(stats_A)
                display_B = self._format_continuous_display(stats_B)
                
                table_data.append({
                    'Feature': original_name,
                    'Feature_Code': feature,
                    'Type': 'Continuous',
                    'Train_Cohort_A': display_A,
                    'Test_Cohort_B': display_B,
                    'Is_Subrow': False
                })
        
        # æ·»åŠ æ ·æœ¬å¤§å°ä¿¡æ¯
        table_data.insert(0, {
            'Feature': 'Sample Size',
            'Feature_Code': 'N',
            'Type': 'Summary',
            'Train_Cohort_A': f"n = {len(y_A)}",
            'Test_Cohort_B': f"n = {len(y_B)}",
            'Is_Subrow': False
        })
        
        # æ·»åŠ æ ‡ç­¾åˆ†å¸ƒä¿¡æ¯
        # è®¡ç®—æ ‡ç­¾åˆ†å¸ƒ
        label_stats_A = self._calculate_categorical_stats(y_A)
        label_stats_B = self._calculate_categorical_stats(y_B)
        
        table_data.append({
            'Feature': 'Outcome (Malignant)',
            'Feature_Code': 'Label',
            'Type': 'Categorical',
            'Train_Cohort_A': f"n = {label_stats_A['n_valid']}",
            'Test_Cohort_B': f"n = {label_stats_B['n_valid']}",
            'Is_Subrow': False
        })
        
        # æ·»åŠ æ ‡ç­¾å­è¡Œ
        label_display_A = self._format_categorical_display(label_stats_A)
        label_display_B = self._format_categorical_display(label_stats_B)
        
        max_labels = max(len(label_display_A), len(label_display_B))
        for j in range(max_labels):
            sub_A = label_display_A[j] if j < len(label_display_A) else ""
            sub_B = label_display_B[j] if j < len(label_display_B) else ""
            
            table_data.append({
                'Feature': "",
                'Feature_Code': 'Label',
                'Type': 'Categorical_Sub',
                'Train_Cohort_A': sub_A,
                'Test_Cohort_B': sub_B,
                'Is_Subrow': True
            })
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(table_data)
        
        if self.verbose:
            print(f"âœ… åŸºçº¿ç‰¹å¾è¡¨æ ¼ç”Ÿæˆå®Œæˆ")
            print(f"   è¡¨æ ¼è¡Œæ•°: {len(df)}")
            print(f"   ç‰¹å¾æ•°é‡: {len(feature_names)}")
        
        return df
    
    def save_table(self, df: pd.DataFrame, formats: List[str] = ['csv', 'xlsx']) -> Dict[str, str]:
        """ä¿å­˜åŸºçº¿ç‰¹å¾è¡¨æ ¼"""
        if self.verbose:
            print(f"\nğŸ’¾ ä¿å­˜åŸºçº¿ç‰¹å¾è¡¨æ ¼...")
        
        saved_files = {}
        
        # åˆ›å»ºæ˜¾ç¤ºç”¨çš„DataFrameï¼ˆç§»é™¤è¾…åŠ©åˆ—ï¼‰
        display_df = df[['Feature', 'Train_Cohort_A', 'Test_Cohort_B']].copy()
        display_df.columns = ['Characteristic', 'Train Cohort (A)', 'Test Cohort (B)']
        
        # ä¿å­˜ä¸ºä¸åŒæ ¼å¼
        for format_type in formats:
            if format_type == 'csv':
                file_path = self.output_dir / f"baseline_characteristics_{self.feature_type}.csv"
                display_df.to_csv(file_path, index=False, encoding='utf-8')
                saved_files['csv'] = str(file_path)
            
            elif format_type == 'xlsx':
                file_path = self.output_dir / f"baseline_characteristics_{self.feature_type}.xlsx"
                
                # ä½¿ç”¨ExcelWriterè¿›è¡Œæ ¼å¼åŒ–
                with pd.ExcelWriter(file_path, engine='openpyxl') as writer:
                    display_df.to_excel(writer, sheet_name='Baseline_Characteristics', index=False)
                    
                    # è·å–å·¥ä½œè¡¨
                    worksheet = writer.sheets['Baseline_Characteristics']
                    
                    # è®¾ç½®åˆ—å®½
                    worksheet.column_dimensions['A'].width = 30
                    worksheet.column_dimensions['B'].width = 20
                    worksheet.column_dimensions['C'].width = 20
                    
                    # è®¾ç½®è¡¨å¤´æ ¼å¼
                    from openpyxl.styles import Font, PatternFill, Alignment
                    
                    header_font = Font(bold=True, size=12)
                    header_fill = PatternFill(start_color="CCCCCC", end_color="CCCCCC", fill_type="solid")
                    
                    for cell in worksheet[1]:
                        cell.font = header_font
                        cell.fill = header_fill
                        cell.alignment = Alignment(horizontal="center")
                
                saved_files['xlsx'] = str(file_path)
        
        if self.verbose:
            for format_type, file_path in saved_files.items():
                print(f"âœ… {format_type.upper()} æ–‡ä»¶å·²ä¿å­˜: {file_path}")
        
        return saved_files
    
    def generate_visualization(self, df: pd.DataFrame) -> str:
        """ç”ŸæˆåŸºçº¿ç‰¹å¾å¯è§†åŒ–å›¾è¡¨"""
        if self.verbose:
            print(f"\nğŸ“Š ç”ŸæˆåŸºçº¿ç‰¹å¾å¯è§†åŒ–...")
        
        # ç­›é€‰è¿ç»­ç‰¹å¾è¿›è¡Œå¯è§†åŒ–ï¼ˆæ’é™¤æ ‡ç­¾å’Œæ±‡æ€»è¡Œï¼‰
        continuous_features = df[
            (df['Type'] == 'Continuous') & 
            (~df['Is_Subrow']) &
            (df['Feature_Code'] != 'Label') &
            (df['Feature_Code'] != 'N')
        ].copy()
        
        # ç­›é€‰ç±»åˆ«ç‰¹å¾ï¼ˆæ’é™¤æ ‡ç­¾å’Œæ±‡æ€»è¡Œï¼‰
        categorical_features = df[
            (df['Type'] == 'Categorical') & 
            (~df['Is_Subrow']) &
            (df['Feature_Code'] != 'Label') &
            (df['Feature_Code'] != 'N')
        ].copy()
        
        if len(continuous_features) == 0 and len(categorical_features) == 0:
            if self.verbose:
                print("âš  æ²¡æœ‰ç‰¹å¾å¯ç”¨äºå¯è§†åŒ–")
            return ""
        
        if self.verbose:
            print(f"   è¿ç»­ç‰¹å¾æ•°é‡: {len(continuous_features)}")
            print(f"   ç±»åˆ«ç‰¹å¾æ•°é‡: {len(categorical_features)}")
            print(f"   æ€»è®¡å¯è§†åŒ–ç‰¹å¾: {len(continuous_features) + len(categorical_features)}")
            print(f"   æ³¨æ„: ç±»åˆ«ç‰¹å¾å°†æ˜¾ç¤ºä¸ºæŸ±çŠ¶å›¾ï¼Œè¿ç»­ç‰¹å¾æ˜¾ç¤ºä¸ºç›´æ–¹å›¾")
        
        # é‡æ–°åŠ è½½æ•°æ®ç”¨äºå¯è§†åŒ–
        data_A, data_B = self.load_datasets()
        X_A = data_A['X']
        X_B = data_B['X']
        feature_names = data_A['feature_names']
        
        # åˆå¹¶æ‰€æœ‰è¦å¯è§†åŒ–çš„ç‰¹å¾
        all_features = pd.concat([continuous_features, categorical_features], ignore_index=True)
        
        # åˆ›å»ºå­å›¾
        n_features = len(all_features)
        n_cols = min(3, n_features)
        n_rows = (n_features + n_cols - 1) // n_cols
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))
        if n_features == 1:
            axes = [axes]
        elif n_rows == 1:
            axes = axes.reshape(1, -1)
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # ç»˜åˆ¶æ¯ä¸ªç‰¹å¾çš„åˆ†å¸ƒ
        for idx, (_, row) in enumerate(all_features.iterrows()):
            feature_code = row['Feature_Code']
            feature_name = row['Feature']
            feature_type = row['Type']
            
            # è·å–ç‰¹å¾åœ¨æ•°ç»„ä¸­çš„ç´¢å¼•
            feature_idx = feature_names.index(feature_code)
            
            # æå–ç‰¹å¾æ•°æ®
            data_A_feature = X_A[:, feature_idx]
            data_B_feature = X_B[:, feature_idx]
            
            # ç§»é™¤ç¼ºå¤±å€¼
            data_A_clean = data_A_feature[~np.isnan(data_A_feature)]
            data_B_clean = data_B_feature[~np.isnan(data_B_feature)]
            
            # è®¡ç®—å­å›¾ä½ç½®
            row_idx = idx // n_cols
            col_idx = idx % n_cols
            
            if n_rows == 1:
                ax = axes[col_idx]
            else:
                ax = axes[row_idx, col_idx]
            
            # æ ¹æ®ç‰¹å¾ç±»å‹ç»˜åˆ¶ä¸åŒçš„å›¾è¡¨
            if feature_type == 'Continuous':
                # è¿ç»­ç‰¹å¾ï¼šç›´æ–¹å›¾
                ax.hist(data_A_clean, bins=20, alpha=0.7, label='Train Cohort (A)', color='skyblue', density=True)
                ax.hist(data_B_clean, bins=20, alpha=0.7, label='Test Cohort (B)', color='lightcoral', density=True)
                ax.set_xlabel('Value')
                ax.set_ylabel('Density')
                ax.set_title(f'{feature_name}\n(Continuous)', fontsize=10, fontweight='bold')
                
            elif feature_type == 'Categorical':
                # ç±»åˆ«ç‰¹å¾ï¼šæŸ±çŠ¶å›¾
                # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„é¢‘æ•°
                unique_values = np.unique(np.concatenate([data_A_clean, data_B_clean]))
                
                counts_A = []
                counts_B = []
                for val in unique_values:
                    counts_A.append(np.sum(data_A_clean == val))
                    counts_B.append(np.sum(data_B_clean == val))
                
                # è®¡ç®—ç™¾åˆ†æ¯”
                pct_A = np.array(counts_A) / len(data_A_clean) * 100
                pct_B = np.array(counts_B) / len(data_B_clean) * 100
                
                x = np.arange(len(unique_values))
                width = 0.35
                
                ax.bar(x - width/2, pct_A, width, label='Train Cohort (A)', color='skyblue', alpha=0.7)
                ax.bar(x + width/2, pct_B, width, label='Test Cohort (B)', color='lightcoral', alpha=0.7)
                
                # è®¾ç½®xè½´æ ‡ç­¾
                category_labels = []
                total_categories = len(unique_values)
                
                for val in unique_values:
                    if total_categories == 2:
                        # äºŒåˆ†ç±»ç‰¹å¾
                        if val == 0:
                            category_labels.append('No/Negative')
                        elif val == 1:
                            category_labels.append('Yes/Positive')
                        else:
                            category_labels.append(f'Cat {int(val)}')
                    else:
                        # å¤šåˆ†ç±»ç‰¹å¾ï¼šç›´æ¥ä½¿ç”¨åŸå§‹ç±»åˆ«å€¼
                        category_labels.append(f'Cat {int(val)}')
                
                ax.set_xticks(x)
                ax.set_xticklabels(category_labels)
                ax.set_xlabel('Category')
                ax.set_ylabel('Percentage (%)')
                ax.set_title(f'{feature_name}\n(Categorical)', fontsize=10, fontweight='bold')
            
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(n_features, n_rows * n_cols):
            row_idx = idx // n_cols
            col_idx = idx % n_cols
            if n_rows == 1:
                axes[col_idx].set_visible(False)
            else:
                axes[row_idx, col_idx].set_visible(False)
        
        plt.tight_layout()
        
        # æ·»åŠ æ€»æ ‡é¢˜
        fig.suptitle(f'Baseline Characteristics Distribution ({self.feature_type.upper()} Features)\n'
                    f'Continuous Features: {len(continuous_features)}, Categorical Features: {len(categorical_features)}', 
                    fontsize=14, fontweight='bold', y=0.96)
        
        plt.tight_layout()
        plt.subplots_adjust(top=0.88)  # ä¸ºæ€»æ ‡é¢˜ç•™å‡ºæ›´å¤šç©ºé—´
        
        # ä¿å­˜å›¾è¡¨
        viz_file = self.output_dir / f"baseline_characteristics_all_features_{self.feature_type}.png"
        plt.savefig(viz_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        if self.verbose:
            print(f"âœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {viz_file}")
            print(f"   åŒ…å« {len(continuous_features)} ä¸ªè¿ç»­ç‰¹å¾å’Œ {len(categorical_features)} ä¸ªç±»åˆ«ç‰¹å¾")
        
        return str(viz_file)
    
    def generate_summary_report(self, df: pd.DataFrame, saved_files: Dict[str, str], viz_file: str) -> str:
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        if self.verbose:
            print(f"\nğŸ“‹ ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š...")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_features = len(df[~df['Is_Subrow']])
        categorical_features = len(df[(df['Type'] == 'Categorical') & (~df['Is_Subrow'])])
        continuous_features = len(df[(df['Type'] == 'Continuous') & (~df['Is_Subrow'])])
        
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_content = []
        report_content.append("# åŸºçº¿ç‰¹å¾è¡¨æ ¼ç”ŸæˆæŠ¥å‘Š\n")
        report_content.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # é…ç½®ä¿¡æ¯
        report_content.append("## é…ç½®ä¿¡æ¯\n")
        report_content.append(f"- ç‰¹å¾é›†: {self.feature_type}")
        report_content.append(f"- è¾“å‡ºç›®å½•: {self.output_dir}")
        report_content.append("")
        
        # æ•°æ®é›†ä¿¡æ¯
        report_content.append("## æ•°æ®é›†ä¿¡æ¯\n")
        data_A, data_B = self.load_datasets()
        report_content.append(f"- Train Cohort (A): {data_A['n_samples']} æ ·æœ¬")
        report_content.append(f"- Test Cohort (B): {data_B['n_samples']} æ ·æœ¬")
        report_content.append(f"- æ€»ç‰¹å¾æ•°: {total_features}")
        report_content.append(f"- è¿ç»­ç‰¹å¾æ•°: {continuous_features}")
        report_content.append(f"- ç±»åˆ«ç‰¹å¾æ•°: {categorical_features}")
        report_content.append("")
        
        # è¾“å‡ºæ–‡ä»¶
        report_content.append("## è¾“å‡ºæ–‡ä»¶\n")
        for format_type, file_path in saved_files.items():
            report_content.append(f"- {format_type.upper()} è¡¨æ ¼: {file_path}")
        
        if viz_file:
            report_content.append(f"- å¯è§†åŒ–å›¾è¡¨: {viz_file}")
        
        report_content.append("")
        
        # ä½¿ç”¨è¯´æ˜
        report_content.append("## ä½¿ç”¨è¯´æ˜\n")
        report_content.append("### è¡¨æ ¼æ ¼å¼è¯´æ˜")
        report_content.append("- **è¿ç»­å˜é‡**: æ˜¾ç¤ºä¸º `å‡å€¼ Â± æ ‡å‡†å·®`")
        report_content.append("- **ç±»åˆ«å˜é‡**: æ˜¾ç¤ºä¸º `é¢‘æ•° (ç™¾åˆ†æ¯”%)`")
        report_content.append("- **Train Cohort (A)**: æºåŸŸæ•°æ®é›†ï¼ˆAI4healthï¼‰")
        report_content.append("- **Test Cohort (B)**: ç›®æ ‡åŸŸæ•°æ®é›†ï¼ˆHenanCancerHospitalï¼‰")
        report_content.append("")
        
        report_content.append("### ç‰¹å¾æ˜ å°„")
        report_content.append("ç‰¹å¾ä»£ç åˆ°åŸå§‹åç§°çš„æ˜ å°„åŸºäº `data/Feature_Ranking_with_Original_Names.csv`")
        report_content.append("")
        
        report_content.append("### å¼•ç”¨æ ¼å¼")
        report_content.append("æ­¤è¡¨æ ¼é€‚ç”¨äºä¸´åºŠç ”ç©¶è®ºæ–‡ï¼Œæ ¼å¼å‚è€ƒã€ŠLancetã€‹ã€ŠJAMAã€‹ã€ŠNEJMã€‹ç­‰æœŸåˆŠçš„åŸºçº¿ç‰¹å¾è¡¨æ ¼æ ‡å‡†ã€‚")
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / f"baseline_characteristics_report_{self.feature_type}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_content))
        
        if self.verbose:
            print(f"âœ… æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return str(report_file)
    
    def run_complete_analysis(self) -> Dict[str, str]:
        """è¿è¡Œå®Œæ•´çš„åŸºçº¿ç‰¹å¾åˆ†æ"""
        if self.verbose:
            print(f"ğŸ¥ å¼€å§‹åŸºçº¿ç‰¹å¾è¡¨æ ¼ç”Ÿæˆ")
            print("=" * 60)
        
        try:
            # 1. ç”ŸæˆåŸºçº¿ç‰¹å¾è¡¨æ ¼
            df = self.generate_baseline_table()
            
            # 2. ä¿å­˜è¡¨æ ¼
            saved_files = self.save_table(df, formats=['csv', 'xlsx'])
            
            # 3. ç”Ÿæˆå¯è§†åŒ–
            viz_file = self.generate_visualization(df)
            
            # 4. ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
            report_file = self.generate_summary_report(df, saved_files, viz_file)
            
            # 5. è¿”å›ç»“æœ
            results = {
                'table_csv': saved_files.get('csv', ''),
                'table_xlsx': saved_files.get('xlsx', ''),
                'visualization': viz_file,
                'report': report_file,
                'output_dir': str(self.output_dir)
            }
            
            if self.verbose:
                print(f"\nâœ… åŸºçº¿ç‰¹å¾è¡¨æ ¼ç”Ÿæˆå®Œæˆï¼")
                print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
                print(f"ğŸ“Š CSVè¡¨æ ¼: {results['table_csv']}")
                print(f"ğŸ“Š Excelè¡¨æ ¼: {results['table_xlsx']}")
                print(f"ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨: {results['visualization']}")
                print(f"ğŸ“‹ æ±‡æ€»æŠ¥å‘Š: {results['report']}")
            
            return results
            
        except Exception as e:
            if self.verbose:
                print(f"âŒ åŸºçº¿ç‰¹å¾è¡¨æ ¼ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {'error': str(e)}


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¥ åŸºçº¿ç‰¹å¾è¡¨æ ¼ç”Ÿæˆå™¨")
    print("=" * 60)
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = BaselineCharacteristicsGenerator(
        feature_type='best8',
        verbose=True
    )
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    results = generator.run_complete_analysis()
    
    if 'error' not in results:
        print(f"\nğŸ‰ åŸºçº¿ç‰¹å¾è¡¨æ ¼ç”ŸæˆæˆåŠŸï¼")
        print(f"ğŸ“ æŸ¥çœ‹è¾“å‡ºç›®å½•: {results['output_dir']}")
    else:
        print(f"\nâŒ ç”Ÿæˆå¤±è´¥: {results['error']}")


if __name__ == "__main__":
    main() 