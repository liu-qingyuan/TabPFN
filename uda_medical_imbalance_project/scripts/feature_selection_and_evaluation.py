#!/usr/bin/env python3
"""
ç»Ÿä¸€RFEç‰¹å¾é€‰æ‹©å’Œæ€§èƒ½è¯„ä¼°è„šæœ¬ (ä½¿ç”¨ABäº¤é›†58ä¸ªç‰¹å¾)

è¿™ä¸ªè„šæœ¬ç»“åˆäº†ä»¥ä¸‹åŠŸèƒ½ï¼š
1. predict_healthcare_RFE.py - ä½¿ç”¨TabPFNè¿›è¡ŒRFEç‰¹å¾é€‰æ‹©
2. evaluate_feature_numbers.py - è·¨ä¸åŒç‰¹å¾æ•°é‡çš„æ€§èƒ½è¯„ä¼°

ç‰¹å¾é›†è¯´æ˜ï¼š
- ä½¿ç”¨Aæ•°æ®é›†ï¼ˆAI4healthcare.xlsxï¼‰
- ä»…ä½¿ç”¨ABäº¤é›†çš„58ä¸ªç‰¹å¾ï¼ˆç§»é™¤Feature12, Feature33, Feature34, Feature36, Feature40ï¼‰
- è¯„ä¼°èŒƒå›´ï¼š3-58ä¸ªç‰¹å¾ï¼ˆç”Ÿæˆ56è¡Œç»“æœæ•°æ®ï¼‰

è¿è¡Œç¤ºä¾‹: python scripts/feature_selection_and_evaluation.py

Author: Generated for UDA Medical Imbalance Project
Date: 2024
"""

import sys
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import time
import os
import warnings
from datetime import datetime
from typing import Dict, List, Tuple, Optional

warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥é¡¹ç›®é…ç½®
from config.settings import get_features_by_type, SELECTED_58_FEATURES

from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix
from sklearn.feature_selection import RFE
from sklearn.inspection import permutation_importance
from tabpfn import TabPFNClassifier
from types import SimpleNamespace
from tqdm import tqdm
import torch


class TabPFNWrapper(BaseEstimator, ClassifierMixin):
    """
    Wrapper class to make TabPFN compatible with sklearn's RFE
    """
    _estimator_type = "classifier"
    
    def __sklearn_tags__(self):
        return SimpleNamespace(
            estimator_type="classifier",
            binary_only=True,
            classifier_tags=SimpleNamespace(poor_score=False),
            regressor_tags=SimpleNamespace(poor_score=False),
            input_tags=SimpleNamespace(sparse=False, allow_nan=True),
            target_tags=SimpleNamespace(required=True)
        )

    def __init__(self, device='cuda', n_estimators=32, softmax_temperature=0.9,
                 balance_probabilities=False, average_before_softmax=False,
                 ignore_pretraining_limits=True, random_state=42,
                 n_repeats=5):
        self.device = device
        self.n_estimators = n_estimators
        self.softmax_temperature = softmax_temperature
        self.balance_probabilities = balance_probabilities
        self.average_before_softmax = average_before_softmax
        self.ignore_pretraining_limits = ignore_pretraining_limits
        self.random_state = random_state
        self.n_repeats = n_repeats

    def fit(self, X, y):
        # Set class information for scoring functions
        self.classes_ = np.unique(y)
        
        # Initialize TabPFN model
        self.model_ = TabPFNClassifier(
            device=self.device,
            n_estimators=self.n_estimators,
            softmax_temperature=self.softmax_temperature,
            balance_probabilities=self.balance_probabilities,
            average_before_softmax=self.average_before_softmax,
            ignore_pretraining_limits=self.ignore_pretraining_limits,
            random_state=self.random_state
        )
        self.model_.fit(X, y)
        
        # Calculate feature importance using permutation importance
        result = permutation_importance(
            self, X, y, 
            scoring='roc_auc',
            n_repeats=self.n_repeats,
            random_state=self.random_state
        )
        self.feature_importances_ = result.importances_mean
        self.feature_importances_std_ = result.importances_std
        
        return self

    def predict(self, X):
        return self.model_.predict(X)

    def predict_proba(self, X):
        return self.model_.predict_proba(X)
    
    def score(self, X, y):
        y_proba = self.predict_proba(X)[:, 1]
        return roc_auc_score(y, y_proba)
    
    def get_feature_importance(self):
        """Return feature importance scores"""
        return self.feature_importances_

    def get_feature_importance_scores(self):
        """Return feature importance scores and standard deviations"""
        return {
            'mean': self.feature_importances_,
            'std': self.feature_importances_std_
        }


def select_features_rfe(X, y, n_features=3):
    """
    Use RFE with TabPFN as base estimator for feature selection
    """
    n_features_total = X.shape[1]
    n_iterations = n_features_total - n_features
    
    # Initialize TabPFN wrapper
    base_model = TabPFNWrapper(
        device='cuda',
        n_estimators=32,
        softmax_temperature=0.9,
        balance_probabilities=False,
        average_before_softmax=False,
        ignore_pretraining_limits=True,
        random_state=42
    )
    
    # Initialize RFE
    rfe = RFE(
        estimator=base_model,
        n_features_to_select=n_features,
        step=1,
        verbose=2
    )
    
    # Create progress bar and fit RFE
    print("Fitting RFE with TabPFN as base model...")
    with tqdm(total=n_iterations, desc='Eliminating features') as pbar:
        rfe.fit(X, y)
        pbar.update(n_iterations)
    
    # Get selected features
    selected_features = X.columns[rfe.support_].tolist()
    
    # Get feature importance ranking
    feature_ranking = pd.DataFrame({
        'Feature': X.columns,
        'Rank': rfe.ranking_
    }).sort_values('Rank')
    
    return selected_features, feature_ranking


def evaluate_feature_performance(X, y, feature_ranking, results_dir):
    """
    Evaluate model performance across different numbers of features
    Using the complete RFE ranking from most to least important features
    """
    print("\n" + "="*60)
    print("ğŸ”„ ç¬¬äºŒé˜¶æ®µï¼šè·¨ç‰¹å¾æ•°é‡æ€§èƒ½è¯„ä¼°")
    print("="*60)
    
    # Get ranked features (sorted by RFE rank: 1=most important, 63=least important)
    ranked_features = feature_ranking.sort_values('Rank')['Feature'].tolist()
    
    print(f"ğŸ“‹ ä½¿ç”¨RFEæ’åº: {ranked_features[:3]}...{ranked_features[-3:]}")
    print(f"ğŸ“Š å¯ç”¨ç‰¹å¾æ€»æ•°: {len(ranked_features)}")
    
    # Set random seed for reproducibility
    np.random.seed(42)
    
    # Define feature numbers to test (3 to 58, ABäº¤é›†ç‰¹å¾)
    feature_numbers = list(range(3, len(ranked_features) + 1))
    print(f"ğŸ¯ å°†è¯„ä¼°ç‰¹å¾æ•°: {feature_numbers[0]} åˆ° {feature_numbers[-1]} (å…±{len(feature_numbers)}æ¬¡è¯„ä¼°)")
    print(f"â° é¢„è®¡æ€»ç”¨æ—¶: {len(feature_numbers) * 2:.1f}-{len(feature_numbers) * 4:.1f}åˆ†é’Ÿ (ç›¸æ¯”63ç‰¹å¾çº¦èŠ‚çœ15%æ—¶é—´)")
    print("ğŸ“ ä½¿ç”¨10æŠ˜äº¤å‰éªŒè¯è¯„ä¼°æ¯ä¸ªç‰¹å¾ç»„åˆ...")
    print("ğŸ“Š ç”Ÿæˆç»“æœï¼š3-58ç‰¹å¾æ€§èƒ½å¯¹æ¯”æ•°æ®")
    
    
    # Store all results
    all_results = []
    
    # Evaluate each feature count
    for n_features in tqdm(feature_numbers, desc=f"Evaluating features (3-{len(ranked_features)})"):
        # Select top n features according to RFE ranking (rank 1 = most important)
        selected_features = ranked_features[:n_features]
        X_selected = X[selected_features]
        
        # åªåœ¨å…³é”®é‡Œç¨‹ç¢‘æ‰“å°è¯¦ç»†ä¿¡æ¯ä»¥å‡å°‘è¾“å‡ºå†—ä½™
        if n_features <= 10 or n_features % 10 == 0:
            print(f"\nğŸ” è¯„ä¼° {n_features} ä¸ªç‰¹å¾...")
            print(f"å‰{min(5, len(selected_features))}ä¸ªç‰¹å¾: {selected_features[:5]}")
        
        # 10-fold cross validation
        kf = KFold(n_splits=10, shuffle=True, random_state=42)
        fold_scores = []
        
        for fold, (train_idx, test_idx) in enumerate(kf.split(X_selected), 1):
            X_train, X_test = X_selected.iloc[train_idx], X_selected.iloc[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
            
            # Train model
            start_time = time.time()
            clf = TabPFNClassifier(
                device='cuda',
                n_estimators=32,
                softmax_temperature=0.9,
                balance_probabilities=False,
                average_before_softmax=False,
                ignore_pretraining_limits=True,
                random_state=42
            )
            
            # Ensure reproducible results
            torch.manual_seed(42)
            if torch.cuda.is_available():
                torch.cuda.manual_seed_all(42)
                
            clf.fit(X_train, y_train)
            
            # Make predictions
            y_pred = clf.predict(X_test)
            y_pred_proba = clf.predict_proba(X_test)
            fold_time = time.time() - start_time
            
            # Calculate metrics
            acc = accuracy_score(y_test, y_pred)
            auc = roc_auc_score(y_test, y_pred_proba[:, 1])
            f1 = f1_score(y_test, y_pred)
            
            # Calculate per-class accuracy
            conf_matrix = confusion_matrix(y_test, y_pred)
            acc_0 = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])
            acc_1 = conf_matrix[1, 1] / (conf_matrix[1, 0] + conf_matrix[1, 1])
            
            fold_scores.append({
                'fold': fold,
                'accuracy': acc,
                'auc': auc,
                'f1': f1,
                'acc_0': acc_0,
                'acc_1': acc_1,
                'time': fold_time
            })
        
        # Calculate mean and std
        metrics = ['accuracy', 'auc', 'f1', 'acc_0', 'acc_1', 'time']
        mean_scores = {f'mean_{m}': np.mean([s[m] for s in fold_scores]) for m in metrics}
        std_scores = {f'std_{m}': np.std([s[m] for s in fold_scores]) for m in metrics}
        
        # Store result
        result = {
            'n_features': n_features,
            'features': ', '.join(selected_features),
            **mean_scores,
            **std_scores
        }
        all_results.append(result)
        
        # æ‰“å°å½“å‰ç»“æœ (ç®€åŒ–è¾“å‡º)
        if n_features <= 10 or n_features % 10 == 0:
            print(f"âœ… {n_features}ä¸ªç‰¹å¾ç»“æœ:")
            print(f"   AUC: {mean_scores['mean_auc']:.4f}Â±{std_scores['std_auc']:.4f}")
            print(f"   å‡†ç¡®ç‡: {mean_scores['mean_accuracy']:.4f}Â±{std_scores['std_accuracy']:.4f}")
            print(f"   F1: {mean_scores['mean_f1']:.4f}Â±{std_scores['std_f1']:.4f}")
        else:
            # ä¸­é—´ç»“æœçš„å¿«é€Ÿæ‘˜è¦
            print(f"âš¡ N={n_features}: AUC={mean_scores['mean_auc']:.3f}")
    
    # ä¿å­˜ç»“æœåˆ°CSV
    results_df = pd.DataFrame(all_results)
    csv_path = results_dir / "feature_number_comparison.csv"
    results_df.to_csv(csv_path, index=False)
    
    print(f"\nğŸ“Š æ€§èƒ½è¯„ä¼°ç»“æœå·²ä¿å­˜: {csv_path}")
    
    # åˆ›å»ºå¯è§†åŒ–
    create_performance_visualization(all_results, feature_numbers, results_dir)
    
    return results_df


def create_performance_visualization(all_results, feature_numbers, results_dir):
    """
    Create performance comparison visualization
    """
    plt.figure(figsize=(15, 10))
    metrics = ['auc', 'accuracy', 'f1']
    colors = ['blue', 'green', 'red']
    
    for metric, color in zip(metrics, colors):
        mean_values = [result[f'mean_{metric}'] for result in all_results]
        std_values = [result[f'std_{metric}'] for result in all_results]
        
        plt.plot(feature_numbers, mean_values, marker='o', color=color, label=metric.upper())
        plt.fill_between(
            feature_numbers,
            [m - s for m, s in zip(mean_values, std_values)],
            [m + s for m, s in zip(mean_values, std_values)],
            color=color,
            alpha=0.2
        )
    
    plt.xlabel('Number of Features')
    plt.ylabel('Score')
    plt.title('Model Performance vs Number of Features')
    plt.legend()
    plt.grid(True)
    
    # ä¿å­˜å›¾è¡¨
    plot_path = results_dir / "performance_comparison.png"
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {plot_path}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§¬ ç»Ÿä¸€RFEç‰¹å¾é€‰æ‹©å’Œæ€§èƒ½è¯„ä¼°")
    print("=" * 60)
    
    # åˆ›å»ºæ—¶é—´æˆ³è¾“å‡ºç›®å½• (æ ‡æ³¨ä½¿ç”¨58ä¸ªç‰¹å¾)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = project_root / "results" / f"feature_selection_evaluation_58features_{timestamp}"
    results_dir.mkdir(parents=True, exist_ok=True)
    
    # æ•°æ®è·¯å¾„é…ç½® (åŸºäºloader.pyçš„è·¯å¾„è®¾ç½®)
    data_path = "/home/24052432g/TabPFN/data/AI4healthcare.xlsx"
    
    # åŠ è½½æ•°æ®
    print(f"\nğŸ“‚ åŠ è½½æ•°æ®...")
    print(f"æ•°æ®è·¯å¾„: {data_path}")
    
    try:
        df = pd.read_excel(data_path)
        
        # ä½¿ç”¨ABäº¤é›†çš„58ä¸ªç‰¹å¾ (ç§»é™¤Feature12, Feature33, Feature34, Feature36, Feature40)
        required_features = get_features_by_type('selected58')
        
        # éªŒè¯Aæ•°æ®é›†æ˜¯å¦åŒ…å«æ‰€éœ€çš„58ä¸ªç‰¹å¾
        available_features = [f for f in required_features if f in df.columns]
        missing_features = [f for f in required_features if f not in df.columns]
        
        if missing_features:
            print(f"âš ï¸ è­¦å‘Šï¼šAæ•°æ®é›†ä¸­ç¼ºå¤±ä»¥ä¸‹ç‰¹å¾: {missing_features}")
            print(f"å°†ä½¿ç”¨å¯ç”¨çš„{len(available_features)}ä¸ªç‰¹å¾è¿›è¡Œåˆ†æ")
        else:
            print(f"âœ… Aæ•°æ®é›†åŒ…å«æ‰€æœ‰58ä¸ªABäº¤é›†ç‰¹å¾")
            
        X = df[available_features].copy()
        y = df["Label"].copy()
        
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ")
        print(f"   æ ·æœ¬æ•°: {X.shape[0]}")
        print(f"   ç‰¹å¾æ•°: {X.shape[1]} (ABäº¤é›†ç‰¹å¾)")
        print(f"   ç‰¹å¾èŒƒå›´: {available_features[0]} åˆ° {available_features[-1]}")
        print(f"   æ ‡ç­¾åˆ†å¸ƒ: {y.value_counts().to_dict()}")
        print(f"   ç§»é™¤çš„ç‰¹å¾: Feature12, Feature33, Feature34, Feature36, Feature40")
        
    except FileNotFoundError:
        print(f"âŒ æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {data_path}")
        print("è¯·ç¡®ä¿æ•°æ®æ–‡ä»¶è·¯å¾„æ­£ç¡®")
        return None, None
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None, None
    
    # Phase 1: RFEç‰¹å¾é€‰æ‹©
    print("\n" + "="*60)
    print("ğŸ”¬ ç¬¬ä¸€é˜¶æ®µï¼šåŸºäºTabPFNçš„RFEç‰¹å¾é€‰æ‹©")
    print("="*60)
    
    print("ğŸ§  ä½¿ç”¨TabPFNæ‰§è¡Œé€’å½’ç‰¹å¾æ¶ˆé™¤(RFE)...")
    print("ğŸ“‹ è¿™å°†ç”ŸæˆABäº¤é›†58ä¸ªç‰¹å¾çš„å®Œæ•´é‡è¦æ€§æ’åº")
    print("â° é¢„è®¡ç”¨æ—¶ï¼š4-8åˆ†é’Ÿ (å–å†³äºGPUæ€§èƒ½ï¼Œç›¸æ¯”63ç‰¹å¾ç•¥å¿«)")
    
    try:
        # æ‰§è¡ŒRFEç‰¹å¾é€‰æ‹©ï¼Œé€‰æ‹©3ä¸ªæœ€ä¼˜ç‰¹å¾ä½†è·å¾—å®Œæ•´æ’åº
        selected_features, feature_ranking = select_features_rfe(X, y, n_features=3)
        
        # ä¿å­˜ç‰¹å¾æ’åºç»“æœ (58ä¸ªç‰¹å¾)
        ranking_path = results_dir / "RFE_feature_ranking_58features.csv"
        feature_ranking.to_csv(ranking_path, index=False)
        
        print(f"âœ… RFEç‰¹å¾é€‰æ‹©å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ RFEç‰¹å¾é€‰æ‹©å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None
    
    print(f"ğŸ“ å®Œæ•´ç‰¹å¾æ’åºå·²ä¿å­˜: {ranking_path}")
    print(f"ğŸ“Š RFEå¤„ç†: ä»ABäº¤é›†{X.shape[1]}ä¸ªç‰¹å¾å¼€å§‹ï¼Œé€æ­¥æ¶ˆé™¤åˆ°3ä¸ªç‰¹å¾")
    print(f"ğŸ“‹ æ’åºè¯´æ˜: Rank 1 = æœ€é‡è¦ (æœ€åä¿ç•™), Rank {X.shape[1]} = æœ€ä¸é‡è¦ (æœ€å…ˆæ¶ˆé™¤)")
    print(f"ğŸ—‘ï¸ å·²æ’é™¤çš„ç‰¹å¾: Feature12, Feature33, Feature34, Feature36, Feature40")
    
    print("\nğŸ† Top 10 æœ€é‡è¦ç‰¹å¾ (Rank 1-10):")
    print(feature_ranking.head(10).to_string(index=False))
    print("\nğŸ—‘ï¸ Bottom 10 æœ€ä¸é‡è¦ç‰¹å¾:")
    print(feature_ranking.tail(10).to_string(index=False))
    
    # éªŒè¯RFEé€»è¾‘: é€‰ä¸­çš„3ä¸ªç‰¹å¾åº”è¯¥rankä¸º1,2,3
    print(f"\nâœ… éªŒè¯ - é€‰ä¸­çš„3ä¸ªç‰¹å¾åŠå…¶æ’åº:")
    selected_feature_ranks = feature_ranking[feature_ranking['Feature'].isin(selected_features)].sort_values('Rank')
    print(selected_feature_ranks.to_string(index=False))
    
    # Phase 2: æ€§èƒ½è¯„ä¼°
    print(f"\nğŸ”„ ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨RFEæ’åºè¿›è¡Œ3-58ç‰¹å¾æ€§èƒ½è¯„ä¼°...")
    try:
        results_df = evaluate_feature_performance(X, y, feature_ranking, results_dir)
    except Exception as e:
        print(f"âŒ æ€§èƒ½è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ‰ æ‰§è¡Œå®Œæˆ")
    print("="*60)
    
    print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"1. ç‰¹å¾æ’åº(58ç‰¹å¾): {ranking_path}")
    print(f"2. æ€§èƒ½ç»“æœ(3-58ç‰¹å¾): {results_dir / 'feature_number_comparison.csv'}")
    print(f"3. æ€§èƒ½å›¾è¡¨: {results_dir / 'performance_comparison.png'}")
    print(f"4. ç»“æœæ•°æ®è¡Œæ•°: {len(results_df)} è¡Œ (ä»3ä¸ªç‰¹å¾åˆ°58ä¸ªç‰¹å¾)")
    
    # æ‰¾åˆ°æœ€ä½³æ€§èƒ½çš„ç‰¹å¾æ•°é‡
    best_auc_idx = results_df['mean_auc'].idxmax()
    best_result = results_df.iloc[best_auc_idx]
    
    print(f"\nğŸ† æœ€ä½³æ€§èƒ½æ‘˜è¦:")
    print(f"æœ€ä½³ç‰¹å¾æ•°é‡: {best_result['n_features']}")
    print(f"æœ€ä½³AUC: {best_result['mean_auc']:.4f} Â± {best_result['std_auc']:.4f}")
    print(f"æœ€ä½³å‡†ç¡®ç‡: {best_result['mean_accuracy']:.4f} Â± {best_result['std_accuracy']:.4f}")
    print(f"æœ€ä½³F1åˆ†æ•°: {best_result['mean_f1']:.4f} Â± {best_result['std_f1']:.4f}")
    
    print(f"\nğŸ“‚ ç»“æœç›®å½•: {results_dir}")
    
    return results_df, feature_ranking


if __name__ == "__main__":
    results_df, feature_ranking = main()