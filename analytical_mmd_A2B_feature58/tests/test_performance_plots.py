#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ€§èƒ½å¯¹æ¯”å›¾åŠŸèƒ½æµ‹è¯•è„šæœ¬

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ–°çš„æ€§èƒ½å¯¹æ¯”å›¾æ¨¡å—ç”Ÿæˆå„ç§ç±»å‹çš„æ€§èƒ½å¯è§†åŒ–å›¾è¡¨
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def create_sample_results():
    """åˆ›å»ºç¤ºä¾‹å®éªŒç»“æœæ•°æ®"""
    
    # æ¨¡æ‹Ÿè·¨åŸŸå®éªŒç»“æœ
    cross_domain_results = {
        'Dataset_B_MMD_Linear': {
            'without_domain_adaptation': {
                'accuracy': 0.75, 'auc': 0.78, 'f1': 0.72,
                'acc_0': 0.77, 'acc_1': 0.73
            },
            'with_domain_adaptation': {
                'accuracy': 0.82, 'auc': 0.85, 'f1': 0.79,
                'acc_0': 0.84, 'acc_1': 0.80
            },
            'improvement': {
                'auc_improvement': 0.07,
                'accuracy_improvement': 0.07
            }
        },
        'Dataset_B_MMD_KPCA': {
            'without_domain_adaptation': {
                'accuracy': 0.75, 'auc': 0.78, 'f1': 0.72,
                'acc_0': 0.77, 'acc_1': 0.73
            },
            'with_domain_adaptation': {
                'accuracy': 0.80, 'auc': 0.83, 'f1': 0.77,
                'acc_0': 0.82, 'acc_1': 0.78
            },
            'improvement': {
                'auc_improvement': 0.05,
                'accuracy_improvement': 0.05
            }
        },
        'Dataset_C_MMD_Linear': {
            'without_domain_adaptation': {
                'accuracy': 0.70, 'auc': 0.73, 'f1': 0.68,
                'acc_0': 0.72, 'acc_1': 0.68
            },
            'with_domain_adaptation': {
                'accuracy': 0.78, 'auc': 0.81, 'f1': 0.75,
                'acc_0': 0.80, 'acc_1': 0.76
            },
            'improvement': {
                'auc_improvement': 0.08,
                'accuracy_improvement': 0.08
            }
        }
    }
    
    # æ¨¡æ‹ŸCVç»“æœæ ¼å¼
    cv_results = {
        'AutoTabPFN': {
            'accuracy': '0.89 Â± 0.03',
            'auc': '0.92 Â± 0.02',
            'f1': '0.87 Â± 0.04',
            'acc_0': '0.91 Â± 0.02',
            'acc_1': '0.87 Â± 0.03'
        },
        'TunedTabPFN': {
            'accuracy': '0.87 Â± 0.02',
            'auc': '0.90 Â± 0.03',
            'f1': '0.85 Â± 0.03',
            'acc_0': '0.89 Â± 0.02',
            'acc_1': '0.85 Â± 0.04'
        },
        'BaseTabPFN': {
            'accuracy': '0.85 Â± 0.04',
            'auc': '0.88 Â± 0.03',
            'f1': '0.82 Â± 0.05',
            'acc_0': '0.87 Â± 0.03',
            'acc_1': '0.83 Â± 0.04'
        }
    }
    
    # æ¨¡æ‹Ÿå¤šæ¨¡å‹ç»“æœ
    model_comparison_results = {
        'AutoTabPFN': {
            'accuracy': 0.89, 'auc': 0.92, 'f1': 0.87,
            'acc_0': 0.91, 'acc_1': 0.87
        },
        'TunedTabPFN': {
            'accuracy': 0.87, 'auc': 0.90, 'f1': 0.85,
            'acc_0': 0.89, 'acc_1': 0.85
        },
        'BaseTabPFN': {
            'accuracy': 0.85, 'auc': 0.88, 'f1': 0.82,
            'acc_0': 0.87, 'acc_1': 0.83
        },
        'RF_TabPFN': {
            'accuracy': 0.83, 'auc': 0.86, 'f1': 0.80,
            'acc_0': 0.85, 'acc_1': 0.81
        }
    }
    
    return cross_domain_results, cv_results, model_comparison_results

def test_individual_plots():
    """æµ‹è¯•å•ç‹¬çš„æ€§èƒ½å¯¹æ¯”å›¾åŠŸèƒ½"""
    print("æµ‹è¯•å•ç‹¬çš„æ€§èƒ½å¯¹æ¯”å›¾åŠŸèƒ½...")
    
    try:
        from visualization.performance_plots import (
            plot_metrics_comparison, plot_domain_adaptation_improvement,
            plot_cross_dataset_performance, plot_model_comparison,
            plot_metrics_radar_chart, create_performance_summary_table
        )
        
        cross_domain_results, cv_results, model_comparison_results = create_sample_results()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = project_root / "tests" / "test_results" / "performance_plots_test"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. æµ‹è¯•åŸºç¡€æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”å›¾
        print("  ç”ŸæˆåŸºç¡€æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”å›¾...")
        plot_metrics_comparison(
            results_dict=cv_results,
            save_path=str(output_dir / "metrics_comparison_test.png"),
            title="Model Performance Comparison Test"
        )
        
        # 2. æµ‹è¯•åŸŸé€‚åº”æ”¹è¿›æ•ˆæœå›¾
        print("  ç”ŸæˆåŸŸé€‚åº”æ”¹è¿›æ•ˆæœå›¾...")
        plot_domain_adaptation_improvement(
            results_dict=cross_domain_results,
            save_path=str(output_dir / "domain_adaptation_improvement_test.png"),
            title="Domain Adaptation Improvement Test"
        )
        
        # 3. æµ‹è¯•è·¨æ•°æ®é›†æ€§èƒ½å¯¹æ¯”å›¾
        print("  ç”Ÿæˆè·¨æ•°æ®é›†æ€§èƒ½å¯¹æ¯”å›¾...")
        cross_dataset_data = {
            'Dataset_B': cross_domain_results['Dataset_B_MMD_Linear'],
            'Dataset_C': cross_domain_results['Dataset_C_MMD_Linear']
        }
        plot_cross_dataset_performance(
            results_dict=cross_dataset_data,
            save_path=str(output_dir / "cross_dataset_performance_test.png"),
            title="Cross-Dataset Performance Test"
        )
        
        # 4. æµ‹è¯•æ¨¡å‹æ€§èƒ½å¯¹æ¯”çƒ­åŠ›å›¾
        print("  ç”Ÿæˆæ¨¡å‹æ€§èƒ½å¯¹æ¯”çƒ­åŠ›å›¾...")
        plot_model_comparison(
            results_dict=model_comparison_results,
            save_path=str(output_dir / "model_comparison_heatmap_test.png"),
            title="Model Performance Heatmap Test"
        )
        
        # 5. æµ‹è¯•æ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾
        print("  ç”Ÿæˆæ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾...")
        plot_metrics_radar_chart(
            results_dict=model_comparison_results,
            save_path=str(output_dir / "performance_radar_chart_test.png"),
            title="Performance Radar Chart Test"
        )
        
        # 6. æµ‹è¯•æ€§èƒ½æ±‡æ€»è¡¨æ ¼
        print("  ç”Ÿæˆæ€§èƒ½æ±‡æ€»è¡¨æ ¼...")
        summary_df = create_performance_summary_table(
            results_dict=cross_domain_results,
            save_path=str(output_dir / "performance_summary_table_test.png"),
            title="Performance Summary Table Test"
        )
        
        print(f"  âœ“ å•ç‹¬æµ‹è¯•å®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {output_dir}")
        if summary_df is not None:
            print(f"  âœ“ æ±‡æ€»è¡¨æ ¼å½¢çŠ¶: {summary_df.shape}")
        
        return True
        
    except Exception as e:
        print(f"  âœ— å•ç‹¬æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_integrated_plots():
    """æµ‹è¯•é›†æˆçš„æ€§èƒ½å¯¹æ¯”å›¾åŠŸèƒ½"""
    print("æµ‹è¯•é›†æˆçš„æ€§èƒ½å¯¹æ¯”å›¾åŠŸèƒ½...")
    
    try:
        from visualization.comparison_plots import generate_performance_comparison_plots
        
        cross_domain_results, _, _ = create_sample_results()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = project_root / "tests" / "test_results" / "integrated_performance_test"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # æµ‹è¯•é›†æˆåŠŸèƒ½
        print("  ç”Ÿæˆå®Œæ•´æ€§èƒ½å¯¹æ¯”å›¾è¡¨å¥—ä»¶...")
        summary_df = generate_performance_comparison_plots(
            results_dict=cross_domain_results,
            save_dir=str(output_dir),
            experiment_name="MMD_Domain_Adaptation_Test"
        )
        
        # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
        expected_files = [
            "MMD_Domain_Adaptation_Test_metrics_comparison.png",
            "MMD_Domain_Adaptation_Test_domain_adaptation_improvement.png",
            "MMD_Domain_Adaptation_Test_cross_dataset_performance.png",
            "MMD_Domain_Adaptation_Test_model_comparison.png",
            "MMD_Domain_Adaptation_Test_performance_radar.png",
            "MMD_Domain_Adaptation_Test_performance_summary.png",
            "MMD_Domain_Adaptation_Test_performance_summary.csv"
        ]
        
        generated_files = []
        for file_name in expected_files:
            file_path = output_dir / file_name
            if file_path.exists():
                generated_files.append(file_name)
        
        print(f"  âœ“ é›†æˆæµ‹è¯•å®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {output_dir}")
        print(f"  âœ“ ç”Ÿæˆæ–‡ä»¶æ•°é‡: {len(generated_files)}/{len(expected_files)}")
        if summary_df is not None:
            print(f"  âœ“ æ±‡æ€»è¡¨æ ¼å½¢çŠ¶: {summary_df.shape}")
        
        # æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨
        print("  ç”Ÿæˆçš„æ–‡ä»¶:")
        for file_name in generated_files:
            print(f"    - {file_name}")
        
        return len(generated_files) >= len(expected_files) // 2  # è‡³å°‘ç”Ÿæˆä¸€åŠçš„æ–‡ä»¶å°±ç®—æˆåŠŸ
        
    except Exception as e:
        print(f"  âœ— é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_data_format_compatibility():
    """æµ‹è¯•æ•°æ®æ ¼å¼å…¼å®¹æ€§"""
    print("æµ‹è¯•æ•°æ®æ ¼å¼å…¼å®¹æ€§...")
    
    try:
        from visualization.performance_plots import plot_metrics_comparison
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = project_root / "tests" / "test_results" / "format_compatibility_test"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # æµ‹è¯•ä¸åŒæ•°æ®æ ¼å¼
        formats_to_test = [
            # æ ¼å¼1: è·¨åŸŸå®éªŒæ ¼å¼
            {
                'Cross_Domain_Format': {
                    'without_domain_adaptation': {
                        'accuracy': 0.85, 'auc': 0.88, 'f1': 0.82,
                        'acc_0': 0.87, 'acc_1': 0.83
                    },
                    'with_domain_adaptation': {
                        'accuracy': 0.89, 'auc': 0.92, 'f1': 0.87,
                        'acc_0': 0.91, 'acc_1': 0.87
                    }
                }
            },
            # æ ¼å¼2: CVå­—ç¬¦ä¸²æ ¼å¼
            {
                'CV_String_Format': {
                    'accuracy': '0.85 Â± 0.03',
                    'auc': '0.88 Â± 0.02',
                    'f1': '0.82 Â± 0.04',
                    'acc_0': '0.87 Â± 0.02',
                    'acc_1': '0.83 Â± 0.03'
                }
            },
            # æ ¼å¼3: ç›´æ¥æ•°å€¼æ ¼å¼
            {
                'Direct_Values_Format': {
                    'accuracy': 0.85, 'auc': 0.88, 'f1': 0.82,
                    'acc_0': 0.87, 'acc_1': 0.83
                }
            }
        ]
        
        success_count = 0
        for i, test_format in enumerate(formats_to_test):
            try:
                format_name = list(test_format.keys())[0]
                print(f"  æµ‹è¯•æ ¼å¼ {i+1}: {format_name}")
                
                plot_metrics_comparison(
                    results_dict=test_format,
                    save_path=str(output_dir / f"format_test_{i+1}_{format_name}.png"),
                    title=f"Format Test {i+1}: {format_name}"
                )
                
                success_count += 1
                print(f"    âœ“ æ ¼å¼ {i+1} æµ‹è¯•æˆåŠŸ")
                
            except Exception as e:
                print(f"    âœ— æ ¼å¼ {i+1} æµ‹è¯•å¤±è´¥: {e}")
        
        print(f"  âœ“ æ ¼å¼å…¼å®¹æ€§æµ‹è¯•å®Œæˆ: {success_count}/{len(formats_to_test)} æˆåŠŸ")
        return success_count >= len(formats_to_test) // 2  # è‡³å°‘ä¸€åŠæˆåŠŸå°±ç®—é€šè¿‡
        
    except Exception as e:
        print(f"  âœ— æ ¼å¼å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 60)
    print("æ€§èƒ½å¯¹æ¯”å›¾åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        ("å•ç‹¬åŠŸèƒ½æµ‹è¯•", test_individual_plots),
        ("é›†æˆåŠŸèƒ½æµ‹è¯•", test_integrated_plots),
        ("æ•°æ®æ ¼å¼å…¼å®¹æ€§æµ‹è¯•", test_data_format_compatibility)
    ]
    
    results = []
    for test_name, test_func in tests:
        print(f"\nğŸ§ª {test_name}")
        print("-" * 40)
        success = test_func()
        results.append((test_name, success))
    
    # æ˜¾ç¤ºæµ‹è¯•æ€»ç»“
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    
    success_count = 0
    for test_name, success in results:
        status = "âœ“ é€šè¿‡" if success else "âœ— å¤±è´¥"
        print(f"{test_name}: {status}")
        if success:
            success_count += 1
    
    print(f"\næ€»ä½“ç»“æœ: {success_count}/{len(results)} æµ‹è¯•é€šè¿‡")
    
    if success_count == len(results):
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ€§èƒ½å¯¹æ¯”å›¾åŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
    
    return success_count == len(results)

if __name__ == "__main__":
    main() 