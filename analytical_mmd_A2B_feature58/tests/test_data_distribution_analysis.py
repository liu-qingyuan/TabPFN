#!/usr/bin/env python3
"""
æ•°æ®åˆ†å¸ƒåˆ†ææµ‹è¯•è„šæœ¬

ç”¨äºè¯Šæ–­è·¨åŸŸå®éªŒä¸­æµ‹è¯•é›†AUCä½çš„é—®é¢˜ï¼Œåˆ†æå¯èƒ½çš„åŸå› ï¼š
1. æ•°æ®åˆ†å¸ƒå·®å¼‚
2. ç‰¹å¾æ ‡å‡†åŒ–é—®é¢˜
3. ç±»åˆ«ä¸å¹³è¡¡
4. ç‰¹å¾é€‰æ‹©é—®é¢˜
"""

import sys
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

from config.settings import DATA_PATHS, LABEL_COL, get_features_by_type, get_categorical_indices

def load_datasets():
    """åŠ è½½æ•°æ®é›†Aå’ŒB"""
    print("åŠ è½½æ•°æ®é›†...")
    
    # åŠ è½½æ•°æ®é›†A (AI4Health)
    df_A = pd.read_excel(DATA_PATHS['A'])
    print(f"æ•°æ®é›†Aå½¢çŠ¶: {df_A.shape}")
    
    # åŠ è½½æ•°æ®é›†B (Henan)
    df_B = pd.read_excel(DATA_PATHS['B'])
    print(f"æ•°æ®é›†Bå½¢çŠ¶: {df_B.shape}")
    
    return df_A, df_B

def analyze_basic_statistics(df_A, df_B, features):
    """åˆ†æåŸºç¡€ç»Ÿè®¡ä¿¡æ¯"""
    print("\n" + "="*60)
    print("åŸºç¡€ç»Ÿè®¡ä¿¡æ¯åˆ†æ")
    print("="*60)
    
    # æå–ç‰¹å¾å’Œæ ‡ç­¾
    X_A = df_A[features]
    y_A = df_A[LABEL_COL]
    X_B = df_B[features]
    y_B = df_B[LABEL_COL]
    
    print(f"\næ•°æ®é›†A:")
    print(f"  æ ·æœ¬æ•°: {len(X_A)}")
    print(f"  ç‰¹å¾æ•°: {len(features)}")
    print(f"  æ ‡ç­¾åˆ†å¸ƒ: {dict(y_A.value_counts())}")
    print(f"  æ­£æ ·æœ¬æ¯”ä¾‹: {y_A.mean():.3f}")
    
    print(f"\næ•°æ®é›†B:")
    print(f"  æ ·æœ¬æ•°: {len(X_B)}")
    print(f"  ç‰¹å¾æ•°: {len(features)}")
    print(f"  æ ‡ç­¾åˆ†å¸ƒ: {dict(y_B.value_counts())}")
    print(f"  æ­£æ ·æœ¬æ¯”ä¾‹: {y_B.mean():.3f}")
    
    # æ£€æŸ¥ç¼ºå¤±å€¼
    print(f"\nç¼ºå¤±å€¼åˆ†æ:")
    print(f"  æ•°æ®é›†Aç¼ºå¤±å€¼: {X_A.isnull().sum().sum()}")
    print(f"  æ•°æ®é›†Bç¼ºå¤±å€¼: {X_B.isnull().sum().sum()}")
    
    return X_A, y_A, X_B, y_B

def analyze_feature_distributions(X_A, X_B, features, categorical_indices):
    """åˆ†æç‰¹å¾åˆ†å¸ƒå·®å¼‚"""
    print("\n" + "="*60)
    print("ç‰¹å¾åˆ†å¸ƒå·®å¼‚åˆ†æ")
    print("="*60)
    
    # åˆ†ç¦»æ•°å€¼ç‰¹å¾å’Œç±»åˆ«ç‰¹å¾
    numerical_features = [f for i, f in enumerate(features) if i not in categorical_indices]
    categorical_features = [f for i, f in enumerate(features) if i in categorical_indices]
    
    print(f"\næ•°å€¼ç‰¹å¾æ•°é‡: {len(numerical_features)}")
    print(f"ç±»åˆ«ç‰¹å¾æ•°é‡: {len(categorical_features)}")
    
    # åˆ†ææ•°å€¼ç‰¹å¾
    if numerical_features:
        print(f"\næ•°å€¼ç‰¹å¾åˆ†å¸ƒå·®å¼‚:")
        for feature in numerical_features:
            if feature in X_A.columns and feature in X_B.columns:
                # è®¡ç®—ç»Ÿè®¡é‡
                mean_A = X_A[feature].mean()
                std_A = X_A[feature].std()
                mean_B = X_B[feature].mean()
                std_B = X_B[feature].std()
                
                # è®¡ç®—å·®å¼‚
                mean_diff = abs(mean_B - mean_A) / (std_A + 1e-8)
                std_ratio = std_B / (std_A + 1e-8)
                
                # KSæ£€éªŒ
                ks_stat, ks_pval = stats.ks_2samp(X_A[feature].dropna(), X_B[feature].dropna())
                
                print(f"  {feature}:")
                print(f"    å‡å€¼å·®å¼‚(æ ‡å‡†åŒ–): {mean_diff:.3f}")
                print(f"    æ ‡å‡†å·®æ¯”ä¾‹: {std_ratio:.3f}")
                print(f"    KSç»Ÿè®¡é‡: {ks_stat:.3f} (p={ks_pval:.3e})")
                
                if ks_pval < 0.001:
                    print(f"    âš ï¸  åˆ†å¸ƒæ˜¾è‘—ä¸åŒ!")
    
    # åˆ†æç±»åˆ«ç‰¹å¾
    if categorical_features:
        print(f"\nç±»åˆ«ç‰¹å¾åˆ†å¸ƒå·®å¼‚:")
        for feature in categorical_features:
            if feature in X_A.columns and feature in X_B.columns:
                # è®¡ç®—ç±»åˆ«åˆ†å¸ƒ
                dist_A = X_A[feature].value_counts(normalize=True).sort_index()
                dist_B = X_B[feature].value_counts(normalize=True).sort_index()
                
                # è®¡ç®—JSæ•£åº¦
                all_categories = set(dist_A.index) | set(dist_B.index)
                p = np.array([dist_A.get(cat, 0) for cat in all_categories])
                q = np.array([dist_B.get(cat, 0) for cat in all_categories])
                
                # JSæ•£åº¦è®¡ç®—
                m = 0.5 * (p + q)
                js_div = 0.5 * stats.entropy(p, m) + 0.5 * stats.entropy(q, m)
                
                print(f"  {feature}:")
                print(f"    JSæ•£åº¦: {js_div:.3f}")
                print(f"    AåŸŸç±»åˆ«æ•°: {len(dist_A)}")
                print(f"    BåŸŸç±»åˆ«æ•°: {len(dist_B)}")
                
                if js_div > 0.1:
                    print(f"    âš ï¸  ç±»åˆ«åˆ†å¸ƒå·®å¼‚è¾ƒå¤§!")

def analyze_standardization_effect(X_A, X_B, features, categorical_indices):
    """åˆ†ææ ‡å‡†åŒ–çš„æ•ˆæœ"""
    print("\n" + "="*60)
    print("æ ‡å‡†åŒ–æ•ˆæœåˆ†æ")
    print("="*60)
    
    # åˆ†ç¦»æ•°å€¼ç‰¹å¾
    numerical_indices = [i for i in range(len(features)) if i not in categorical_indices]
    
    if not numerical_indices:
        print("æ²¡æœ‰æ•°å€¼ç‰¹å¾éœ€è¦æ ‡å‡†åŒ–")
        return None, None
    
    # åŸå§‹æ•°æ®
    X_A_num = X_A.iloc[:, numerical_indices].values
    X_B_num = X_B.iloc[:, numerical_indices].values
    
    # æ ‡å‡†åŒ– (ç”¨Aæ‹Ÿåˆ)
    scaler = StandardScaler()
    X_A_scaled = scaler.fit_transform(X_A_num)
    X_B_scaled = scaler.transform(X_B_num)
    
    print(f"\næ ‡å‡†åŒ–å‰:")
    print(f"  AåŸŸæ•°å€¼ç‰¹å¾å‡å€¼èŒƒå›´: [{X_A_num.mean(axis=0).min():.3f}, {X_A_num.mean(axis=0).max():.3f}]")
    print(f"  AåŸŸæ•°å€¼ç‰¹å¾æ ‡å‡†å·®èŒƒå›´: [{X_A_num.std(axis=0).min():.3f}, {X_A_num.std(axis=0).max():.3f}]")
    print(f"  BåŸŸæ•°å€¼ç‰¹å¾å‡å€¼èŒƒå›´: [{X_B_num.mean(axis=0).min():.3f}, {X_B_num.mean(axis=0).max():.3f}]")
    print(f"  BåŸŸæ•°å€¼ç‰¹å¾æ ‡å‡†å·®èŒƒå›´: [{X_B_num.std(axis=0).min():.3f}, {X_B_num.std(axis=0).max():.3f}]")
    
    print(f"\næ ‡å‡†åŒ–å:")
    print(f"  AåŸŸæ•°å€¼ç‰¹å¾å‡å€¼èŒƒå›´: [{X_A_scaled.mean(axis=0).min():.3f}, {X_A_scaled.mean(axis=0).max():.3f}]")
    print(f"  AåŸŸæ•°å€¼ç‰¹å¾æ ‡å‡†å·®èŒƒå›´: [{X_A_scaled.std(axis=0).min():.3f}, {X_A_scaled.std(axis=0).max():.3f}]")
    print(f"  BåŸŸæ•°å€¼ç‰¹å¾å‡å€¼èŒƒå›´: [{X_B_scaled.mean(axis=0).min():.3f}, {X_B_scaled.mean(axis=0).max():.3f}]")
    print(f"  BåŸŸæ•°å€¼ç‰¹å¾æ ‡å‡†å·®èŒƒå›´: [{X_B_scaled.std(axis=0).min():.3f}, {X_B_scaled.std(axis=0).max():.3f}]")
    
    # è®¡ç®—æ ‡å‡†åŒ–åçš„åˆ†å¸ƒå·®å¼‚
    mean_diffs_before = np.abs(X_A_num.mean(axis=0) - X_B_num.mean(axis=0))
    mean_diffs_after = np.abs(X_A_scaled.mean(axis=0) - X_B_scaled.mean(axis=0))
    
    print(f"\nå‡å€¼å·®å¼‚æ”¹å–„:")
    print(f"  æ ‡å‡†åŒ–å‰å¹³å‡å·®å¼‚: {mean_diffs_before.mean():.3f}")
    print(f"  æ ‡å‡†åŒ–åå¹³å‡å·®å¼‚: {mean_diffs_after.mean():.3f}")
    print(f"  æ”¹å–„æ¯”ä¾‹: {(1 - mean_diffs_after.mean()/mean_diffs_before.mean())*100:.1f}%")
    
    return X_A_scaled, X_B_scaled

def analyze_domain_gap(X_A, X_B, y_A, y_B):
    """åˆ†æåŸŸå·®è·"""
    print("\n" + "="*60)
    print("åŸŸå·®è·åˆ†æ")
    print("="*60)
    
    # åˆå¹¶æ•°æ®è¿›è¡ŒPCAåˆ†æ
    X_combined = np.vstack([X_A, X_B])
    domain_labels = np.hstack([np.zeros(len(X_A)), np.ones(len(X_B))])
    
    # PCAé™ç»´
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_combined)
    
    print(f"PCAè§£é‡Šæ–¹å·®æ¯”ä¾‹: {pca.explained_variance_ratio_}")
    print(f"å‰2ä¸ªä¸»æˆåˆ†ç´¯è®¡è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_.sum():.3f}")
    
    # è®¡ç®—åŸŸé—´è·ç¦»
    centroid_A = X_pca[domain_labels == 0].mean(axis=0)
    centroid_B = X_pca[domain_labels == 1].mean(axis=0)
    domain_distance = np.linalg.norm(centroid_A - centroid_B)
    
    print(f"åŸŸé—´è´¨å¿ƒè·ç¦» (PCAç©ºé—´): {domain_distance:.3f}")
    
    # åˆ†æç±»åˆ«æ¡ä»¶åˆ†å¸ƒ
    print(f"\nç±»åˆ«æ¡ä»¶åˆ†æ:")
    for class_label in [0, 1]:
        A_class_mask = (domain_labels == 0) & (np.hstack([y_A, y_B]) == class_label)
        B_class_mask = (domain_labels == 1) & (np.hstack([y_A, y_B]) == class_label)
        
        if A_class_mask.sum() > 0 and B_class_mask.sum() > 0:
            centroid_A_class = X_pca[A_class_mask].mean(axis=0)
            centroid_B_class = X_pca[B_class_mask].mean(axis=0)
            class_distance = np.linalg.norm(centroid_A_class - centroid_B_class)
            
            print(f"  ç±»åˆ«{class_label}åŸŸé—´è·ç¦»: {class_distance:.3f}")
    
    return X_pca, domain_labels

def analyze_feature_importance_stability(X_A, y_A, X_B, y_B, features):
    """åˆ†æç‰¹å¾é‡è¦æ€§çš„ç¨³å®šæ€§"""
    print("\n" + "="*60)
    print("ç‰¹å¾é‡è¦æ€§ç¨³å®šæ€§åˆ†æ")
    print("="*60)
    
    try:
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.feature_selection import mutual_info_classif
        
        # éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§
        rf_A = RandomForestClassifier(n_estimators=100, random_state=42)
        rf_A.fit(X_A, y_A)
        importance_A = rf_A.feature_importances_
        
        rf_B = RandomForestClassifier(n_estimators=100, random_state=42)
        rf_B.fit(X_B, y_B)
        importance_B = rf_B.feature_importances_
        
        # è®¡ç®—é‡è¦æ€§ç›¸å…³æ€§
        importance_corr = np.corrcoef(importance_A, importance_B)[0, 1]
        print(f"ç‰¹å¾é‡è¦æ€§ç›¸å…³æ€§ (RF): {importance_corr:.3f}")
        
        # äº’ä¿¡æ¯
        mi_A = mutual_info_classif(X_A, y_A, random_state=42)
        mi_B = mutual_info_classif(X_B, y_B, random_state=42)
        mi_corr = np.corrcoef(mi_A, mi_B)[0, 1]
        print(f"ç‰¹å¾é‡è¦æ€§ç›¸å…³æ€§ (MI): {mi_corr:.3f}")
        
        # æ‰“å°topç‰¹å¾å¯¹æ¯”
        print(f"\nTop 5 é‡è¦ç‰¹å¾å¯¹æ¯”:")
        top_A = np.argsort(importance_A)[-5:][::-1]
        top_B = np.argsort(importance_B)[-5:][::-1]
        
        print("æ•°æ®é›†A Top 5:", [features[i] for i in top_A])
        print("æ•°æ®é›†B Top 5:", [features[i] for i in top_B])
        
        # è®¡ç®—é‡å åº¦
        overlap = len(set(top_A) & set(top_B))
        print(f"Top 5ç‰¹å¾é‡å æ•°: {overlap}/5")
        
        if importance_corr < 0.5:
            print("âš ï¸  ç‰¹å¾é‡è¦æ€§åœ¨ä¸¤ä¸ªåŸŸé—´å·®å¼‚å¾ˆå¤§!")
        
    except ImportError:
        print("sklearnä¸å¯ç”¨ï¼Œè·³è¿‡ç‰¹å¾é‡è¦æ€§åˆ†æ")

def generate_diagnostic_report(feature_type='best7'):
    """ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š"""
    print("="*80)
    print("è·¨åŸŸæ€§èƒ½è¯Šæ–­æŠ¥å‘Š")
    print("="*80)
    
    # åŠ è½½æ•°æ®
    df_A, df_B = load_datasets()
    
    # è·å–ç‰¹å¾
    features = get_features_by_type(feature_type)
    categorical_indices = get_categorical_indices(feature_type)
    
    print(f"\nä½¿ç”¨ç‰¹å¾ç±»å‹: {feature_type}")
    print(f"ç‰¹å¾åˆ—è¡¨: {features}")
    print(f"ç±»åˆ«ç‰¹å¾ç´¢å¼•: {categorical_indices}")
    
    # åŸºç¡€ç»Ÿè®¡åˆ†æ
    X_A, y_A, X_B, y_B = analyze_basic_statistics(df_A, df_B, features)
    
    # ç‰¹å¾åˆ†å¸ƒåˆ†æ
    analyze_feature_distributions(X_A, X_B, features, categorical_indices)
    
    # æ ‡å‡†åŒ–æ•ˆæœåˆ†æ
    X_A_scaled, X_B_scaled = analyze_standardization_effect(X_A, X_B, features, categorical_indices)
    
    # åŸŸå·®è·åˆ†æ
    if X_A_scaled is not None and X_B_scaled is not None:
        # é‡æ–°ç»„åˆæ ‡å‡†åŒ–åçš„æ•°æ®
        X_A_combined = X_A.copy()
        X_B_combined = X_B.copy()
        
        # åªæ›¿æ¢æ•°å€¼ç‰¹å¾
        numerical_features = [f for i, f in enumerate(features) if i not in categorical_indices]
        if numerical_features:
            X_A_combined[numerical_features] = X_A_scaled
            X_B_combined[numerical_features] = X_B_scaled
        
        analyze_domain_gap(X_A_combined.values, X_B_combined.values, y_A, y_B)
    else:
        analyze_domain_gap(X_A.values, X_B.values, y_A, y_B)
    
    # ç‰¹å¾é‡è¦æ€§åˆ†æ
    analyze_feature_importance_stability(X_A, y_A, X_B, y_B, features)
    
    # ç”Ÿæˆå»ºè®®
    print("\n" + "="*60)
    print("æ”¹è¿›å»ºè®®")
    print("="*60)
    
    print("\nåŸºäºåˆ†æç»“æœçš„å»ºè®®:")
    print("1. ğŸ”§ ç¡®ä¿æ•°æ®æ ‡å‡†åŒ–: ç”¨AåŸŸæ•°æ®æ‹ŸåˆStandardScalerï¼Œç„¶ååº”ç”¨åˆ°BåŸŸ")
    print("2. ğŸ“Š æ£€æŸ¥ç±»åˆ«ä¸å¹³è¡¡: è€ƒè™‘ä½¿ç”¨class_weight='balanced'")
    print("3. ğŸ¯ ç‰¹å¾é€‰æ‹©: é€‰æ‹©åœ¨ä¸¤ä¸ªåŸŸé—´éƒ½ç¨³å®šçš„ç‰¹å¾")
    print("4. ğŸ”„ åŸŸé€‚åº”: è€ƒè™‘ä½¿ç”¨MMDç­‰åŸŸé€‚åº”æ–¹æ³•")
    print("5. ğŸ“ˆ æ¨¡å‹é€‰æ‹©: å°è¯•ä¸åŒçš„æ¨¡å‹ç±»å‹å’Œè¶…å‚æ•°")
    print("6. ğŸ² æ•°æ®å¢å¼º: è€ƒè™‘å¯¹å°‘æ•°ç±»è¿›è¡Œè¿‡é‡‡æ ·")
    
    # å…·ä½“çš„æ•°å€¼å»ºè®®
    pos_ratio_A = y_A.mean()
    pos_ratio_B = y_B.mean()
    ratio_diff = abs(pos_ratio_A - pos_ratio_B)
    
    if ratio_diff > 0.1:
        print(f"\nâš ï¸  ç±»åˆ«åˆ†å¸ƒå·®å¼‚è¾ƒå¤§ (AåŸŸ: {pos_ratio_A:.3f}, BåŸŸ: {pos_ratio_B:.3f})")
        print("   å»ºè®®: ä½¿ç”¨åˆ†å±‚é‡‡æ ·æˆ–è°ƒæ•´class_weight")
    
    if len(X_A) < 500:
        print(f"\nâš ï¸  è®­ç»ƒæ ·æœ¬è¾ƒå°‘ (AåŸŸ: {len(X_A)}æ ·æœ¬)")
        print("   å»ºè®®: è€ƒè™‘æ•°æ®å¢å¼ºæˆ–ä½¿ç”¨æ›´ç®€å•çš„æ¨¡å‹")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ•°æ®åˆ†å¸ƒåˆ†æè¯Šæ–­')
    parser.add_argument('--feature-type', type=str, default='best7',
                       choices=['all', 'best7'], help='ç‰¹å¾ç±»å‹')
    
    args = parser.parse_args()
    
    try:
        generate_diagnostic_report(args.feature_type)
    except Exception as e:
        print(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 