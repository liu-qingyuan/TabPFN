"""
è´å¶æ–¯ä¼˜åŒ–æ¨¡å—

å®ç°åŸºäºç›®æ ‡åŸŸéªŒè¯é›†çš„è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–ï¼Œé‡‡ç”¨ä¸‰åˆ†æ³•æ•°æ®åˆ’åˆ†ï¼š
1. AåŸŸè®­ç»ƒé›†ï¼šç”¨äºæ¨¡å‹è®­ç»ƒ
2. ç›®æ ‡åŸŸéªŒè¯é›†ï¼šç”¨äºè´å¶æ–¯ä¼˜åŒ–ç›®æ ‡å‡½æ•°è¯„ä¼°
3. ç›®æ ‡åŸŸä¿ç•™æµ‹è¯•é›†ï¼šç”¨äºæœ€ç»ˆæ¨¡å‹æ³›åŒ–èƒ½åŠ›è¯„ä¼°
"""

import os
import json
import logging
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Any, Tuple, List, Optional
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, confusion_matrix
from skopt import gp_minimize
from skopt.space import Real, Integer, Categorical
from skopt.utils import use_named_args
import warnings
import time

# å¯¼å…¥ AutoTabPFNClassifier
try:
    from tabpfn_extensions.post_hoc_ensembles.sklearn_interface import AutoTabPFNClassifier
    AUTO_TABPFN_AVAILABLE = True
except ImportError:
    logging.warning("AutoTabPFNä¸å¯ç”¨ï¼Œè¯·å®‰è£…tabpfn_extensions")
    AUTO_TABPFN_AVAILABLE = False

from ..config.settings import (
    DATA_PATHS, LABEL_COL, get_features_by_type, get_categorical_indices
)
from ..data.loader import load_all_datasets
from ..preprocessing.scaler import fit_apply_scaler
from ..modeling.model_selector import get_model

# è¿‡æ»¤è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)

class NumpyEncoder(json.JSONEncoder):
    """è‡ªå®šä¹‰JSONç¼–ç å™¨ï¼Œå¤„ç†NumPyæ•°æ®ç±»å‹"""
    def default(self, o: Any) -> Any:
        if isinstance(o, np.integer):
            return int(o)
        elif isinstance(o, np.floating):
            return float(o)
        elif isinstance(o, np.ndarray):
            return o.tolist()
        elif isinstance(o, np.bool_):
            return bool(o)
        return super().default(o)

class BayesianOptimizer:
    """è´å¶æ–¯ä¼˜åŒ–å™¨ç±»"""
    
    def __init__(self, 
                 model_type: str = 'auto',
                 feature_type: str = 'best7',
                 use_categorical: bool = True,
                 validation_split: float = 0.7,
                 n_calls: int = 50,
                 random_state: int = 42,
                 save_path: str = './results_bayesian_optimization',
                 target_domain: str = 'B'):  # æ·»åŠ ç›®æ ‡åŸŸå‚æ•°
        """
        åˆå§‹åŒ–è´å¶æ–¯ä¼˜åŒ–å™¨
        
        å‚æ•°:
        - model_type: æ¨¡å‹ç±»å‹ ('auto', 'base', 'rf', 'tuned')
        - feature_type: ç‰¹å¾ç±»å‹ ('all', 'best7')
        - use_categorical: æ˜¯å¦ä½¿ç”¨ç±»åˆ«ç‰¹å¾
        - validation_split: éªŒè¯é›†æ¯”ä¾‹ (0.7è¡¨ç¤º70%ç”¨äºéªŒè¯ï¼Œ30%ç”¨äºholdout)
        - n_calls: è´å¶æ–¯ä¼˜åŒ–è¿­ä»£æ¬¡æ•°
        - random_state: éšæœºç§å­
        - save_path: ç»“æœä¿å­˜è·¯å¾„
        - target_domain: ç›®æ ‡åŸŸé€‰æ‹© ('B' æˆ– 'C')
        """
        self.model_type = model_type
        self.feature_type = feature_type
        self.use_categorical = use_categorical
        self.validation_split = validation_split
        self.n_calls = n_calls
        self.random_state = random_state
        self.save_path = save_path
        self.target_domain = target_domain  # æ·»åŠ ç›®æ ‡åŸŸå±æ€§
        
        # è·å–ç‰¹å¾å’Œç±»åˆ«ç´¢å¼•
        self.features = get_features_by_type(feature_type)
        self.categorical_indices = get_categorical_indices(feature_type) if use_categorical else []
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_path, exist_ok=True)
        
        # åˆå§‹åŒ–æ•°æ®å­˜å‚¨
        self.X_train = None
        self.y_train = None
        self.X_ext_val = None
        self.y_ext_val = None
        self.X_ext_holdout = None
        self.y_ext_holdout = None
        self.scaler = None
        
        # ä¼˜åŒ–ç»“æœå­˜å‚¨
        self.optimization_results = []
        self.best_params = None
        self.best_score = None
        self.final_model = None
        
        # æ·»åŠ ä¼˜ç§€é…ç½®å­˜å‚¨
        self.good_configs = []  # å­˜å‚¨æµ‹è¯•é›†AUC > 0.7çš„é…ç½®
        
        logging.info(f"åˆå§‹åŒ–è´å¶æ–¯ä¼˜åŒ–å™¨:")
        logging.info(f"  æ¨¡å‹ç±»å‹: {model_type}")
        logging.info(f"  ç‰¹å¾ç±»å‹: {feature_type} ({len(self.features)}ä¸ªç‰¹å¾)")
        logging.info(f"  ä½¿ç”¨ç±»åˆ«ç‰¹å¾: {use_categorical}")
        logging.info(f"  éªŒè¯é›†æ¯”ä¾‹: {validation_split}")
        logging.info(f"  ä¼˜åŒ–è¿­ä»£æ¬¡æ•°: {n_calls}")
        logging.info(f"  ç›®æ ‡åŸŸ: {target_domain}")  # æ·»åŠ ç›®æ ‡åŸŸæ—¥å¿—
        logging.info(f"  å›ºå®šå‚æ•°: validation_method=cv, n_folds=5, max_time=1")
        
    def load_and_prepare_data(self) -> None:
        """åŠ è½½å¹¶å‡†å¤‡æ•°æ®ï¼Œå®ç°ä¸‰åˆ†æ³•åˆ’åˆ†"""
        logging.info("åŠ è½½å’Œå‡†å¤‡æ•°æ®...")
        
        # åŠ è½½æ•°æ®é›†A (è®­ç»ƒé›†)
        df_A = pd.read_excel(DATA_PATHS['A'])
        X_A_raw = df_A[self.features].values
        y_A = df_A[LABEL_COL].values
        
        # åŠ è½½ç›®æ ‡åŸŸæ•°æ®é›† (éœ€è¦åˆ’åˆ†ä¸ºéªŒè¯é›†å’Œä¿ç•™æµ‹è¯•é›†)
        df_target = pd.read_excel(DATA_PATHS[self.target_domain])
        X_target_raw = df_target[self.features].values
        y_target = df_target[LABEL_COL].values
        
        # æ•°æ®æ ‡å‡†åŒ– - ç”¨Aæ•°æ®é›†æ‹Ÿåˆscaler
        X_A_scaled, X_target_scaled, self.scaler = fit_apply_scaler(X_A_raw, X_target_raw)
        
        # è®¾ç½®AåŸŸè®­ç»ƒæ•°æ®
        self.X_train = X_A_scaled
        self.y_train = y_A
        
        # å¯¹ç›®æ ‡åŸŸæ•°æ®è¿›è¡Œä¸‰åˆ†æ³•åˆ’åˆ†ï¼šéªŒè¯é›† vs ä¿ç•™æµ‹è¯•é›†
        self.X_ext_val, self.X_ext_holdout, self.y_ext_val, self.y_ext_holdout = train_test_split(
            X_target_scaled, y_target,
            train_size=self.validation_split,
            stratify=y_target,
            random_state=self.random_state
        )
        
        # æ‰“å°æ•°æ®ä¿¡æ¯
        logging.info(f"æ•°æ®åˆ’åˆ†å®Œæˆ:")
        logging.info(f"  AåŸŸè®­ç»ƒé›†: {self.X_train.shape[0]} æ ·æœ¬")
        logging.info(f"  {self.target_domain}åŸŸéªŒè¯é›†: {self.X_ext_val.shape[0]} æ ·æœ¬")
        logging.info(f"  {self.target_domain}åŸŸä¿ç•™æµ‹è¯•é›†: {self.X_ext_holdout.shape[0]} æ ·æœ¬")
        logging.info(f"  AåŸŸæ ‡ç­¾åˆ†å¸ƒ: {np.bincount(self.y_train.astype(int))}")
        logging.info(f"  {self.target_domain}åŸŸéªŒè¯é›†æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(self.y_ext_val.astype(int))}")
        logging.info(f"  {self.target_domain}åŸŸä¿ç•™æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(self.y_ext_holdout.astype(int))}")
        
    def define_search_space(self) -> List:
        """å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´"""
        if self.model_type == 'auto':
            # ä¼˜åŒ–çš„AutoTabPFNå‚æ•°ç©ºé—´ - ä¿®å¤max_timeæ”¯æŒé—®é¢˜
            search_space = [
                # è®­ç»ƒæ—¶é—´ - é™åˆ¶ä¸ºAutoTabPFNæ”¯æŒçš„èŒƒå›´ (1-180ç§’)
                # æ ¹æ®é”™è¯¯æ—¥å¿—ï¼Œå¤§äº180çš„å€¼ä¸è¢«æ”¯æŒ
                Categorical([1, 5, 10, 15, 30, 60, 120, 180], name='max_time'),
                
                # é¢„è®¾é…ç½® - å¯¹æ³›åŒ–èƒ½åŠ›å¾ˆé‡è¦
                Categorical(['default', 'avoid_overfitting'], name='preset'),
                
                # è¯„åˆ†æŒ‡æ ‡ - åŒ»ç–—æ•°æ®é€šå¸¸å…³æ³¨AUCå’ŒF1
                Categorical(['accuracy', 'roc', 'f1'], name='ges_scoring'),
                
                # æ¨¡å‹æ•°é‡ - é€‚ä¸­èŒƒå›´ï¼Œé¿å…è¿‡æ‹Ÿåˆ
                Categorical([10, 15, 20, 25, 30], name='max_models'),
                
                # éªŒè¯æ–¹æ³• - CVå¯¹å°æ•°æ®é›†æ›´ç¨³å®š
                Categorical(['cv'], name='validation_method'),  # å›ºå®šä¸ºCVï¼Œæ›´é€‚åˆåŒ»ç–—æ•°æ®
                
                # é‡å¤æ¬¡æ•° - å‡å°‘èŒƒå›´ï¼Œæé«˜æ•ˆç‡
                Integer(50, 150, name='n_repeats'),
                
                # æŠ˜æ•° - åŒ»ç–—æ•°æ®å¸¸ç”¨5æŠ˜
                Categorical([5], name='n_folds'),  # å›ºå®šä¸º5æŠ˜
                
                # GESè¿­ä»£æ¬¡æ•° - é‡è¦çš„è¶…å‚æ•°
                Integer(20, 40, name='ges_n_iterations'),
                
                # æ˜¯å¦å¿½ç•¥é¢„è®­ç»ƒé™åˆ¶ - å¯¹è·¨åŸŸå¯èƒ½é‡è¦
                Categorical([True, False], name='ignore_limits'),
            ]
            
            logging.info("å®šä¹‰äº†9ä¸ªè¶…å‚æ•°çš„æœç´¢ç©ºé—´")
            logging.info("æœç´¢ç©ºé—´ä¼˜åŒ–è¦ç‚¹:")
            logging.info("  - ä¿®å¤max_timeèŒƒå›´ (1-180ç§’)ï¼Œç§»é™¤ä¸æ”¯æŒçš„300ç§’")
            logging.info("  - ä¸“æ³¨äºAUCå’ŒF1è¯„åˆ†æŒ‡æ ‡")
            logging.info("  - å›ºå®šä½¿ç”¨CVéªŒè¯æ–¹æ³•")
            logging.info("  - ç§»é™¤holdout_fractionå‚æ•°")
            logging.info("  - å¢åŠ ges_n_iterationsæœç´¢èŒƒå›´")
            
        elif self.model_type == 'rf':
            # Random Forestå‚æ•°ç©ºé—´
            search_space = [
                Integer(50, 500, name='n_estimators'),  # æ ‘çš„æ•°é‡
                Integer(1, 20, name='max_depth'),  # æœ€å¤§æ·±åº¦
                Integer(2, 20, name='min_samples_split'),  # æœ€å°åˆ†å‰²æ ·æœ¬æ•°
                Integer(1, 10, name='min_samples_leaf'),  # æœ€å°å¶å­æ ·æœ¬æ•°
                Real(0.1, 1.0, name='max_features'),  # æœ€å¤§ç‰¹å¾æ¯”ä¾‹
                Categorical(['auto', 'balanced'], name='class_weight'),  # ç±»åˆ«æƒé‡
            ]
        elif self.model_type == 'base':
            # åŸºç¡€TabPFNå‚æ•°ç©ºé—´ (å‚æ•°è¾ƒå°‘)
            search_space = [
                Integer(1, 5, name='N_ensemble_configurations'),  # é›†æˆé…ç½®æ•°
                Categorical(['auto', 'balanced'], name='class_weight'),  # ç±»åˆ«æƒé‡
            ]
        else:
            # é»˜è®¤å‚æ•°ç©ºé—´
            search_space = [
                Real(0.01, 1.0, name='learning_rate'),
                Integer(10, 200, name='max_iter'),
            ]
        
        return search_space
    
    def objective_function(self, params: List) -> float:
        """
        ç›®æ ‡å‡½æ•°ï¼šè¯„ä¼°ç»™å®šè¶…å‚æ•°ç»„åˆçš„æ€§èƒ½
        
        å‚æ•°:
        - params: è¶…å‚æ•°å€¼åˆ—è¡¨ï¼Œé¡ºåºå¯¹åº”æœç´¢ç©ºé—´å®šä¹‰
        
        è¿”å›:
        - float: è´ŸéªŒè¯é›†AUC (å› ä¸ºgp_minimizeæœ€å°åŒ–ç›®æ ‡å‡½æ•°)
        """
        try:
            # å°†å‚æ•°åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸
            search_space = self.define_search_space()
            param_dict = {dim.name: params[i] for i, dim in enumerate(search_space)}
            
            logging.info(f"å¼€å§‹è¯„ä¼°å‚æ•°ç»„åˆ: {param_dict}")
            
            # æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºæ¨¡å‹
            if self.model_type == 'auto':
                # æ£€æŸ¥AutoTabPFNæ˜¯å¦å¯ç”¨
                if not AUTO_TABPFN_AVAILABLE:
                    logging.error("AutoTabPFNä¸å¯ç”¨ï¼Œæ— æ³•åˆ›å»ºæ¨¡å‹")
                    raise ImportError("AutoTabPFNä¸å¯ç”¨ï¼Œè¯·å®‰è£…tabpfn_extensions")
                
                # æ„å»ºphe_init_argså‚æ•°
                phe_init_args = {
                    'max_models': param_dict.get('max_models', 15),
                    'validation_method': param_dict.get('validation_method', 'cv'),
                    'n_repeats': param_dict.get('n_repeats', 100),
                    'n_folds': param_dict.get('n_folds', 5),
                    'ges_n_iterations': param_dict.get('ges_n_iterations', 20)
                }
                
                logging.info(f"åˆ›å»ºAutoTabPFNæ¨¡å‹:")
                logging.info(f"  max_time: {param_dict.get('max_time', 30)}")
                logging.info(f"  preset: {param_dict.get('preset', 'default')}")
                logging.info(f"  ges_scoring: {param_dict.get('ges_scoring', 'roc')}")
                logging.info(f"  phe_init_args: {phe_init_args}")
                
                # åˆ›å»ºAutoTabPFNæ¨¡å‹
                model = AutoTabPFNClassifier(
                    max_time=param_dict.get('max_time', 30),  # ä½¿ç”¨æœç´¢åˆ°çš„max_time
                    preset=param_dict.get('preset', 'default'),
                    ges_scoring_string=param_dict.get('ges_scoring', 'roc'),
                    device='cuda',
                    random_state=self.random_state,
                    ignore_pretraining_limits=param_dict.get('ignore_limits', False),
                    categorical_feature_indices=self.categorical_indices if self.categorical_indices else None,
                    phe_init_args=phe_init_args
                )
                
                logging.info("AutoTabPFNæ¨¡å‹åˆ›å»ºæˆåŠŸ")
                
            else:
                # å…¶ä»–æ¨¡å‹ç±»å‹çš„å¤„ç†ä¿æŒåŸæœ‰é€»è¾‘
                logging.info(f"åˆ›å»º{self.model_type}æ¨¡å‹ï¼Œå‚æ•°: {param_dict}")
                model_config = {
                    'categorical_feature_indices': self.categorical_indices if self.use_categorical else []
                }
                model_config.update(param_dict)
                model = get_model(self.model_type, **model_config)
                logging.info(f"{self.model_type}æ¨¡å‹åˆ›å»ºæˆåŠŸ")
            
            # è®­ç»ƒæ¨¡å‹
            if self.X_train is not None:
                logging.info(f"å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œè®­ç»ƒé›†å¤§å°: {self.X_train.shape}")
                start_time = time.time()
                model.fit(self.X_train, self.y_train)
                training_time = time.time() - start_time
                logging.info(f"æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")
            else:
                logging.error("è®­ç»ƒæ•°æ®æœªåŠ è½½")
                raise ValueError("è®­ç»ƒæ•°æ®æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_and_prepare_data()")
            
            # åœ¨éªŒè¯é›†ä¸Šé¢„æµ‹
            if self.X_ext_val is not None:
                logging.info(f"åœ¨éªŒè¯é›†ä¸Šé¢„æµ‹ï¼ŒéªŒè¯é›†å¤§å°: {self.X_ext_val.shape}")
                y_pred_proba = model.predict_proba(self.X_ext_val)
                if y_pred_proba.ndim > 1:
                    y_pred_proba = y_pred_proba[:, 1]  # å–æ­£ç±»æ¦‚ç‡
            else:
                logging.error("éªŒè¯æ•°æ®æœªåŠ è½½")
                raise ValueError("éªŒè¯æ•°æ®æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_and_prepare_data()")
            
            # è®¡ç®—éªŒè¯é›†AUC
            val_auc_score = roc_auc_score(self.y_ext_val, y_pred_proba)
            logging.info(f"éªŒè¯é›†AUC: {val_auc_score:.4f}")
            
            # åœ¨æµ‹è¯•é›†ä¸Šé¢„æµ‹å’Œè¯„ä¼°
            if self.X_ext_holdout is not None:
                logging.info(f"åœ¨æµ‹è¯•é›†ä¸Šé¢„æµ‹ï¼Œæµ‹è¯•é›†å¤§å°: {self.X_ext_holdout.shape}")
                y_test_pred_proba = model.predict_proba(self.X_ext_holdout)
                if y_test_pred_proba.ndim > 1:
                    y_test_pred_proba = y_test_pred_proba[:, 1]
            else:
                logging.error("æµ‹è¯•æ•°æ®æœªåŠ è½½")
                raise ValueError("æµ‹è¯•æ•°æ®æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_and_prepare_data()")
                
            # è®¡ç®—æµ‹è¯•é›†AUC
            test_auc_score = roc_auc_score(self.y_ext_holdout, y_test_pred_proba)
            logging.info(f"æµ‹è¯•é›†AUC: {test_auc_score:.4f}")
            
            # è®°å½•è¯•éªŒç»“æœ
            trial_result = {
                'params': {k: (int(v) if isinstance(v, np.integer) else float(v) if isinstance(v, np.floating) else v) 
                          for k, v in param_dict.items()},  # è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
                'validation_auc': float(val_auc_score),
                'test_auc': float(test_auc_score),
                'training_time': float(training_time),
                'trial_id': len(self.optimization_results)
            }
            self.optimization_results.append(trial_result)
            
            # å¦‚æœæµ‹è¯•é›†AUC > 0.7ï¼Œä¿å­˜é…ç½®
            if test_auc_score > 0.7:
                good_config = {
                    'trial_id': len(self.optimization_results),
                    'params': {k: (int(v) if isinstance(v, np.integer) else float(v) if isinstance(v, np.floating) else v) 
                              for k, v in param_dict.items()},
                    'validation_auc': float(val_auc_score),
                    'test_auc': float(test_auc_score),
                    'training_time': float(training_time)
                }
                self.good_configs.append(good_config)
                logging.info(f"â˜… ä¼˜ç§€é…ç½® {len(self.good_configs)}: æµ‹è¯•é›†AUC={test_auc_score:.4f} > 0.7ï¼Œå·²ä¿å­˜é…ç½®")
            
            logging.info(f"è¯•éªŒ {len(self.optimization_results)}: éªŒè¯é›†AUC={val_auc_score:.4f}, æµ‹è¯•é›†AUC={test_auc_score:.4f}, å‚æ•°={param_dict}")
            
            # è¿”å›è´ŸéªŒè¯é›†AUC (å› ä¸ºgp_minimizeæœ€å°åŒ–ç›®æ ‡å‡½æ•°)
            return float(-val_auc_score)
            
        except Exception as e:
            error_msg = str(e)
            error_type = type(e).__name__
            
            logging.error("=" * 60)
            logging.error("ç›®æ ‡å‡½æ•°è¯„ä¼°å¤±è´¥ - è¯¦ç»†é”™è¯¯æŠ¥å‘Š")
            logging.error("=" * 60)
            logging.error(f"é”™è¯¯ç±»å‹: {error_type}")
            logging.error(f"é”™è¯¯ä¿¡æ¯: {error_msg}")
            logging.error(f"å¤±è´¥çš„å‚æ•°ç»„åˆ: {param_dict}")
            logging.error(f"æ¨¡å‹ç±»å‹: {self.model_type}")
            
            # è¯¦ç»†çš„é”™è¯¯åˆ†æ
            if 'max_time' in error_msg and 'not supported' in error_msg:
                logging.error("ğŸ” é”™è¯¯åˆ†æ: max_timeå‚æ•°ä¸è¢«æ”¯æŒ")
                logging.error(f"   å½“å‰max_timeå€¼: {param_dict.get('max_time', 'unknown')}")
                logging.error("   å¯èƒ½åŸå› :")
                logging.error("   1. ä¼ é€’ç»™äº†æ™®é€šTabPFNè€Œä¸æ˜¯AutoTabPFN")
                logging.error("   2. AutoTabPFNç‰ˆæœ¬ä¸æ”¯æŒè¯¥max_timeå€¼")
                logging.error("   3. æ¨¡å‹åˆ›å»ºè¿‡ç¨‹ä¸­å‚æ•°ä¼ é€’é”™è¯¯")
                
            elif 'memory' in error_msg.lower() or 'cuda' in error_msg.lower():
                logging.error("ğŸ” é”™è¯¯åˆ†æ: GPUå†…å­˜ç›¸å…³é—®é¢˜")
                logging.error("   å»ºè®®è§£å†³æ–¹æ¡ˆ:")
                logging.error("   1. å‡å°‘max_modelså‚æ•°")
                logging.error("   2. å‡å°‘n_repeatså‚æ•°")
                logging.error("   3. ä½¿ç”¨CPUè®¾å¤‡")
                
            elif 'import' in error_msg.lower() or 'module' in error_msg.lower():
                logging.error("ğŸ” é”™è¯¯åˆ†æ: æ¨¡å—å¯¼å…¥é—®é¢˜")
                logging.error("   å¯èƒ½ç¼ºå°‘ä¾èµ–åŒ…:")
                logging.error("   1. tabpfn_extensions")
                logging.error("   2. AutoTabPFNClassifier")
                
            elif 'categorical' in error_msg.lower():
                logging.error("ğŸ” é”™è¯¯åˆ†æ: ç±»åˆ«ç‰¹å¾å¤„ç†é—®é¢˜")
                logging.error(f"   ç±»åˆ«ç‰¹å¾ç´¢å¼•: {self.categorical_indices}")
                
            else:
                logging.error("ğŸ” é”™è¯¯åˆ†æ: æœªçŸ¥é”™è¯¯ç±»å‹")
                logging.error("   å»ºè®®:")
                logging.error("   1. æ£€æŸ¥æ•°æ®æ ¼å¼å’Œå¤§å°")
                logging.error("   2. éªŒè¯æ¨¡å‹å‚æ•°æœ‰æ•ˆæ€§")
                logging.error("   3. æŸ¥çœ‹å®Œæ•´çš„é”™è¯¯å †æ ˆ")
            
            # å¯¼å…¥tracebackä»¥è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯
            import traceback
            logging.error("å®Œæ•´é”™è¯¯å †æ ˆ:")
            logging.error(traceback.format_exc())
            logging.error("=" * 60)
            
            # è¿”å›ä¸€ä¸ªè¾ƒå¤§çš„å€¼è¡¨ç¤ºå¤±è´¥ (å› ä¸ºæˆ‘ä»¬è¦æœ€å°åŒ–)
            return 1.0
    
    def run_optimization(self) -> Dict[str, Any]:
        """è¿è¡Œè´å¶æ–¯ä¼˜åŒ–"""
        logging.info("å¼€å§‹è´å¶æ–¯ä¼˜åŒ–...")
        
        # ç¡®ä¿æ•°æ®å·²åŠ è½½
        if self.X_train is None:
            self.load_and_prepare_data()
        
        # å®šä¹‰æœç´¢ç©ºé—´
        search_space = self.define_search_space()
        
        # åˆ›å»ºç›®æ ‡å‡½æ•°è£…é¥°å™¨
        @use_named_args(search_space)
        def objective(**params):
            # å°†å‘½åå‚æ•°è½¬æ¢ä¸ºåˆ—è¡¨å½¢å¼
            param_values = [params[dim.name] for dim in search_space]
            return self.objective_function(param_values)
        
        # è¿è¡Œè´å¶æ–¯ä¼˜åŒ–
        logging.info(f"å¼€å§‹{self.n_calls}æ¬¡è´å¶æ–¯ä¼˜åŒ–è¿­ä»£...")
        result = gp_minimize(
            func=objective,
            dimensions=search_space,
            n_calls=self.n_calls,
            random_state=self.random_state,
            acq_func='EI',  # Expected Improvement
            n_initial_points=20  # 20%çš„åˆå§‹éšæœºç‚¹æ•°ï¼Œæ›´åˆç†çš„æ¯”ä¾‹
        )
        
        # æå–æœ€ä½³å‚æ•°
        self.best_params = {}
        for i, dim in enumerate(search_space):
            self.best_params[dim.name] = result.x[i]
        
        self.best_score = -result.fun  # è½¬æ¢å›æ­£AUC
        
        logging.info("è´å¶æ–¯ä¼˜åŒ–å®Œæˆ!")
        logging.info(f"æœ€ä½³éªŒè¯é›†AUC: {self.best_score:.4f}")
        logging.info(f"æœ€ä½³å‚æ•°: {self.best_params}")
        
        # è¾“å‡ºä¼˜ç§€é…ç½®æ±‡æ€»
        if self.good_configs:
            logging.info(f"\nå‘ç° {len(self.good_configs)} ä¸ªä¼˜ç§€é…ç½® (æµ‹è¯•é›†AUC > 0.7):")
            for i, config in enumerate(self.good_configs):
                logging.info(f"  é…ç½® {i+1}: éªŒè¯é›†AUC={config['validation_auc']:.4f}, æµ‹è¯•é›†AUC={config['test_auc']:.4f}")
        else:
            logging.info("æœªå‘ç°æµ‹è¯•é›†AUC > 0.7çš„é…ç½®")
        
        return {
            'best_params': self.best_params,
            'best_validation_auc': self.best_score,
            'optimization_history': self.optimization_results,
            'good_configs': self.good_configs,  # æ·»åŠ ä¼˜ç§€é…ç½®
            'total_trials': len(self.optimization_results)
        }
    
    def train_final_model(self) -> Dict[str, Any]:
        """ä½¿ç”¨æœ€ä½³å‚æ•°åœ¨å®Œæ•´AåŸŸæ•°æ®ä¸Šè®­ç»ƒæœ€ç»ˆæ¨¡å‹"""
        logging.info("ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
        
        if self.best_params is None:
            raise ValueError("è¯·å…ˆè¿è¡Œè´å¶æ–¯ä¼˜åŒ–è·å–æœ€ä½³å‚æ•°")
        
        # æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºæœ€ç»ˆæ¨¡å‹
        if self.model_type == 'auto':
            # åˆ›å»ºAutoTabPFNæ¨¡å‹
            self.final_model = AutoTabPFNClassifier(
                max_time=self.best_params.get('max_time', 30),  # ä½¿ç”¨æœç´¢åˆ°çš„max_time
                preset=self.best_params.get('preset', 'default'),
                ges_scoring_string=self.best_params.get('ges_scoring', 'roc'),
                device='cuda',
                random_state=self.random_state,
                ignore_pretraining_limits=self.best_params.get('ignore_limits', False),
                categorical_feature_indices=self.categorical_indices if self.categorical_indices else None,
                phe_init_args={
                    'max_models': self.best_params.get('max_models', 15),
                    'validation_method': 'cv',
                    'n_folds': 5,
                    'ges_n_iterations': self.best_params.get('ges_n_iterations', 20)
                }
            )
        else:
            # å…¶ä»–æ¨¡å‹ç±»å‹çš„å¤„ç†ä¿æŒåŸæœ‰é€»è¾‘
            final_config = {
                'categorical_feature_indices': self.categorical_indices if self.use_categorical else []
            }
            final_config.update(self.best_params)
            self.final_model = get_model(self.model_type, **final_config)
        
        # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        self.final_model.fit(self.X_train, self.y_train)
        
        logging.info("æœ€ç»ˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        return self.best_params
    
    def evaluate_final_model(self) -> Dict[str, Any]:
        """åœ¨ä¿ç•™æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ¨¡å‹"""
        logging.info("åœ¨ä¿ç•™æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ¨¡å‹...")
        
        if self.final_model is None:
            raise ValueError("è¯·å…ˆè®­ç»ƒæœ€ç»ˆæ¨¡å‹")
        
        # åœ¨ä¿ç•™æµ‹è¯•é›†ä¸Šé¢„æµ‹
        y_pred = self.final_model.predict(self.X_ext_holdout)
        y_pred_proba = self.final_model.predict_proba(self.X_ext_holdout)
        if y_pred_proba.ndim > 1:
            y_pred_proba = y_pred_proba[:, 1]
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        holdout_results = {
            'auc': roc_auc_score(self.y_ext_holdout, y_pred_proba),
            'f1': f1_score(self.y_ext_holdout, y_pred),
            'accuracy': accuracy_score(self.y_ext_holdout, y_pred),
            'confusion_matrix': confusion_matrix(self.y_ext_holdout, y_pred).tolist()
        }
        
        # åŒæ—¶åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°ä»¥ä¾¿å¯¹æ¯”
        y_val_pred = self.final_model.predict(self.X_ext_val)
        y_val_pred_proba = self.final_model.predict_proba(self.X_ext_val)
        if y_val_pred_proba.ndim > 1:
            y_val_pred_proba = y_val_pred_proba[:, 1]
        
        validation_results = {
            'auc': roc_auc_score(self.y_ext_val, y_val_pred_proba),
            'f1': f1_score(self.y_ext_val, y_val_pred),
            'accuracy': accuracy_score(self.y_ext_val, y_val_pred),
            'confusion_matrix': confusion_matrix(self.y_ext_val, y_val_pred).tolist()
        }
        
        logging.info("æœ€ç»ˆæ¨¡å‹è¯„ä¼°å®Œæˆ:")
        logging.info(f"  éªŒè¯é›† - AUC: {validation_results['auc']:.4f}, F1: {validation_results['f1']:.4f}, Acc: {validation_results['accuracy']:.4f}")
        logging.info(f"  ä¿ç•™æµ‹è¯•é›† - AUC: {holdout_results['auc']:.4f}, F1: {holdout_results['f1']:.4f}, Acc: {holdout_results['accuracy']:.4f}")
        
        return {
            'validation_performance': validation_results,
            'holdout_performance': holdout_results
        }
    
    def plot_confusion_matrix(self, results: Dict[str, Any]) -> None:
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        logging.info("ç»˜åˆ¶æ··æ·†çŸ©é˜µ...")
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        # éªŒè¯é›†æ··æ·†çŸ©é˜µ
        cm_val = np.array(results['validation_performance']['confusion_matrix'])
        sns.heatmap(cm_val, annot=True, fmt='d', cmap='Blues', ax=ax1)
        ax1.set_title('Validation Set Confusion Matrix')
        ax1.set_xlabel('Predicted')
        ax1.set_ylabel('Actual')
        
        # ä¿ç•™æµ‹è¯•é›†æ··æ·†çŸ©é˜µ
        cm_holdout = np.array(results['holdout_performance']['confusion_matrix'])
        sns.heatmap(cm_holdout, annot=True, fmt='d', cmap='Blues', ax=ax2)
        ax2.set_title('Holdout Test Set Confusion Matrix')
        ax2.set_xlabel('Predicted')
        ax2.set_ylabel('Actual')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        cm_path = os.path.join(self.save_path, 'confusion_matrices.png')
        plt.savefig(cm_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logging.info(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {cm_path}")
    
    def save_results(self, optimization_results: Dict[str, Any], 
                    final_results: Dict[str, Any]) -> None:
        """ä¿å­˜æ‰€æœ‰ç»“æœ"""
        logging.info("ä¿å­˜ä¼˜åŒ–ç»“æœ...")
        
        # ä¿å­˜ä¼˜åŒ–å†å²
        optimization_path = os.path.join(self.save_path, 'optimization_history.json')
        with open(optimization_path, 'w', encoding='utf-8') as f:
            json.dump(optimization_results, f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
        
        # ä¿å­˜æœ€ç»ˆè¯„ä¼°ç»“æœ
        final_path = os.path.join(self.save_path, 'final_evaluation.json')
        final_save_data = {
            'best_params': self.best_params,
            'model_config': {
                'model_type': self.model_type,
                'feature_type': self.feature_type,
                'use_categorical': self.use_categorical,
                'features_count': len(self.features),
                'categorical_indices_count': len(self.categorical_indices)
            },
            'data_split': {
                'train_samples': self.X_train.shape[0],
                'validation_samples': self.X_ext_val.shape[0],
                'holdout_samples': self.X_ext_holdout.shape[0],
                'validation_split_ratio': self.validation_split
            },
            'performance': final_results
        }
        
        with open(final_path, 'w', encoding='utf-8') as f:
            json.dump(final_save_data, f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
        
        logging.info(f"ä¼˜åŒ–å†å²å·²ä¿å­˜åˆ°: {optimization_path}")
        logging.info(f"æœ€ç»ˆè¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {final_path}")
    
    def run_complete_optimization(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„è´å¶æ–¯ä¼˜åŒ–æµç¨‹"""
        logging.info("=" * 60)
        logging.info("å¼€å§‹å®Œæ•´è´å¶æ–¯ä¼˜åŒ–æµç¨‹")
        logging.info("=" * 60)
        
        try:
            # 1. åŠ è½½å’Œå‡†å¤‡æ•°æ®
            self.load_and_prepare_data()
            
            # 2. è¿è¡Œè´å¶æ–¯ä¼˜åŒ–
            optimization_results = self.run_optimization()
            
            # 3. è®­ç»ƒæœ€ç»ˆæ¨¡å‹
            final_config = self.train_final_model()
            
            # 4. è¯„ä¼°æœ€ç»ˆæ¨¡å‹
            final_results = self.evaluate_final_model()
            
            # 5. ç»˜åˆ¶æ··æ·†çŸ©é˜µ
            self.plot_confusion_matrix(final_results)
            
            # 6. ä¿å­˜ç»“æœ
            self.save_results(optimization_results, final_results)
            
            logging.info("=" * 60)
            logging.info("è´å¶æ–¯ä¼˜åŒ–æµç¨‹å®Œæˆ!")
            logging.info("=" * 60)
            
            return {
                'optimization_results': optimization_results,
                'final_results': final_results,
                'final_config': final_config
            }
            
        except Exception as e:
            logging.error(f"è´å¶æ–¯ä¼˜åŒ–æµç¨‹å¤±è´¥: {e}")
            raise

def run_bayesian_optimization(model_type: str = 'auto',
                            feature_type: str = 'best7',
                            use_categorical: bool = True,
                            validation_split: float = 0.7,
                            n_calls: int = 50,
                            save_path: str = './results_bayesian_optimization',
                            target_domain: str = 'B',  # æ·»åŠ ç›®æ ‡åŸŸå‚æ•°
                            **kwargs) -> Dict[str, Any]:
    """è¿è¡Œå®Œæ•´çš„è´å¶æ–¯ä¼˜åŒ–æµç¨‹
    
    å‚æ•°:
        model_type: æ¨¡å‹ç±»å‹
        feature_type: ç‰¹å¾ç±»å‹
        use_categorical: æ˜¯å¦ä½¿ç”¨ç±»åˆ«ç‰¹å¾
        validation_split: éªŒè¯é›†æ¯”ä¾‹
        n_calls: è´å¶æ–¯ä¼˜åŒ–è¿­ä»£æ¬¡æ•°
        save_path: ç»“æœä¿å­˜è·¯å¾„
        target_domain: ç›®æ ‡åŸŸé€‰æ‹© ('B' æˆ– 'C')
    """
    logging.info("=" * 60)
    logging.info("å¼€å§‹å®Œæ•´è´å¶æ–¯ä¼˜åŒ–æµç¨‹")
    logging.info("=" * 60)
    
    # åˆ›å»ºä¼˜åŒ–å™¨å®ä¾‹
    optimizer = BayesianOptimizer(
        model_type=model_type,
        feature_type=feature_type,
        use_categorical=use_categorical,
        validation_split=validation_split,
        n_calls=n_calls,
        save_path=save_path,
        target_domain=target_domain  # ä¼ é€’ç›®æ ‡åŸŸå‚æ•°
    )
    
    # è¿è¡Œå®Œæ•´ä¼˜åŒ–æµç¨‹
    results = optimizer.run_complete_optimization()
    
    return results 