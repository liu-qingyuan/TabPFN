# è´å¶æ–¯MMDä¼˜åŒ–æ–‡æ¡£

## æ¦‚è¿°

è´å¶æ–¯MMDä¼˜åŒ–æ˜¯æœ¬é¡¹ç›®çš„æ ¸å¿ƒåˆ›æ–°åŠŸèƒ½ï¼Œå®ç°äº†**æ¨¡å‹è¶…å‚æ•°**å’Œ**MMDåŸŸé€‚åº”å‚æ•°**çš„è”åˆä¼˜åŒ–ã€‚ä¸ä¼ ç»Ÿçš„åˆ†æ­¥ä¼˜åŒ–ç›¸æ¯”ï¼Œè¿™ç§ç«¯åˆ°ç«¯çš„ä¼˜åŒ–æ–¹æ³•èƒ½å¤Ÿè·å¾—æ›´å¥½çš„åŸŸé€‚åº”æ•ˆæœã€‚

## æ ¸å¿ƒä¼˜åŠ¿

### ğŸ¯ ç«¯åˆ°ç«¯ä¼˜åŒ–
- **ä¼ ç»Ÿæ–¹æ³•**ï¼šå…ˆä¼˜åŒ–æ¨¡å‹å‚æ•°ï¼Œå†è°ƒæ•´MMDå‚æ•°
- **æˆ‘ä»¬çš„æ–¹æ³•**ï¼šåŒæ—¶ä¼˜åŒ–ä¸¤ç±»å‚æ•°ï¼Œé¿å…å±€éƒ¨æœ€ä¼˜

### ğŸ”¬ ç§‘å­¦çš„è¯„ä¼°ç­–ç•¥
- **ä¸‰åˆ†æ³•æ•°æ®åˆ’åˆ†**ï¼šè®­ç»ƒé›†ã€éªŒè¯é›†ã€ä¿ç•™æµ‹è¯•é›†
- **æ— åè¯„ä¼°**ï¼šéªŒè¯é›†ç”¨äºä¼˜åŒ–ï¼Œä¿ç•™æµ‹è¯•é›†ç”¨äºæœ€ç»ˆè¯„ä¼°
- **æ³›åŒ–èƒ½åŠ›åˆ†æ**ï¼šè‡ªåŠ¨è®¡ç®—å¹¶æŠ¥å‘Šæ³›åŒ–å·®è·

### ğŸš€ é«˜æ•ˆçš„æœç´¢ç­–ç•¥
- **è´å¶æ–¯ä¼˜åŒ–**ï¼šåŸºäºé«˜æ–¯è¿‡ç¨‹çš„æ™ºèƒ½æœç´¢
- **Expected Improvement**ï¼šå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨
- **è‡ªé€‚åº”æœç´¢ç©ºé—´**ï¼šæ ¹æ®MMDæ–¹æ³•åŠ¨æ€è°ƒæ•´å‚æ•°èŒƒå›´

## æŠ€æœ¯å®ç°

### æœç´¢ç©ºé—´è®¾è®¡

#### AutoTabPFNæ¨¡å‹å‚æ•°
```python
search_space = [
    Categorical([1, 5, 10, 15, 30, 60, 120, 180], name='max_time'),
    Categorical(['default', 'avoid_overfitting'], name='preset'),
    Categorical(['accuracy', 'roc', 'f1'], name='ges_scoring'),
    Categorical([10, 15, 20, 25, 30], name='max_models'),
    Integer(50, 150, name='n_repeats'),
    Integer(20, 40, name='ges_n_iterations'),
    Categorical([True, False], name='ignore_limits'),
]
```

#### Linear MMDå‚æ•°
```python
mmd_search_space = [
    Real(1e-5, 1e-1, prior='log-uniform', name='mmd_lr'),
    Integer(100, 1000, name='mmd_n_epochs'),
    Integer(32, 256, name='mmd_batch_size'),
    Real(1e-5, 1e-1, prior='log-uniform', name='mmd_lambda_reg'),
    Real(0.1, 10.0, name='mmd_gamma'),
    Categorical([True, False], name='mmd_staged_training'),
    Categorical([True, False], name='mmd_dynamic_gamma'),
]
```

#### KPCA MMDå‚æ•°
```python
kpca_search_space = [
    Integer(10, min(50, n_features), name='mmd_n_components'),
    Real(0.1, 10.0, name='mmd_gamma'),
    Categorical([True, False], name='mmd_standardize'),
]
```

### ç›®æ ‡å‡½æ•°è®¾è®¡

```python
def objective_function(params):
    """
    ç›®æ ‡å‡½æ•°ï¼šè¯„ä¼°ç»™å®šè¶…å‚æ•°ç»„åˆçš„æ€§èƒ½
    
    æµç¨‹ï¼š
    1. åˆ†ç¦»æ¨¡å‹å‚æ•°å’ŒMMDå‚æ•°
    2. åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹ï¼ˆAåŸŸè®­ç»ƒé›†ï¼‰
    3. è¿›è¡ŒMMDåŸŸé€‚åº”ï¼ˆAåŸŸâ†’ç›®æ ‡åŸŸéªŒè¯é›†ï¼‰
    4. åœ¨åŸŸé€‚åº”åçš„éªŒè¯é›†ä¸Šè¯„ä¼°
    5. è¿”å›è´ŸAUCï¼ˆå› ä¸ºgp_minimizeæœ€å°åŒ–ç›®æ ‡å‡½æ•°ï¼‰
    """
    
    # 1. å‚æ•°åˆ†ç¦»
    model_params = {k: v for k, v in params.items() if not k.startswith('mmd_')}
    mmd_params = {k[4:]: v for k, v in params.items() if k.startswith('mmd_')}
    
    # 2. æ¨¡å‹è®­ç»ƒ
    model = create_model(model_params)
    model.fit(X_train, y_train)
    
    # 3. MMDåŸŸé€‚åº”
    X_val_adapted, adaptation_info = mmd_transform(
        X_train_raw, X_val_raw, 
        method=mmd_method, 
        **mmd_params
    )
    
    # 4. éªŒè¯é›†è¯„ä¼°
    y_pred_proba = model.predict_proba(X_val_adapted)[:, 1]
    val_auc = roc_auc_score(y_val, y_pred_proba)
    
    # 5. ä¿ç•™æµ‹è¯•é›†è¯„ä¼°ï¼ˆä»…è®°å½•ï¼Œä¸ç”¨äºä¼˜åŒ–ï¼‰
    X_test_adapted, _ = mmd_transform(
        X_train_raw, X_test_raw, 
        method=mmd_method, 
        **mmd_params
    )
    y_test_pred_proba = model.predict_proba(X_test_adapted)[:, 1]
    test_auc = roc_auc_score(y_test, y_test_pred_proba)
    
    # è®°å½•è¯•éªŒç»“æœ
    record_trial(model_params, mmd_params, val_auc, test_auc)
    
    return -val_auc  # è¿”å›è´Ÿå€¼ç”¨äºæœ€å°åŒ–
```

### æ•°æ®åˆ’åˆ†ç­–ç•¥

```python
def load_and_prepare_data():
    """ä¸‰åˆ†æ³•æ•°æ®åˆ’åˆ†"""
    
    # åŠ è½½æ•°æ®
    df_A = pd.read_excel(DATA_PATHS['A'])  # è®­ç»ƒåŸŸ
    df_target = pd.read_excel(DATA_PATHS[target_domain])  # ç›®æ ‡åŸŸ
    
    # æ•°æ®é¢„å¤„ç†
    X_A_scaled, X_target_scaled, scaler = fit_apply_scaler(
        X_A_raw, X_target_raw, categorical_indices
    )
    
    # ç›®æ ‡åŸŸä¸‰åˆ†æ³•åˆ’åˆ†
    X_val, X_holdout, y_val, y_holdout = train_test_split(
        X_target_scaled, y_target,
        train_size=validation_split,  # é»˜è®¤0.7
        stratify=y_target,
        random_state=random_state
    )
    
    return {
        'X_train': X_A_scaled, 'y_train': y_A,
        'X_val': X_val, 'y_val': y_val,
        'X_holdout': X_holdout, 'y_holdout': y_holdout,
        'X_train_raw': X_A_raw,
        'X_val_raw': X_val_raw,
        'X_holdout_raw': X_holdout_raw
    }
```

## ä½¿ç”¨æŒ‡å—

### åŸºæœ¬ç”¨æ³•

```bash
# æœ€ç®€å•çš„ç”¨æ³•
python scripts/run_bayesian_mmd_optimization.py

# æŒ‡å®šæ¨¡å‹å’ŒMMDæ–¹æ³•
python scripts/run_bayesian_mmd_optimization.py \
    --model-type auto \
    --mmd-method linear

# é€‰æ‹©ç›®æ ‡åŸŸ
python scripts/run_bayesian_mmd_optimization.py \
    --model-type auto \
    --mmd-method linear \
    --target-domain C
```

### é«˜çº§é…ç½®

```bash
# å®Œæ•´é…ç½®ç¤ºä¾‹
python scripts/run_bayesian_mmd_optimization.py \
    --model-type auto \
    --feature-type best7 \
    --mmd-method linear \
    --use-class-conditional \
    --target-domain B \
    --validation-split 0.7 \
    --n-calls 50 \
    --random-state 42 \
    --auto-run-mmd-after-bo \
    --output-dir ./my_optimization_results
```

### å‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `--model-type` | str | 'auto' | æ¨¡å‹ç±»å‹ï¼šauto, base, rf, tuned |
| `--feature-type` | str | 'best7' | ç‰¹å¾ç±»å‹ï¼šall, best7 |
| `--mmd-method` | str | 'linear' | MMDæ–¹æ³•ï¼šlinear, kpca, mean_std |
| `--use-class-conditional` | flag | False | æ˜¯å¦ä½¿ç”¨ç±»æ¡ä»¶MMD |
| `--target-domain` | str | 'B' | ç›®æ ‡åŸŸï¼šB, C |
| `--validation-split` | float | 0.7 | éªŒè¯é›†æ¯”ä¾‹ |
| `--n-calls` | int | 50 | ä¼˜åŒ–è¿­ä»£æ¬¡æ•° |
| `--random-state` | int | 42 | éšæœºç§å­ |
| `--auto-run-mmd-after-bo` | flag | False | ä¼˜åŒ–åè‡ªåŠ¨è¿è¡Œå®Œæ•´å®éªŒ |

## è¾“å‡ºç»“æœ

### æ–‡ä»¶ç»“æ„
```
results_bayesian_mmd_optimization_auto_linear_best7/
â”œâ”€â”€ bayesian_mmd_optimization_history.json    # ä¼˜åŒ–å†å²
â”œâ”€â”€ final_mmd_evaluation.json                 # æœ€ç»ˆè¯„ä¼°ç»“æœ
â”œâ”€â”€ experiment_config.json                    # å®éªŒé…ç½®
â””â”€â”€ bayesian_mmd_optimization.log            # è¯¦ç»†æ—¥å¿—
```

### ç»“æœè§£è¯»

#### ä¼˜åŒ–å†å² (bayesian_mmd_optimization_history.json)
```json
{
  "best_params": {
    "max_time": 30,
    "preset": "avoid_overfitting",
    "mmd_lr": 0.001,
    "mmd_gamma": 1.5
  },
  "best_validation_auc": 0.8234,
  "total_trials": 50,
  "good_configs": [
    {
      "validation_auc": 0.8234,
      "test_auc": 0.7891,
      "model_params": {...},
      "mmd_params": {...}
    }
  ],
  "optimization_history": [...]
}
```

#### æœ€ç»ˆè¯„ä¼° (final_mmd_evaluation.json)
```json
{
  "validation_performance": {
    "auc": 0.8234,
    "f1": 0.7456,
    "accuracy": 0.8012
  },
  "holdout_performance": {
    "auc": 0.7891,
    "f1": 0.7123,
    "accuracy": 0.7789
  },
  "generalization_gap": {
    "auc_gap": 0.0343,
    "f1_gap": 0.0333,
    "accuracy_gap": 0.0223
  }
}
```

### æ€§èƒ½åˆ†æ

#### æ³›åŒ–èƒ½åŠ›è¯„ä¼°
- **AUCå·®è· < 0.05**ï¼šæ³›åŒ–èƒ½åŠ›è‰¯å¥½ âœ…
- **AUCå·®è· â‰¥ 0.05**ï¼šå¯èƒ½è¿‡æ‹Ÿåˆ âš ï¸

#### ä¼˜åŒ–æ•ˆæœè¯„ä¼°
- **éªŒè¯é›†AUC > 0.8**ï¼šä¼˜åŒ–æ•ˆæœä¼˜ç§€
- **æµ‹è¯•é›†AUC > 0.75**ï¼šå®é™…æ€§èƒ½è‰¯å¥½
- **å‘ç°ä¼˜ç§€é…ç½®æ•°é‡**ï¼šæœç´¢ç©ºé—´è¦†ç›–åº¦

## æœ€ä½³å®è·µ

### å‚æ•°é€‰æ‹©å»ºè®®

#### éªŒè¯é›†æ¯”ä¾‹
- **0.7**ï¼šå¹³è¡¡ä¼˜åŒ–ç¨³å®šæ€§å’Œæµ‹è¯•å¯é æ€§ï¼ˆæ¨èï¼‰
- **0.9**ï¼šæ›´å¤§çš„éªŒè¯é›†ï¼Œæ›´ç¨³å®šçš„ä¼˜åŒ–è¿‡ç¨‹

#### ä¼˜åŒ–è¿­ä»£æ¬¡æ•°
- **å¿«é€Ÿæµ‹è¯•**ï¼š20-30æ¬¡
- **æ ‡å‡†ä¼˜åŒ–**ï¼š50æ¬¡ï¼ˆæ¨èï¼‰
- **æ·±åº¦ä¼˜åŒ–**ï¼š100æ¬¡ä»¥ä¸Š

#### MMDæ–¹æ³•é€‰æ‹©
- **Linear**ï¼šæœ€çµæ´»ï¼Œå‚æ•°æœ€å¤šï¼Œé€‚åˆæ·±åº¦ä¼˜åŒ–
- **KPCA**ï¼šä¸­ç­‰å¤æ‚åº¦ï¼Œé€‚åˆä¸­ç­‰è§„æ¨¡ä¼˜åŒ–
- **Mean-Std**ï¼šæœ€ç®€å•ï¼Œé€‚åˆå¿«é€Ÿæµ‹è¯•

### å®éªŒè®¾è®¡å»ºè®®

#### å¯¹æ¯”å®éªŒ
```bash
# 1. åŸºçº¿ï¼šæ— åŸŸé€‚åº”
python scripts/run_analytical_mmd.py --model-type auto --skip-cv-on-a

# 2. æ ‡å‡†MMDï¼šæ‰‹åŠ¨å‚æ•°
python scripts/run_analytical_mmd.py --model-type auto --method linear

# 3. è´å¶æ–¯MMDï¼šä¼˜åŒ–å‚æ•°
python scripts/run_bayesian_mmd_optimization.py --model-type auto --mmd-method linear
```

#### æ¶ˆèå®éªŒ
```bash
# ä»…ä¼˜åŒ–æ¨¡å‹å‚æ•°
python scripts/run_bayesian_optimization.py --model-type auto

# åŒæ—¶ä¼˜åŒ–æ¨¡å‹å’ŒMMDå‚æ•°
python scripts/run_bayesian_mmd_optimization.py --model-type auto --mmd-method linear
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. ä¼˜åŒ–æ”¶æ•›æ…¢
**åŸå› **ï¼šæœç´¢ç©ºé—´è¿‡å¤§æˆ–ç›®æ ‡å‡½æ•°å™ªå£°å¤§
**è§£å†³**ï¼š
- å‡å°‘æœç´¢ç©ºé—´ç»´åº¦
- å¢åŠ åˆå§‹éšæœºç‚¹æ•°é‡
- ä½¿ç”¨æ›´ç¨³å®šçš„è¯„ä¼°æŒ‡æ ‡

#### 2. æ³›åŒ–å·®è·å¤§
**åŸå› **ï¼šéªŒè¯é›†è¿‡æ‹Ÿåˆ
**è§£å†³**ï¼š
- å¢åŠ éªŒè¯é›†å¤§å°ï¼ˆå‡å°‘validation_splitï¼‰
- ä½¿ç”¨æ›´ä¿å®ˆçš„æ¨¡å‹å‚æ•°
- å¢åŠ æ­£åˆ™åŒ–å¼ºåº¦

#### 3. ä¼˜åŒ–ç»“æœä¸ç¨³å®š
**åŸå› **ï¼šéšæœºæ€§å½±å“
**è§£å†³**ï¼š
- å›ºå®šéšæœºç§å­
- å¢åŠ ä¼˜åŒ–è¿­ä»£æ¬¡æ•°
- ä½¿ç”¨å¤šæ¬¡è¿è¡Œçš„å¹³å‡ç»“æœ

### è°ƒè¯•æŠ€å·§

#### å¯ç”¨è¯¦ç»†æ—¥å¿—
```bash
python scripts/run_bayesian_mmd_optimization.py \
    --model-type auto \
    --mmd-method linear \
    --n-calls 10 \
    --log-file debug.log
```

#### å¿«é€ŸéªŒè¯
```bash
# æœ€å°é…ç½®æµ‹è¯•
python scripts/run_bayesian_mmd_optimization.py \
    --model-type auto \
    --mmd-method mean_std \
    --n-calls 5
```

## æ‰©å±•å¼€å‘

### æ·»åŠ æ–°çš„æœç´¢ç©ºé—´
```python
# åœ¨ define_search_space() ä¸­æ·»åŠ 
if self.model_type == 'new_model':
    search_space.extend([
        Real(0.1, 1.0, name='new_param'),
        Integer(1, 10, name='another_param'),
    ])
```

### è‡ªå®šä¹‰ç›®æ ‡å‡½æ•°
```python
def custom_objective_function(self, params):
    """è‡ªå®šä¹‰ç›®æ ‡å‡½æ•°ï¼Œå¯ä»¥ä¼˜åŒ–å¤šä¸ªæŒ‡æ ‡"""
    
    # è·å–åŸºæœ¬æ€§èƒ½
    val_auc = self.evaluate_performance(params)
    
    # æ·»åŠ è‡ªå®šä¹‰çº¦æŸ
    if some_constraint_violated(params):
        return 1.0  # æƒ©ç½šè¿åçº¦æŸçš„å‚æ•°
    
    # å¤šç›®æ ‡ä¼˜åŒ–
    val_f1 = self.evaluate_f1(params)
    combined_score = 0.7 * val_auc + 0.3 * val_f1
    
    return -combined_score
```

### é›†æˆæ–°çš„MMDæ–¹æ³•
```python
# åœ¨æœç´¢ç©ºé—´å®šä¹‰ä¸­æ·»åŠ 
elif self.mmd_method == 'new_mmd_method':
    search_space.extend([
        Real(0.01, 1.0, name='mmd_new_param1'),
        Integer(5, 50, name='mmd_new_param2'),
    ])
```

## ç†è®ºèƒŒæ™¯

### è´å¶æ–¯ä¼˜åŒ–åŸç†
è´å¶æ–¯ä¼˜åŒ–ä½¿ç”¨é«˜æ–¯è¿‡ç¨‹ï¼ˆGaussian Processï¼‰å»ºæ¨¡ç›®æ ‡å‡½æ•°ï¼Œé€šè¿‡è·å–å‡½æ•°ï¼ˆAcquisition Functionï¼‰æŒ‡å¯¼ä¸‹ä¸€æ­¥æœç´¢æ–¹å‘ã€‚

#### Expected Improvement (EI)
```
EI(x) = E[max(f(x) - f(x*), 0)]
```
å…¶ä¸­ f(x*) æ˜¯å½“å‰æœ€ä½³å€¼ã€‚

### MMDåŸŸé€‚åº”ç†è®º
Maximum Mean Discrepancy (MMD) è¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼š
```
MMDÂ²(P, Q) = ||Î¼_P - Î¼_Q||Â²_H
```
å…¶ä¸­ H æ˜¯å†ç”Ÿæ ¸å¸Œå°”ä¼¯ç‰¹ç©ºé—´ã€‚

### è”åˆä¼˜åŒ–çš„ç†è®ºä¼˜åŠ¿
ä¼ ç»Ÿçš„åˆ†æ­¥ä¼˜åŒ–å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼š
```
Î¸* = argmin L(Î¸_model, Î¸_mmd_fixed)
Ï†* = argmin L(Î¸_model_fixed, Î¸_mmd)
```

è€Œè”åˆä¼˜åŒ–èƒ½æ‰¾åˆ°å…¨å±€æœ€ä¼˜ï¼š
```
(Î¸*, Ï†*) = argmin L(Î¸_model, Î¸_mmd)
```

## å‚è€ƒæ–‡çŒ®

1. Snoek, J., et al. "Practical Bayesian optimization of machine learning algorithms." NIPS 2012.
2. Gretton, A., et al. "A kernel two-sample test." JMLR 2012.
3. Long, M., et al. "Learning transferable features with deep adaptation networks." ICML 2015.

---

*æœ¬æ–‡æ¡£æŒç»­æ›´æ–°ä¸­ï¼Œå¦‚æœ‰é—®é¢˜è¯·æäº¤Issueæˆ–è”ç³»å¼€å‘å›¢é˜Ÿã€‚* 