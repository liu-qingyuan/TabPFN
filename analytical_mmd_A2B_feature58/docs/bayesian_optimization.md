# è´å¶æ–¯ä¼˜åŒ–æ¨¡å—æ–‡æ¡£

## æ¦‚è¿°

è´å¶æ–¯ä¼˜åŒ–æ¨¡å—å®ç°äº†åŸºäºç›®æ ‡åŸŸéªŒè¯é›†çš„è¶…å‚æ•°ä¼˜åŒ–ï¼Œé‡‡ç”¨ä¸‰åˆ†æ³•æ•°æ®åˆ’åˆ†ç­–ç•¥ï¼Œç¡®ä¿æ¨¡å‹é€‰æ‹©å’Œæœ€ç»ˆè¯„ä¼°çš„ç‹¬ç«‹æ€§ã€‚

## ğŸ¯ æ ¸å¿ƒç‰¹æ€§

### 1. ä¸‰åˆ†æ³•æ•°æ®åˆ’åˆ†
- **AåŸŸè®­ç»ƒé›†**: ç”¨äºæ¨¡å‹è®­ç»ƒ
- **BåŸŸéªŒè¯é›†**: ç”¨äºè´å¶æ–¯ä¼˜åŒ–ç›®æ ‡å‡½æ•°è¯„ä¼° (80%)
- **BåŸŸä¿ç•™æµ‹è¯•é›†**: ç”¨äºæœ€ç»ˆæ¨¡å‹æ³›åŒ–èƒ½åŠ›è¯„ä¼° (20%)

### 2. æ”¯æŒçš„æ¨¡å‹ç±»å‹
- **AutoTabPFN** (`auto`): è‡ªåŠ¨åŒ–TabPFNï¼Œæ”¯æŒæ—¶é—´é™åˆ¶å’Œæ¨¡å‹æ•°é‡ä¼˜åŒ–
- **åŸºç¡€TabPFN** (`base`): åŸç”ŸTabPFNï¼Œå‚æ•°è¾ƒå°‘
- **Random Forest** (`rf`): ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå‚æ•°ä¸°å¯Œ
- **TunedTabPFN** (`tuned`): è°ƒä¼˜ç‰ˆTabPFN

### 3. æ™ºèƒ½è¶…å‚æ•°ç©ºé—´
æ ¹æ®ä¸åŒæ¨¡å‹ç±»å‹è‡ªåŠ¨å®šä¹‰åˆé€‚çš„è¶…å‚æ•°æœç´¢ç©ºé—´ï¼š

#### AutoTabPFN å‚æ•°ç©ºé—´
```python
search_space = [
    Integer(30, 300, name='max_time'),        # æœ€å¤§è®­ç»ƒæ—¶é—´(ç§’)
    Integer(1, 10, name='max_models'),        # æœ€å¤§æ¨¡å‹æ•°
    Real(0.1, 1.0, name='ensemble_size'),     # é›†æˆå¤§å°
    Categorical(['auto', 'balanced'], name='class_weight')  # ç±»åˆ«æƒé‡
]
```

#### Random Forest å‚æ•°ç©ºé—´
```python
search_space = [
    Integer(50, 500, name='n_estimators'),    # æ ‘çš„æ•°é‡
    Integer(1, 20, name='max_depth'),         # æœ€å¤§æ·±åº¦
    Integer(2, 20, name='min_samples_split'), # æœ€å°åˆ†å‰²æ ·æœ¬æ•°
    Integer(1, 10, name='min_samples_leaf'),  # æœ€å°å¶å­æ ·æœ¬æ•°
    Real(0.1, 1.0, name='max_features'),      # æœ€å¤§ç‰¹å¾æ¯”ä¾‹
    Categorical(['auto', 'balanced'], name='class_weight')
]
```

## ğŸ“‹ ä½¿ç”¨æ–¹æ³•

### å‘½ä»¤è¡Œä½¿ç”¨

#### åŸºæœ¬ç”¨æ³•
```bash
# AutoTabPFNæ¨¡å‹ï¼Œä½¿ç”¨æœ€ä½³7ç‰¹å¾
python scripts/run_bayesian_optimization.py --model-type auto --feature-type best7

# Random Forestæ¨¡å‹ï¼Œ30æ¬¡ä¼˜åŒ–è¿­ä»£
python scripts/run_bayesian_optimization.py --model-type rf --n-calls 30

# ä¸ä½¿ç”¨ç±»åˆ«ç‰¹å¾
python scripts/run_bayesian_optimization.py --model-type auto --no-categorical
```

#### é«˜çº§é…ç½®
```bash
# è‡ªå®šä¹‰éªŒè¯é›†æ¯”ä¾‹å’Œä¼˜åŒ–æ¬¡æ•°
python scripts/run_bayesian_optimization.py \
    --model-type auto \
    --validation-split 0.7 \
    --n-calls 100 \
    --random-state 123

# æŒ‡å®šè¾“å‡ºç›®å½•
python scripts/run_bayesian_optimization.py \
    --model-type auto \
    --output-dir ./my_optimization_results
```

### Python API ä½¿ç”¨

#### ç®€å•è°ƒç”¨
```python
from analytical_mmd_A2B_feature58.modeling.bayesian_optimizer import run_bayesian_optimization

# è¿è¡Œè´å¶æ–¯ä¼˜åŒ–
results = run_bayesian_optimization(
    model_type='auto',
    feature_type='best7',
    use_categorical=True,
    n_calls=50
)

print(f"æœ€ä½³AUC: {results['optimization_results']['best_validation_auc']:.4f}")
print(f"æœ€ä½³å‚æ•°: {results['optimization_results']['best_params']}")
```

#### é«˜çº§ä½¿ç”¨
```python
from analytical_mmd_A2B_feature58.modeling.bayesian_optimizer import BayesianOptimizer

# åˆ›å»ºä¼˜åŒ–å™¨å®ä¾‹
optimizer = BayesianOptimizer(
    model_type='auto',
    feature_type='best7',
    use_categorical=True,
    validation_split=0.8,
    n_calls=50,
    save_path='./my_results'
)

# è¿è¡Œå®Œæ•´ä¼˜åŒ–æµç¨‹
results = optimizer.run_complete_optimization()

# è®¿é—®è¯¦ç»†ç»“æœ
print("ä¼˜åŒ–å†å²:", len(optimizer.optimization_results))
print("æœ€ä½³å‚æ•°:", optimizer.best_params)
print("æœ€ç»ˆæ¨¡å‹:", optimizer.final_model)
```

## ğŸ“Š è¾“å‡ºç»“æœ

### ç›®å½•ç»“æ„
```
results_bayesian_optimization_auto_best7_with_categorical/
â”œâ”€â”€ bayesian_optimization.log           # è¯¦ç»†æ—¥å¿—
â”œâ”€â”€ optimization_history.json           # ä¼˜åŒ–å†å²è®°å½•
â”œâ”€â”€ final_evaluation.json              # æœ€ç»ˆè¯„ä¼°ç»“æœ
â””â”€â”€ confusion_matrices.png             # æ··æ·†çŸ©é˜µå›¾
```

### optimization_history.json ç»“æ„
```json
{
  "best_params": {
    "max_time": 180,
    "max_models": 5,
    "ensemble_size": 0.8,
    "class_weight": "balanced"
  },
  "best_validation_auc": 0.8234,
  "optimization_history": [
    {
      "trial_id": 0,
      "params": {...},
      "validation_auc": 0.7856
    },
    ...
  ],
  "total_trials": 50
}
```

### final_evaluation.json ç»“æ„
```json
{
  "best_params": {...},
  "model_config": {
    "model_type": "auto",
    "feature_type": "best7",
    "use_categorical": true,
    "features_count": 7,
    "categorical_indices_count": 3
  },
  "data_split": {
    "train_samples": 1234,
    "validation_samples": 456,
    "holdout_samples": 114,
    "validation_split_ratio": 0.8
  },
  "performance": {
    "validation_performance": {
      "auc": 0.8234,
      "f1": 0.7856,
      "accuracy": 0.8012,
      "confusion_matrix": [[45, 8], [12, 49]]
    },
    "holdout_performance": {
      "auc": 0.8156,
      "f1": 0.7723,
      "accuracy": 0.7895,
      "confusion_matrix": [[23, 4], [6, 25]]
    }
  }
}
```

## ğŸ”§ æ ¸å¿ƒç®—æ³•æµç¨‹

### 1. æ•°æ®å‡†å¤‡é˜¶æ®µ
```python
def load_and_prepare_data(self):
    # 1. åŠ è½½AåŸŸå’ŒBåŸŸæ•°æ®
    df_A = pd.read_excel(DATA_PATHS['A'])
    df_B = pd.read_excel(DATA_PATHS['B'])
    
    # 2. ç‰¹å¾æå–
    X_A = df_A[self.features].values
    X_B = df_B[self.features].values
    
    # 3. æ•°æ®æ ‡å‡†åŒ– (ç”¨AåŸŸæ‹Ÿåˆscaler)
    X_A_scaled, X_B_scaled, scaler = fit_apply_scaler(X_A, X_B)
    
    # 4. BåŸŸä¸‰åˆ†æ³•åˆ’åˆ†
    X_val, X_holdout, y_val, y_holdout = train_test_split(
        X_B_scaled, y_B, 
        train_size=0.8, 
        stratify=y_B
    )
```

### 2. è´å¶æ–¯ä¼˜åŒ–é˜¶æ®µ
```python
def objective_function(self, params):
    # 1. åˆ›å»ºæ¨¡å‹é…ç½®
    model_config = {'categorical_feature_indices': self.categorical_indices}
    model_config.update(params)
    
    # 2. è®­ç»ƒæ¨¡å‹ (åœ¨AåŸŸè®­ç»ƒé›†ä¸Š)
    model = get_model(self.model_type, **model_config)
    model.fit(self.X_train, self.y_train)
    
    # 3. éªŒè¯é›†è¯„ä¼° (åœ¨BåŸŸéªŒè¯é›†ä¸Š)
    y_pred_proba = model.predict_proba(self.X_ext_val)[:, 1]
    auc_score = roc_auc_score(self.y_ext_val, y_pred_proba)
    
    # 4. è¿”å›è´ŸAUC (gp_minimizeæœ€å°åŒ–ç›®æ ‡)
    return -auc_score
```

### 3. æœ€ç»ˆè¯„ä¼°é˜¶æ®µ
```python
def evaluate_final_model(self):
    # 1. ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    final_model = get_model(self.model_type, **self.best_params)
    final_model.fit(self.X_train, self.y_train)
    
    # 2. éªŒè¯é›†æ€§èƒ½ (ç”¨äºå¯¹æ¯”)
    val_metrics = evaluate_on_dataset(final_model, self.X_ext_val, self.y_ext_val)
    
    # 3. ä¿ç•™æµ‹è¯•é›†æ€§èƒ½ (çœŸå®æ³›åŒ–èƒ½åŠ›)
    holdout_metrics = evaluate_on_dataset(final_model, self.X_ext_holdout, self.y_ext_holdout)
    
    return {'validation_performance': val_metrics, 'holdout_performance': holdout_metrics}
```

## ğŸ“ˆ æ€§èƒ½åˆ†æ

### æ³›åŒ–èƒ½åŠ›è¯„ä¼°
ç³»ç»Ÿè‡ªåŠ¨è®¡ç®—éªŒè¯é›†å’Œä¿ç•™æµ‹è¯•é›†ä¹‹é—´çš„æ€§èƒ½å·®è·ï¼š

```python
# è®¡ç®—æ³›åŒ–å·®è·
auc_gap = validation_auc - holdout_auc
f1_gap = validation_f1 - holdout_f1
accuracy_gap = validation_accuracy - holdout_accuracy

# æ³›åŒ–èƒ½åŠ›åˆ¤æ–­
if abs(auc_gap) < 0.05:
    print("âœ“ æ¨¡å‹æ³›åŒ–èƒ½åŠ›è‰¯å¥½")
else:
    print("âš  æ¨¡å‹å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆ")
```

### ä¼˜åŒ–æ”¶æ•›åˆ†æ
é€šè¿‡ `optimization_history.json` å¯ä»¥åˆ†æä¼˜åŒ–è¿‡ç¨‹ï¼š

```python
import json
import matplotlib.pyplot as plt

# åŠ è½½ä¼˜åŒ–å†å²
with open('optimization_history.json', 'r') as f:
    history = json.load(f)

# ç»˜åˆ¶æ”¶æ•›æ›²çº¿
aucs = [trial['validation_auc'] for trial in history['optimization_history']]
plt.plot(aucs)
plt.xlabel('Trial')
plt.ylabel('Validation AUC')
plt.title('Bayesian Optimization Convergence')
plt.show()
```

## âš™ï¸ é…ç½®é€‰é¡¹

### æ¨¡å‹ç‰¹å®šé…ç½®

#### AutoTabPFN æ¨èé…ç½®
```python
# å¿«é€Ÿæ¨¡å¼ (é€‚åˆåˆæ­¥æ¢ç´¢)
run_bayesian_optimization(
    model_type='auto',
    n_calls=30,
    validation_split=0.8
)

# ç²¾ç¡®æ¨¡å¼ (é€‚åˆæœ€ç»ˆä¼˜åŒ–)
run_bayesian_optimization(
    model_type='auto',
    n_calls=100,
    validation_split=0.7
)
```

#### Random Forest æ¨èé…ç½®
```python
# å¹³è¡¡æ¨¡å¼
run_bayesian_optimization(
    model_type='rf',
    n_calls=50,
    feature_type='all'  # RFé€šå¸¸èƒ½å¤„ç†æ›´å¤šç‰¹å¾
)
```

### éªŒè¯é›†æ¯”ä¾‹é€‰æ‹©æŒ‡å—

| æ•°æ®é›†å¤§å° | æ¨èéªŒè¯é›†æ¯”ä¾‹ | è¯´æ˜ |
|-----------|---------------|------|
| < 500æ ·æœ¬ | 0.7 | ä¿ç•™æ›´å¤šæ•°æ®ç”¨äºæµ‹è¯• |
| 500-1000æ ·æœ¬ | 0.8 | å¹³è¡¡éªŒè¯å’Œæµ‹è¯• |
| > 1000æ ·æœ¬ | 0.8-0.9 | å¯ä»¥ç”¨æ›´å¤šæ•°æ®è¿›è¡ŒéªŒè¯ |

## ğŸš€ é›†æˆåˆ°ç°æœ‰å·¥ä½œæµç¨‹

### 1. æ·»åŠ åˆ°ä¸»è„šæœ¬
åœ¨ `run_analytical_mmd.py` ä¸­æ·»åŠ è´å¶æ–¯ä¼˜åŒ–é€‰é¡¹ï¼š

```python
# åœ¨parse_arguments()ä¸­æ·»åŠ 
parser.add_argument('--use-bayesian-optimization', action='store_true',
                   help='ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜')
parser.add_argument('--bo-n-calls', type=int, default=50,
                   help='è´å¶æ–¯ä¼˜åŒ–è¿­ä»£æ¬¡æ•°')

# åœ¨ä¸»å‡½æ•°ä¸­æ·»åŠ 
if args.use_bayesian_optimization:
    from .modeling.bayesian_optimizer import run_bayesian_optimization
    
    bo_results = run_bayesian_optimization(
        model_type=args.model_type,
        feature_type=args.feature_type,
        n_calls=args.bo_n_calls,
        save_path=os.path.join(save_path, 'bayesian_optimization')
    )
    
    # ä½¿ç”¨ä¼˜åŒ–åçš„å‚æ•°ç»§ç»­å®éªŒ
    optimized_params = bo_results['optimization_results']['best_params']
    model_kwargs.update(optimized_params)
```

### 2. ä¸MMDåŸŸé€‚åº”ç»“åˆ
```python
# å…ˆè¿›è¡Œè´å¶æ–¯ä¼˜åŒ–
bo_results = run_bayesian_optimization(model_type='auto', feature_type='best7')
best_params = bo_results['optimization_results']['best_params']

# ç„¶åä½¿ç”¨ä¼˜åŒ–å‚æ•°è¿›è¡ŒMMDåŸŸé€‚åº”å®éªŒ
results = run_cross_domain_experiment(
    model_type='auto',
    feature_type='best7',
    mmd_method='linear',
    **best_params  # ä½¿ç”¨ä¼˜åŒ–åçš„å‚æ•°
)
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. ä¼˜åŒ–æ”¶æ•›ç¼“æ…¢
```python
# è§£å†³æ–¹æ¡ˆï¼šå¢åŠ åˆå§‹éšæœºç‚¹
result = gp_minimize(
    func=objective,
    dimensions=search_space,
    n_calls=50,
    n_initial_points=15  # å¢åŠ åˆ°15ä¸ªåˆå§‹ç‚¹
)
```

#### 2. å†…å­˜ä¸è¶³
```python
# è§£å†³æ–¹æ¡ˆï¼šå‡å°‘å¹¶è¡Œåº¦æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹
optimizer = BayesianOptimizer(
    model_type='base',  # ä½¿ç”¨æ›´è½»é‡çš„æ¨¡å‹
    n_calls=30         # å‡å°‘è¿­ä»£æ¬¡æ•°
)
```

#### 3. éªŒè¯é›†æ€§èƒ½ä¸ç¨³å®š
```python
# è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨æ›´å¤§çš„éªŒè¯é›†æ¯”ä¾‹
optimizer = BayesianOptimizer(
    validation_split=0.9,  # å¢åŠ éªŒè¯é›†æ¯”ä¾‹
    random_state=42        # å›ºå®šéšæœºç§å­
)
```

## ğŸ“š å‚è€ƒèµ„æ–™

### ç›¸å…³è®ºæ–‡
1. Mockus, J. (1974). On Bayesian methods for seeking the extremum
2. Snoek, J., Larochelle, H., & Adams, R. P. (2012). Practical bayesian optimization of machine learning algorithms

### ç›¸å…³åŒ…æ–‡æ¡£
- [scikit-optimize](https://scikit-optimize.github.io/stable/)
- [AutoTabPFN](https://github.com/automl/autotabpfn)

### é¡¹ç›®å†…ç›¸å…³æ–‡æ¡£
- [workflow.md](./workflow.md) - ä¸»è¦å·¥ä½œæµç¨‹
- [models.md](./models.md) - æ¨¡å‹é…ç½®è¯´æ˜
- [config.md](./config.md) - é…ç½®æ–‡ä»¶è¯´æ˜ 